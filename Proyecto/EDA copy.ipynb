{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equipo: \n",
    "\n",
    "- Nombre de alumno 1: Nicolas Herrera\n",
    "- Nombre de alumno 2: Lucas Carrasco\n",
    "\n",
    "### **Link de repositorio de GitHub:** [Repositorio](https://github.com/vspartamo/MDS7202)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indice\n",
    "1. [Introducción](#Introducción:)\n",
    "2. [Análisis Exploratorio de Datos](#Análisisexploratoriodedatos:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "**DEBE CONTENER**\n",
    "\n",
    "Esta sección debe incluir:  \n",
    "- Una descripción breve del problema planteado: ¿Qué se busca predecir?  \n",
    "- Un resumen de los datos de entrada proporcionados.  \n",
    "- La métrica seleccionada para evaluar los modelos, con su respectiva justificación. Dado que los datos están desbalanceados, se recomienda evitar el uso de `accuracy` y centrarse en métricas como `precision`, `recall` o `f1-score`, especificando la clase de interés.  \n",
    "- Una mención breve de los modelos empleados para abordar el problema, incluyendo las transformaciones intermedias aplicadas a los datos.  \n",
    "- Un análisis general de los resultados obtenidos, señalando si el modelo final cumplió con los objetivos planteados y cómo se posicionó frente a los de otros equipos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2 Preprocesamiento**\n",
    "### **2.1 Análisis exploratorio de datos**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza un análisis exploratorio de datos para identificar patrones, tendencias y relaciones en ellos. Esto para comprender mejor las características del conjunto de datos y guiar las siguientes decisiones en el pipeline de modelamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pyarrow pandas scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importan las librerías básicas para trabajar los datos y visualizarlos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data/\"\n",
    "X_t0 = pd.read_parquet(DATA_PATH + \"X_t0.parquet\")\n",
    "y_t0 = pd.read_parquet(DATA_PATH + \"y_t0.parquet\")\n",
    "\n",
    "df_t0 = pd.concat([X_t0, y_t0], axis=1)\n",
    "df_t0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t0.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se entrega una breve descripción de cada una de las 78 columnas:\n",
    "#### **Datos generales del wallet**\n",
    "- **`borrow_block_number`**: Número del bloque en el que ocurrió el préstamo más reciente asociado al monedero.\n",
    "- **`borrow_timestamp`**: Marca de tiempo (timestamp) del préstamo más reciente.\n",
    "- **`wallet_address`**: Dirección del monedero que identifica al usuario.\n",
    "- **`first_tx_timestamp`**: Timestamp de la primera transacción registrada para este monedero.\n",
    "- **`last_tx_timestamp`**: Timestamp de la última transacción registrada.\n",
    "- **`wallet_age`**: Tiempo total desde la primera transacción hasta la fecha actual, generalmente en días o meses.\n",
    "\n",
    "#### **Estadísticas de transacciones**\n",
    "- **`incoming_tx_count`**: Número total de transacciones entrantes al monedero.\n",
    "- **`outgoing_tx_count`**: Número total de transacciones salientes desde el monedero.\n",
    "- **`net_incoming_tx_count`**: Diferencia entre las transacciones entrantes y salientes.\n",
    "- **`total_gas_paid_eth`**: Cantidad total de gas pagado en ETH por todas las transacciones.\n",
    "- **`avg_gas_paid_per_tx_eth`**: Promedio de gas pagado por transacción, expresado en ETH.\n",
    "\n",
    "#### **Datos sobre transacciones riesgosas**\n",
    "- **`risky_tx_count`**: Número de transacciones clasificadas como riesgosas.\n",
    "- **`risky_unique_contract_count`**: Número de contratos únicos involucrados en transacciones riesgosas.\n",
    "- **`risky_first_tx_timestamp`**: Timestamp de la primera transacción riesgosa.\n",
    "- **`risky_last_tx_timestamp`**: Timestamp de la última transacción riesgosa.\n",
    "- **`risky_first_last_tx_timestamp_diff`**: Diferencia temporal entre la primera y la última transacción riesgosa.\n",
    "- **`risky_sum_outgoing_amount_eth`**: Suma de ETH enviados en transacciones riesgosas.\n",
    "\n",
    "#### **Estadísticas de ETH en el monedero**\n",
    "- **`outgoing_tx_sum_eth`**: Suma total de ETH enviados en todas las transacciones salientes.\n",
    "- **`incoming_tx_sum_eth`**: Suma total de ETH recibidos en todas las transacciones entrantes.\n",
    "- **`outgoing_tx_avg_eth`**: Promedio de ETH enviados por transacción saliente.\n",
    "- **`incoming_tx_avg_eth`**: Promedio de ETH recibidos por transacción entrante.\n",
    "- **`max_eth_ever`**: Máximo balance de ETH alcanzado en el monedero.\n",
    "- **`min_eth_ever`**: Mínimo balance de ETH registrado en el monedero.\n",
    "- **`total_balance_eth`**: Balance actual del monedero en ETH.\n",
    "- **`risk_factor`**: Indicador del nivel de riesgo asociado al monedero, basado en algún modelo de análisis.\n",
    "\n",
    "#### **Estadísticas de préstamos y colaterales**\n",
    "- **`total_collateral_eth`**: Suma total de ETH utilizados como colateral.\n",
    "- **`total_collateral_avg_eth`**: Promedio de ETH usados como colateral por préstamo.\n",
    "- **`total_available_borrows_eth`**: Monto total de ETH disponible para préstamo.\n",
    "- **`total_available_borrows_avg_eth`**: Promedio de ETH disponibles para préstamo.\n",
    "- **`avg_weighted_risk_factor`**: Factor de riesgo ponderado promedio.\n",
    "- **`risk_factor_above_threshold_daily_count`**: Número de días en los que el factor de riesgo estuvo por encima de un umbral predefinido.\n",
    "- **`avg_risk_factor`**: Promedio del factor de riesgo del monedero.\n",
    "- **`max_risk_factor`**: Máximo valor del factor de riesgo registrado.\n",
    "- **`borrow_amount_sum_eth`**: Suma total de ETH prestados.\n",
    "- **`borrow_amount_avg_eth`**: Promedio de ETH prestados por transacción.\n",
    "- **`borrow_count`**: Número total de transacciones de préstamo.\n",
    "- **`repay_amount_sum_eth`**: Suma total de ETH devueltos.\n",
    "- **`repay_amount_avg_eth`**: Promedio de ETH devueltos por transacción.\n",
    "- **`repay_count`**: Número total de transacciones de devolución.\n",
    "- **`borrow_repay_diff_eth`**: Diferencia entre ETH prestados y devueltos.\n",
    "\n",
    "#### **Estadísticas de depósitos y retiros**\n",
    "- **`deposit_count`**: Número de transacciones de depósito realizadas.\n",
    "- **`deposit_amount_sum_eth`**: Suma total de ETH depositados.\n",
    "- **`time_since_first_deposit`**: Tiempo transcurrido desde el primer depósito.\n",
    "- **`withdraw_amount_sum_eth`**: Suma total de ETH retirados.\n",
    "- **`withdraw_deposit_diff_if_positive_eth`**: Diferencia positiva entre ETH retirados y depositados.\n",
    "- **`liquidation_count`**: Número de veces que el monedero fue liquidado.\n",
    "- **`time_since_last_liquidated`**: Tiempo transcurrido desde la última liquidación.\n",
    "- **`liquidation_amount_sum_eth`**: Suma total de ETH liquidados.\n",
    "\n",
    "#### **Indicadores del mercado**\n",
    "- **`market_adx`, `market_adxr`, `market_apo`, etc.**: Indicadores técnicos basados en análisis del mercado, como fuerza direccional (ADX), Momentum, osciladores (Aroon), volatilidad (ATR), fuerza relativa (CCI), entre otros. Estos se usan comúnmente para evaluar tendencias o comportamientos del mercado.\n",
    "\n",
    "#### **Estadísticas adicionales**\n",
    "- **`unique_borrow_protocol_count`**: Número de protocolos de préstamos únicos utilizados.\n",
    "- **`unique_lending_protocol_count`**: Número de protocolos de préstamos ofrecidos.\n",
    "- **`target`**: Variable objetivo, posiblemente para un modelo de predicción (como riesgo de impago o clasificación)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Limpieza de los datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se remueve las columnas que corresponden a identificadores o no aportan información relevante al problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    'borrow_block_number',\n",
    "    'wallet_address',\n",
    "    'borrow_timestamp',\n",
    "    'first_tx_timestamp',\n",
    "    'last_tx_timestamp',\n",
    "    'risky_first_tx_timestamp',\n",
    "    'risky_last_tx_timestamp',\n",
    "    'unique_borrow_protocol_count',\n",
    "    'unique_lending_protocol_count',\n",
    "]\n",
    "\n",
    "df_t0_columns_dropped = df_t0.drop(columns=columns_to_drop, inplace=False)\n",
    "\n",
    "df_t0_columns_dropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vemos la existencia de nulos\n",
    "sum(df_t0_columns_dropped.isna().sum() > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t0_columns_dropped.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identificamos las columnas numericas, no hay categoricas\n",
    "numeric_features = df_t0_columns_dropped.select_dtypes(include=['int64', 'float64']).columns\n",
    "train_numeric_features = numeric_features.drop('target')\n",
    "# identificar las columnas categoricas\n",
    "categoric_features = df_t0_columns_dropped.select_dtypes(include = ['object']).columns\n",
    "categoric_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estadísticas generales de las columnas numéricas\n",
    "print(df_t0_columns_dropped.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver el desbalance de clases en la columna target \n",
    "df_t0_columns_dropped['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def plot_numeric_histograms(df, numeric_features, n_cols=6, title=\"Análisis Univariado de las Variables Numéricas\"):\n",
    "    \"\"\"\n",
    "    Función para graficar histogramas de variables numéricas en un DataFrame usando Matplotlib.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): El DataFrame que contiene las variables a graficar.\n",
    "    - numeric_features (list): Lista de nombres de las columnas numéricas a graficar.\n",
    "    - n_cols (int): Número de columnas en la cuadrícula de subplots.\n",
    "    - title (str): Título del gráfico.\n",
    "    \"\"\"\n",
    "    # Calcular el número de filas necesarias\n",
    "    n_rows = math.ceil(len(numeric_features) / n_cols)\n",
    "\n",
    "    # Crear la figura y los ejes\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 4))\n",
    "    axes = axes.flatten()  # Asegurar que sea un array plano para indexar\n",
    "\n",
    "    # Iterar sobre cada variable numérica y graficar\n",
    "    for idx, col in enumerate(numeric_features):\n",
    "        ax = axes[idx]\n",
    "        ax.hist(df[col], bins=30, density=True, color=\"skyblue\", edgecolor=\"black\")\n",
    "        ax.set_title(col, fontsize=12)\n",
    "        ax.set_xlabel(\"Valores\", fontsize=10)\n",
    "        ax.set_ylabel(\"Frecuencia\", fontsize=10)\n",
    "\n",
    "    # Eliminar ejes sobrantes si hay menos gráficos que espacios\n",
    "    for idx in range(len(numeric_features), len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "\n",
    "    # Ajustar el layout y agregar título general\n",
    "    fig.suptitle(title, fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = len(numeric_features) // 3\n",
    "numeric_features_1 = numeric_features[:chunk_size]\n",
    "numeric_features_2 = numeric_features[chunk_size:2*chunk_size]\n",
    "numeric_features_3 = numeric_features[2*chunk_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numeric_histograms(df_t0_columns_dropped, numeric_features_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numeric_histograms(df_t0_columns_dropped, numeric_features_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numeric_histograms(df_t0_columns_dropped, numeric_features_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se estudia la correlación de features\n",
    "correlation_matrix = df_t0_columns_dropped[numeric_features].corr()\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_correlated_groups(df, n=10, m=2):\n",
    "    \"\"\"\n",
    "    Función para obtener los n grupos de m columnas más relacionadas entre sí.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame que contiene las columnas a analizar.\n",
    "    - n (int): Número de grupos a retornar.\n",
    "    - m (int): Número de columnas en cada grupo.\n",
    "\n",
    "    Returns:\n",
    "    - list: Lista de tuplas, donde cada tupla contiene las columnas de un grupo y su correlación promedio.\n",
    "    \"\"\"\n",
    "    corr_matrix = df.corr().abs()  # Matriz de correlación absoluta\n",
    "    np.fill_diagonal(corr_matrix.values, 0)  # Ignorar la diagonal principal\n",
    "\n",
    "    groups = []\n",
    "    used_columns = set()\n",
    "\n",
    "    for _ in range(n):\n",
    "        # Encontrar el par de columnas más correlacionadas no utilizadas\n",
    "        max_corr = corr_matrix.max().max()\n",
    "        idx = corr_matrix.stack().idxmax()\n",
    "        top_features = list(idx)\n",
    "\n",
    "        # Agregar las columnas adicionales más correlacionadas si m > 2\n",
    "        if m > 2:\n",
    "            remaining_corrs = corr_matrix.loc[top_features].mean(axis=0).sort_values(ascending=False)\n",
    "            for col in remaining_corrs.index:\n",
    "                if col not in used_columns and col not in top_features:\n",
    "                    top_features.append(col)\n",
    "                if len(top_features) == m:\n",
    "                    break\n",
    "\n",
    "        # Calcular la correlación promedio del grupo\n",
    "        avg_corr = corr_matrix.loc[top_features, top_features].mean().mean()\n",
    "        groups.append((tuple(top_features), avg_corr))\n",
    "\n",
    "        # Actualizar columnas utilizadas y matriz de correlación\n",
    "        used_columns.update(top_features)\n",
    "        corr_matrix.loc[top_features, :] = 0\n",
    "        corr_matrix.loc[:, top_features] = 0\n",
    "\n",
    "    return groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_correlated_groups(df_t0_columns_dropped, 10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se obtiene las 10 variables más correlacionadas con 'target', pero su correlación es muy baja para considerarlas relevantes\n",
    "correlations = df_t0_columns_dropped.corr(numeric_only=True)['target'].dropna()\n",
    "correlations_sorted = correlations.abs().sort_values(ascending=False)\n",
    "top_10_correlated_variables = correlations_sorted.index[1:11]  \n",
    "print(\"10 variables más correlacionadas con 'target':\")\n",
    "print(correlations[top_10_correlated_variables])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se estudiará la distribución de las 3 variables más correlacionadas con 'target'\n",
    "top_3_correlated_variables = correlations_sorted.index[1:4]\n",
    "def plot_distributions_grid_stacked(df, max_plots_per_row=3, hue=None, normalize=False, clip_percentiles=(1, 99)):\n",
    "    \"\"\"\n",
    "    Plots a grid of histograms with stacked bars and overlaid KDE lines for each column in a DataFrame.\n",
    "    Each plot has its own scale for both X and Y axes.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing the data to plot.\n",
    "    - max_plots_per_row (int): Maximum number of plots per row.\n",
    "    - hue (str): Column name to use for coloring the plots (optional).\n",
    "    - normalize (bool): Whether to normalize histograms for comparison.\n",
    "    - clip_percentiles (tuple): Percentiles to clip the data for better visualization.\n",
    "    \"\"\"\n",
    "    # Calculate the grid dimensions\n",
    "    num_columns = len(df.columns)\n",
    "    if hue in df.columns:\n",
    "        num_columns -= 1  # Exclude hue column from plotting\n",
    "    \n",
    "    num_rows = int(np.ceil(num_columns / max_plots_per_row))\n",
    "    \n",
    "    fig, axes = plt.subplots(num_rows, max_plots_per_row, figsize=(5 * max_plots_per_row, 4 * num_rows))\n",
    "    axes = axes.flatten()  # Flatten to make indexing easier\n",
    "    \n",
    "    columns_to_plot = [col for col in df.columns if col != hue]\n",
    "    \n",
    "    # Plot each column\n",
    "    for i, column in enumerate(columns_to_plot):\n",
    "        ax = axes[i]\n",
    "        # Clip data to remove outliers\n",
    "        lower, upper = np.percentile(df[column], clip_percentiles)\n",
    "        clipped_data = df[(df[column] >= lower) & (df[column] <= upper)]\n",
    "        \n",
    "        if hue and hue in df.columns:\n",
    "            # Plot stacked histogram\n",
    "            sns.histplot(data=clipped_data, x=column, hue=hue, kde=False, \n",
    "                         stat='density' if normalize else 'count', ax=ax, element=\"bars\", multiple=\"stack\")\n",
    "            # Add overlaid KDE lines\n",
    "            sns.kdeplot(data=clipped_data, x=column, hue=hue, ax=ax, common_norm=normalize, legend=False)\n",
    "        else:\n",
    "            sns.histplot(clipped_data[column], kde=True, stat='density' if normalize else 'count', ax=ax)\n",
    "        \n",
    "        ax.set_title(column)\n",
    "        ax.set_xlim(lower, upper)  # Set x-axis limits to clipped range\n",
    "    \n",
    "    # Remove unused subplots\n",
    "    for j in range(len(columns_to_plot), len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "to_plot_cols = top_3_correlated_variables.tolist() + ['target']\n",
    "plot_distributions_grid_stacked(df_t0_columns_dropped[to_plot_cols], max_plots_per_row=3, hue='target', normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen y conclusiones\n",
    "\n",
    "**Hay q escribirlo bonito y ver si hay algo más**\n",
    "Hecho el Análisis Exploratorio de Datos se obtuve las siguientes observaciones:\n",
    " \n",
    "- Se debe eliminar columnas que no aportan información relevantes, entre ellas las del tipo timestamp o identificadores.\n",
    "\n",
    "- No hay nulos ni blancos en X_t0, por lo que en esta instancia no es necesario realizar una imputación o filtrado de datos.\n",
    "\n",
    "- El desbalance de clase en la columna target es despreciable en el dataset inicial \"X_t0\" por lo que no se considerará que esta variable tenga desbalance.\n",
    "\n",
    "- No hay columnas categóricas, por lo que no será necesario implementar en el preprocesamiento pasos relacionados a esto.\n",
    "\n",
    "- Existen diversas variables que concentran sus valores en 0, lo que pareciera no aportar información relevante. \n",
    "- Las variables no muestran una distribución normal, por lo que al escalarlas no se utilizará StandarScaler si no MinMaxScaler.\n",
    "\n",
    "- No existen pares de columnas que tengan una alta correlación, por lo que estas no son redundantes entre sí.\n",
    "\n",
    "- No existen columnas con una alta correlación a la variable target.\n",
    "\n",
    "Con esto en consideración, se concluye el análisis exploratorio de datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 Preprocesamiento de Datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de un primer acercamiento a los datos, se procede a realizar un preprocesamiento de estos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', MinMaxScaler())   # se escoge minmax scaler dado que los datos no tienen una distribución normal en ninguna feature\n",
    "\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features.drop('target'))\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31007, 68), (6644, 68), (6645, 68))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_t0_columns_dropped.drop(columns='target')\n",
    "y = df_t0_columns_dropped['target']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size=0.7, stratify=y_t0, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, train_size=0.5, stratify=y_temp, random_state=42)\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "X_val = pd.DataFrame(X_val, columns=X.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X.columns)\n",
    "\n",
    "y_train = np.array(y_train).ravel()\n",
    "y_val = np.array(y_val).ravel()\n",
    "y_test = np.array(y_test).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Modelamiento**\n",
    "### **3.1 Baseline** \n",
    "Se implementa un baseline, para ello se elige Decision Tree  y se crea una función que englobe los Pipelines y retorne métricas de interés. En lo siguiente, consideraremos como métrica objetivo AUC porque se trabaja un problema de clasificación binaria donde es importante distinguir adecuadamente ambas clases por igual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from typing import Callable, List, Union\n",
    "import mlflow\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "def create_preprocessor(\n",
    "    numeric_features: Union[pd.Index, List[str]],\n",
    "    \n",
    "    scaler=None,\n",
    "    use_pca=False,\n",
    "    pca_components=50,\n",
    "):\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "    # Define numeric transformer\n",
    "    numeric_transformer_steps = [(\"scaler\", scaler)]\n",
    "    if use_pca:\n",
    "        numeric_transformer_steps.append((\"pca\", PCA(n_components=pca_components)))\n",
    "    numeric_transformer = Pipeline(steps=numeric_transformer_steps)\n",
    "\n",
    "\n",
    "\n",
    "    # Combine preprocessors\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "def create_pipeline(\n",
    "    model: BaseEstimator,\n",
    "    preprocessor: ColumnTransformer,\n",
    "):\n",
    "    return Pipeline(steps=[(\"preprocessor\", preprocessor), (\"classifier\", model)])\n",
    "\n",
    "\n",
    "def optimize_hyperparameters(\n",
    "    model: BaseEstimator,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: Union[pd.Series, np.ndarray],\n",
    "    get_optuna_params: Callable,\n",
    "    experiment_name: str,\n",
    "    n_trials: int = 50,\n",
    "    direction: str = \"maximize\",\n",
    "):\n",
    "    def objective(trial: optuna.Trial):\n",
    "        run_name = f\"trial_{trial.number}_optimization\"\n",
    "        with mlflow.start_run(run_name=run_name, nested=True):\n",
    "            optuna_params = get_optuna_params(trial)\n",
    "            model.set_params(**optuna_params)\n",
    "            mlflow.log_params(optuna_params)\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Optimizamos sobre el AUC ROC\n",
    "            y_proba_pred = model.predict_proba(X_train)[:, 1]\n",
    "            roc_auc = roc_auc_score(y_train, y_proba_pred)\n",
    "            mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "            \n",
    "            return roc_auc\n",
    "\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    with mlflow.start_run(run_name=\"optuna_study\"):\n",
    "        study = optuna.create_study(direction=direction)\n",
    "        study.optimize(objective, n_trials=n_trials)\n",
    "        mlflow.end_run()\n",
    "\n",
    "    # Log best parameters in a separate run\n",
    "    with mlflow.start_run(run_name=\"best_params\", nested=False):\n",
    "        mlflow.log_params(study.best_params)\n",
    "\n",
    "    return study.best_params\n",
    "\n",
    "\n",
    "def evaluate_model(y_true, y_pred, y_pred_proba=None):\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, average=\"binary\"),\n",
    "        \"recall\": recall_score(y_true, y_pred, average=\"binary\"),\n",
    "        \"f1_score\": f1_score(y_true, y_pred, average=\"binary\"),\n",
    "        \"roc_auc\": (\n",
    "            roc_auc_score(y_true, y_pred_proba) if y_pred_proba is not None else None\n",
    "        ),\n",
    "    }\n",
    "    print(f\"Métricas del modelo:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"    {k}: {v}\")\n",
    "    print(f\"Confusion matrix:\\n{confusion_matrix(y_true, y_pred)}\")\n",
    "    print(f\"Classification report:\\n{classification_report(y_true, y_pred)}\")\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def log_metrics(metrics: dict, prefix: str):\n",
    "    mlflow.log_metrics({f\"{prefix}_{k}\": v for k, v in metrics.items()})\n",
    "\n",
    "\n",
    "def save_model(pipeline: Pipeline, save_model_path: str):\n",
    "    with open(save_model_path, \"wb\") as f:\n",
    "        pickle.dump(pipeline, f)\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: BaseEstimator,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: Union[pd.Series, np.ndarray],\n",
    "    X_val: pd.DataFrame,\n",
    "    y_val: Union[pd.Series, np.ndarray],\n",
    "    numeric_features: Union[pd.Index, List[str]],\n",
    "    experiment_name: str,\n",
    "    save_model_path: str = None,\n",
    "    scaler=None,\n",
    "    use_pca=False,\n",
    "    pca_components=50,\n",
    "    optimize: bool = False,\n",
    "    get_optuna_params: Callable = None,\n",
    "    n_trials: int = 50,\n",
    "):\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    # Create preprocessor\n",
    "    preprocessor = create_preprocessor(\n",
    "        numeric_features, scaler, use_pca, pca_components\n",
    "    )\n",
    "\n",
    "    # Optimize hyperparameters if required\n",
    "    if optimize and get_optuna_params:\n",
    "        best_params = optimize_hyperparameters(\n",
    "            model, X_train, y_train, get_optuna_params, experiment_name, n_trials\n",
    "        )\n",
    "        mlflow.end_run()\n",
    "        model.set_params(**best_params)\n",
    "\n",
    "    # Create pipeline\n",
    "    pipeline = create_pipeline(model, preprocessor)\n",
    "\n",
    "    # Train the pipeline\n",
    "    with mlflow.start_run(run_name=\"best_params_training\"):\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Log metrics\n",
    "        y_train_pred = pipeline.predict(X_train)\n",
    "        y_val_pred = pipeline.predict(X_val)\n",
    "\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_train_proba_pred = pipeline.predict_proba(X_train)[:, 1]\n",
    "            y_val_proba_pred = pipeline.predict_proba(X_val)[:, 1]\n",
    "        else:\n",
    "            y_train_proba_pred = None\n",
    "            y_val_proba_pred = None\n",
    "\n",
    "        print(\"Evaluación del modelo en el conjunto de entrenamiento:\")\n",
    "        train_metrics = evaluate_model(y_train, y_train_pred, y_train_proba_pred)\n",
    "        print(\"Evaluación del modelo en el conjunto de validación:\")\n",
    "        val_metrics = evaluate_model(y_val, y_val_pred, y_val_proba_pred)\n",
    "\n",
    "        log_metrics(train_metrics, \"train\")\n",
    "        log_metrics(val_metrics, \"val\")\n",
    "\n",
    "        if save_model_path:\n",
    "            # Save model and preprocessor\n",
    "            save_model(pipeline, save_model_path)\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def retrain_model(\n",
    "    pipeline_or_path: Union[str, Pipeline],\n",
    "    X: pd.DataFrame,\n",
    "    y: Union[pd.Series, np.ndarray],\n",
    "    fun_to_update_model: Callable,\n",
    "    save_model_path: str,\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    optimize: bool = False,\n",
    "    get_optuna_params: Callable = None,\n",
    "    n_trials: int = 50,\n",
    "):\n",
    "    if isinstance(pipeline_or_path, str):\n",
    "        with open(pipeline_or_path, \"rb\") as f:\n",
    "            pipeline = pickle.load(f)\n",
    "    else:\n",
    "        pipeline = pipeline_or_path\n",
    "\n",
    "    preprocessor = pipeline.named_steps[\"preprocessor\"]\n",
    "    model: BaseEstimator = pipeline.named_steps[\"classifier\"]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    X_train_transformed = preprocessor.transform(X_train)\n",
    "\n",
    "    if optimize and get_optuna_params:\n",
    "        best_params = optimize_hyperparameters(\n",
    "            model,\n",
    "            X_train_transformed,\n",
    "            y_train,\n",
    "            get_optuna_params,\n",
    "            \"retraining_optimization\",\n",
    "            n_trials,\n",
    "        )\n",
    "        model.set_params(**best_params)\n",
    "\n",
    "    model = fun_to_update_model(model, X_train_transformed, y_train)\n",
    "\n",
    "    pipeline = create_pipeline(model, preprocessor)\n",
    "    save_model(pipeline, save_model_path)\n",
    "\n",
    "    X_test_transformed = preprocessor.transform(X_test)\n",
    "    y_pred = model.predict(X_test_transformed)\n",
    "    y_pred_proba = (\n",
    "        model.predict_proba(X_test_transformed)[:, 1]\n",
    "        if hasattr(model, \"predict_proba\")\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    print(\"Evaluación en el conjunto de prueba:\")\n",
    "    evaluate_model(y_test, y_pred, y_pred_proba)\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/12 16:25:22 INFO mlflow.tracking.fluent: Experiment with name 'Basic Decision Tree' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluación del modelo en el conjunto de entrenamiento:\n",
      "Métricas del modelo:\n",
      "    accuracy: 1.0\n",
      "    precision: 1.0\n",
      "    recall: 1.0\n",
      "    f1_score: 1.0\n",
      "    roc_auc: 1.0\n",
      "Confusion matrix:\n",
      "[[15148     0]\n",
      " [    0 15859]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     15148\n",
      "           1       1.00      1.00      1.00     15859\n",
      "\n",
      "    accuracy                           1.00     31007\n",
      "   macro avg       1.00      1.00      1.00     31007\n",
      "weighted avg       1.00      1.00      1.00     31007\n",
      "\n",
      "Evaluación del modelo en el conjunto de validación:\n",
      "Métricas del modelo:\n",
      "    accuracy: 0.8264599638771825\n",
      "    precision: 0.8258345428156749\n",
      "    recall: 0.8372572101236021\n",
      "    f1_score: 0.8315066491304983\n",
      "    roc_auc: 0.8262071632873094\n",
      "Confusion matrix:\n",
      "[[2646  600]\n",
      " [ 553 2845]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.82      3246\n",
      "           1       0.83      0.84      0.83      3398\n",
      "\n",
      "    accuracy                           0.83      6644\n",
      "   macro avg       0.83      0.83      0.83      6644\n",
      "weighted avg       0.83      0.83      0.83      6644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree_pipe, _ = train_model(\n",
    "    model=DecisionTreeClassifier(),\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    experiment_name=\"Basic Decision Tree\",\n",
    "    numeric_features=train_numeric_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2 Modelos de Machine Learning**\n",
    "\n",
    "Hecho el baseline, se implementará Random Forest, XGBoost, CatBoost y LGBoost.\n",
    "\n",
    "- Random Forest: Es un modelo de basado en árboles de decisión que utiliza aglomerativamente el conocimiento obtenido de cada uno. Cada árbol entrena sobre una muestra random del conjunto de datos y se utiliza una selección aleatoria de características para hacer la división de nodos. Los hiperparámetros iniciales de este modelo son `n_estimators` que corresponde al número de árboles por bosque, que por docente son 100. `max_depth` que es la profundidad máxima de los árboles que a priori es `None`, lo que quiere decir que los nodos se expanden hasta quetodas las hojas son puras o contienen menos muestras que `min_samples_split`, este parámetro corresponde a la cantidad mínima de muestras requeridas para dividir un nodo interno, su valor por defecto es 2. \n",
    "`min_samples_leaf` es el número mínimo de muestras requeridas para que se considere hoja, su valor por defecto es setado como 1,\n",
    "\n",
    "\n",
    "- XGBoost: es un modelo basado en árboles de decisión que utiliza boosting mediante un entrenamiento secuencial de modelos con gradientes, se caracteriza por ser de rápido tiempo de ejecución. Entre los hiperpárametros del modelo se encuentran `n_estimators` que se ajusta igualmente que en RandomForest, `max_depth` es el mismo hiperparámetro descrito anteriormente sólo que en este modelo su valor por defecto es $6$. Otros hiperparámetros que se ajustan por defecto son `learning_rate` que es la tasa de aprendizaje que ajusta la contribución de cada árbol, su valor por defecto es $0.1$, `subsample` es la proporción se muestras utilizadas para contruir cada árbol y su valor por defecto es $1.0$, otro hiperparámetro importante es `reg_alpha` que es la regularización L1 ajustada por defecto como desactivada y en cambio se tiene activada `reg_lambda` que es la regularización L2 que es la que se implementa por defecto.\n",
    "\n",
    "- LGBoost: Es un modelo similar a XGBoost en el sentido de usar boosting, solo que lo hace sobre histogramas, es especialmente útil para datos de gran embergadura que requieran un entrenamiento rápido. Sus hiperparametros seteados de por defecto son `n_estimators` (100), `learning_rate` (0.1), `max_depth` (-1, sin límite), `num_leaves` (31), `lambda_l1` (0, i.e desactivado), `lambda_l2` (0, i.e desactivado) que se ajustan por defecto con los valores dados en los paréntesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/12 16:25:31 INFO mlflow.tracking.fluent: Experiment with name 'Basic Random Forest' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluación del modelo en el conjunto de entrenamiento:\n",
      "Métricas del modelo:\n",
      "    accuracy: 1.0\n",
      "    precision: 1.0\n",
      "    recall: 1.0\n",
      "    f1_score: 1.0\n",
      "    roc_auc: 1.0\n",
      "Confusion matrix:\n",
      "[[15148     0]\n",
      " [    0 15859]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     15148\n",
      "           1       1.00      1.00      1.00     15859\n",
      "\n",
      "    accuracy                           1.00     31007\n",
      "   macro avg       1.00      1.00      1.00     31007\n",
      "weighted avg       1.00      1.00      1.00     31007\n",
      "\n",
      "Evaluación del modelo en el conjunto de validación:\n",
      "Métricas del modelo:\n",
      "    accuracy: 0.8947922937989163\n",
      "    precision: 0.9323934636334508\n",
      "    recall: 0.8563861094761624\n",
      "    f1_score: 0.892774965485504\n",
      "    roc_auc: 0.9601531581224431\n",
      "Confusion matrix:\n",
      "[[3035  211]\n",
      " [ 488 2910]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.93      0.90      3246\n",
      "           1       0.93      0.86      0.89      3398\n",
      "\n",
      "    accuracy                           0.89      6644\n",
      "   macro avg       0.90      0.90      0.89      6644\n",
      "weighted avg       0.90      0.89      0.89      6644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest_pipe, _ = train_model(\n",
    "    model=RandomForestClassifier(),\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    experiment_name=\"Basic Random Forest\",\n",
    "    numeric_features=train_numeric_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/12 16:26:30 INFO mlflow.tracking.fluent: Experiment with name 'Basic XGBoost' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluación del modelo en el conjunto de entrenamiento:\n",
      "Métricas del modelo:\n",
      "    accuracy: 0.9512368174928242\n",
      "    precision: 0.9786481617401748\n",
      "    recall: 0.9248376316287281\n",
      "    f1_score: 0.9509822991635869\n",
      "    roc_auc: 0.9916981588457949\n",
      "Confusion matrix:\n",
      "[[14828   320]\n",
      " [ 1192 14667]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95     15148\n",
      "           1       0.98      0.92      0.95     15859\n",
      "\n",
      "    accuracy                           0.95     31007\n",
      "   macro avg       0.95      0.95      0.95     31007\n",
      "weighted avg       0.95      0.95      0.95     31007\n",
      "\n",
      "Evaluación del modelo en el conjunto de validación:\n",
      "Métricas del modelo:\n",
      "    accuracy: 0.885009030704395\n",
      "    precision: 0.9146725440806045\n",
      "    recall: 0.8549146556798116\n",
      "    f1_score: 0.8837846060237299\n",
      "    roc_auc: 0.9515175013245804\n",
      "Confusion matrix:\n",
      "[[2975  271]\n",
      " [ 493 2905]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.92      0.89      3246\n",
      "           1       0.91      0.85      0.88      3398\n",
      "\n",
      "    accuracy                           0.89      6644\n",
      "   macro avg       0.89      0.89      0.88      6644\n",
      "weighted avg       0.89      0.89      0.88      6644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Entrenar el modelo con XGBoost\n",
    "xgboost_pipe, _ = train_model(\n",
    "    model=XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\"),\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    experiment_name=\"Basic XGBoost\",\n",
    "    numeric_features=train_numeric_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **LGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/12 16:26:33 INFO mlflow.tracking.fluent: Experiment with name 'Basic LightGBM' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 15859, number of negative: 15148\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023235 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15502\n",
      "[LightGBM] [Info] Number of data points in the train set: 31007, number of used features: 68\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.511465 -> initscore=0.045869\n",
      "[LightGBM] [Info] Start training from score 0.045869\n",
      "Evaluación del modelo en el conjunto de entrenamiento:\n",
      "Métricas del modelo:\n",
      "    accuracy: 0.8817363821072661\n",
      "    precision: 0.9371145848271906\n",
      "    recall: 0.824074657922946\n",
      "    f1_score: 0.8769669518537159\n",
      "    roc_auc: 0.9576119359420247\n",
      "Confusion matrix:\n",
      "[[14271   877]\n",
      " [ 2790 13069]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.94      0.89     15148\n",
      "           1       0.94      0.82      0.88     15859\n",
      "\n",
      "    accuracy                           0.88     31007\n",
      "   macro avg       0.89      0.88      0.88     31007\n",
      "weighted avg       0.89      0.88      0.88     31007\n",
      "\n",
      "Evaluación del modelo en el conjunto de validación:\n",
      "Métricas del modelo:\n",
      "    accuracy: 0.8600240818783865\n",
      "    precision: 0.9067237969676994\n",
      "    recall: 0.8095938787522072\n",
      "    f1_score: 0.855410447761194\n",
      "    roc_auc: 0.9318753610637549\n",
      "Confusion matrix:\n",
      "[[2963  283]\n",
      " [ 647 2751]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.91      0.86      3246\n",
      "           1       0.91      0.81      0.86      3398\n",
      "\n",
      "    accuracy                           0.86      6644\n",
      "   macro avg       0.86      0.86      0.86      6644\n",
      "weighted avg       0.86      0.86      0.86      6644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Entrenar el modelo con LightGBM\n",
    "lightgbm_pipe, _ = train_model(\n",
    "    model=LGBMClassifier(),\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    experiment_name=\"Basic LightGBM\",\n",
    "    numeric_features=train_numeric_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestra métrica objetivo es AUC, por lo que XGBoost  es el modelo que funciona mejor, esto notando que el baseline obtuvo en el conjunto de entrenamiento $1.0$ y en validation $0.82$, es decir, se sobreajustó, mientras que XGBoost mostró un desempeñó consistentemente bueno, obteniendo AUC de $0.99$ en train y $0.95$ en validation, y como muestra una diferencia menor a un 5%  (es 4%) se considera que no realizó overfitting pese a que obtiene resultados demasiado buenos en train. Esto se puede explicar dado que XGBoost implementa regularización que es una medida para prevenir el overfitting, los árboles de DecisionTree pueden ser más profundos de lo ideal llevándolo a sobre entrenarse o bien, por la naturaleza misma de los modelos en cuestión, ya que XGBoost está preparado para capturar patrones con más complejidad que DecisionTree. \n",
    "Por otra parte, DecisionTree se ejecuta más lento que XGBoost, demorando aproximadamente 8 segundos mientras que XGBoost rondea los 3 segundos, esto es relativamente esperable dado que XGBoos es conocido por ser de rápidos tiempo de ejecución. Con todo, se elige este modelo para las siguientes etapas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Interpretabilidad del modelo con mejores resultados**  \n",
    "De lo anterior, el modelo con mejores resultados es Extra Trees Classifier en AUC, veamos la interpretabilidad del modelo para comprender en base a qué toma decisiones el modelo y así justificar sus resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "xgboost_pipe_interpret= train_model(\n",
    "    model= XGBClassifier(),\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    experiment_name=\"XGBoost: n_estimators=100, random_state=42\",\n",
    "    numeric_features=train_numeric_features\n",
    ")\n",
    "et_model = xgboost_pipe_interpret.named_steps[\"classifier\"]\n",
    "importances = et_model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Feature Importances (XGBoost)\")\n",
    "preprocessed_X_train = xgboost_pipe_interpret.named_steps[\"preprocessor\"].transform(X_train)\n",
    "\n",
    "plt.bar(range(len(importances)), importances[indices], align=\"center\")\n",
    "plt.xticks(range(len(importances)), np.array(feature_names)[indices], rotation=90)\n",
    "plt.xlim([-1, len(importances)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3 optimización de modelo**\n",
    "Se optimizará el modelo seleccionado anteriormente mediante el uso de optuna y técnicas de re entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Primero el entrenamiento normal\n",
    "model_final = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "XGBoost_pipe_retrain = train_model(\n",
    "    model=model_final,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    experiment_name=\"Modelo Incremental\",\n",
    "    numeric_features=train_numeric_features,\n",
    "    save_model_path=\"XGBoost_model_pre_retrain.pkl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t1_iter_1 = pd.read_parquet(DATA_PATH + \"X_t1_new.parquet\").reset_index(drop=True)\n",
    "y_t1_iter_1 = pd.read_parquet(DATA_PATH + \"y_t1.parquet\").reset_index(drop=True)\n",
    "\n",
    "df_t1_iter_1 = pd.concat([X_t1_iter_1, y_t1_iter_1], axis=1)\n",
    "\n",
    "df_t1_iter_1_columns_dropped = df_t1_iter_1.drop(columns=columns_to_drop, inplace=False)\n",
    "\n",
    "X_t1_iter_1 = df_t1_iter_1_columns_dropped.drop(columns='target')\n",
    "y_t1_iter_1 = df_t1_iter_1_columns_dropped['target']\n",
    "\n",
    "X_t1_iter_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits de nuevos datos\n",
    "X_train_iter_1, X_val_iter_1, y_train_iter_1, y_val_iter_1 = train_test_split(\n",
    "    X_t1_iter_1, y_t1_iter_1, train_size=0.7, stratify=y_t1_iter_1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Primero el entrenamiento normal\n",
    "model_final = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "xgboost_pipe_retrain, model_et_retrain = train_model(\n",
    "    model=model_final,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    experiment_name=\"Modelo Incremental\",\n",
    "    numeric_features=train_numeric_features,\n",
    "    save_model_path=\"xgboost_model_pre_retrain.pkl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t1_iter_1 = pd.read_parquet(DATA_PATH + \"X_t1_new.parquet\").reset_index(drop=True)\n",
    "y_t1_iter_1 = pd.read_parquet(DATA_PATH + \"y_t1.parquet\").reset_index(drop=True)\n",
    "\n",
    "df_t1_iter_1 = pd.concat([X_t1_iter_1, y_t1_iter_1], axis=1)\n",
    "\n",
    "df_t1_iter_1_columns_dropped = df_t1_iter_1.drop(columns=columns_to_drop, inplace=False)\n",
    "\n",
    "X_t1_iter_1 = df_t1_iter_1_columns_dropped.drop(columns='target')\n",
    "y_t1_iter_1 = df_t1_iter_1_columns_dropped['target']\n",
    "\n",
    "X_t1_iter_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_to_retrain_XGB(model, X_train, y_train):\n",
    "    import xgboost as xgb\n",
    "    # We use save_model function of xgboost to save the model\n",
    "    model.save_model(\"tmp_model.model\")\n",
    "    # We use the train method of xgboost to update the model\n",
    "    xgb.train(\n",
    "        model.get_params(),\n",
    "        xgb.DMatrix(X_train, label=y_train),\n",
    "        num_boost_round=10,\n",
    "        xgb_model=\"tmp_model.model\",\n",
    "    )\n",
    "    # Remove the temporary model\n",
    "    os.remove(\"tmp_model.model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrained_pipe, retrained_model = retrain_model(\n",
    "    pipeline_or_path=\"xgboost_model_pre_retrain.pkl\",\n",
    "    X=X_t1_iter_1,\n",
    "    y=y_t1_iter_1,\n",
    "    fun_to_update_model=function_to_retrain_XGB,\n",
    "    save_model_path=\"xgboost_model_post_retrain.pkl\",\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    optimize=True,\n",
    "    get_optuna_params=lambda trial: {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 32),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 32),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.1),\n",
    "    },\n",
    "    n_trials=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Crear directorio para guardar los resultados\n",
    "output_dir = \"shap_results_xgb\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "model = xgboost_pipe_retrain.named_steps[\"classifier\"] # Seleccionar el modelo entrenado\n",
    "X_val_transformed = preprocessor_retrain.transform(X_val_iter_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model.predict, X_val_transformed)\n",
    "shap_values = explainer(X_val_transformed) #se demora muchísimo\n",
    "# disminuir el tiempo de ejecución para el explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values.values[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=2  #acorde a lo anterior modificar adecuadamente este indice\n",
    "shap.plots.waterfall(shap_values[idx,:], \n",
    "                     max_display=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenemos los shap values\n",
    "shap_values_abs = np.mean(np.abs(shap_values.values), axis=0)\n",
    "\n",
    "# Obtenemos los nombres ordenados de mayor a menor\n",
    "feature_importance_names = X_train.columns[shap_values_abs.argsort()[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = model.predict(X) # predicción para todo el conjunto de datos\n",
    "\n",
    "for name in feature_importance_names[:3]:\n",
    "    #shap.dependence_plot(name, shap_values.values, X)\n",
    "    shap.plots.scatter(shap_values[:,name], \n",
    "                       color=color, \n",
    "                       xmin=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "shap.summary_plot(shap_values, X_val_iter_1, plot_type=\"dot\", show=False)\n",
    "plt.title(\"SHAP Summary Plot (Dot)\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "shap.summary_plot(shap_values, X_val_iter_1, plot_type=\"violin\", show=False)\n",
    "plt.title(\"SHAP Beeswarm Plot\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "shap.summary_plot(shap_values, X_val_iter_1, plot_type=\"bar\", show=False)\n",
    "plt.title(\"SHAP Bar Plot\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking de experimentos con MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para visualizar la interfaz gráfica con el progreso de los experimentos, se debe ejecutar la siguiente celda que corre el comando `mlflow ui` en el directorio actual, y luego abrir el navegador en la dirección: `localhost:5000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoreo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp, mannwhitneyu\n",
    "from scipy.stats import cramervonmises_2samp\n",
    "\n",
    "def detect_drift(train_data, production_data, features, target_column, method='ks', alpha=0.05): #ks, mw, cv\n",
    "    drift_detected = False\n",
    "\n",
    "    for feature in features:\n",
    "        train_feature = train_data[feature]\n",
    "        prod_feature = production_data[feature]\n",
    "        \n",
    "        if method == 'ks':\n",
    "            # Prueba Kolmogorov-Smirnov\n",
    "            stat, p_value = ks_2samp(train_feature, prod_feature)\n",
    "        elif method == 'mw':\n",
    "            # Prueba Mann-Whitney U\n",
    "            stat, p_value = mannwhitneyu(train_feature, prod_feature, alternative='two-sided')\n",
    "        elif method == 'cv':\n",
    "            # Prueba Cramér-von Mises\n",
    "            stat, p_value = cramervonmises_2samp(train_feature, prod_feature)\n",
    "        \n",
    "        if p_value < alpha:\n",
    "            drift_detected = True\n",
    "            break\n",
    "\n",
    "    target_train = train_data[target_column]\n",
    "    target_prod = production_data[target_column]\n",
    "\n",
    "    if method == 'ks':\n",
    "        target_stat, target_p_value = ks_2samp(target_train, target_prod)\n",
    "    elif method == 'mw':\n",
    "        target_stat, target_p_value = mannwhitneyu(target_train, target_prod, alternative='two-sided')\n",
    "    elif method == 'cv':\n",
    "        target_stat, target_p_value = cramervonmises_2samp(target_train, target_prod)\n",
    "    \n",
    "    target_drift_detected = target_p_value < alpha\n",
    "    \n",
    "    \n",
    "    return drift_detected, target_drift_detected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_check = ['target'] #completar según correspondaaaa\n",
    "data_train = X_t0\n",
    "data_production = X_t1_iter_1\n",
    "for feature in features_to_check:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(data_train[feature], bins=30, alpha=0.5, label='Train Data', density=True)\n",
    "    plt.hist(data_production[feature], bins=30, alpha=0.5, label='Production Data', density=True)\n",
    "    plt.title(f\"Distribuciones de {feature}\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"Densidad\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
