{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equipo: \n",
    "\n",
    "- Nombre de alumno 1: Nicolas Herrera\n",
    "- Nombre de alumno 2: Lucas Carrasco\n",
    "\n",
    "### **Link de repositorio de GitHub:** [Repositorio](https://github.com/vspartamo/MDS7202)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indice\n",
    "\n",
    "1. [Análisis Exploratorio de Datos](#Análisisexploratoriodedatos:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análsis exploratorio de datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza un análisis exploratorio de datos para identificar patrones, tendencias y relaciones en ellos. Esto para comprender mejor las características del conjunto de datos y guiar las siguientes decisiones en el pipeline de modelamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pyarrow pandas scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importan las librerías básicas para trabajar los datos y visualizarlos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data/\"\n",
    "X_t0 = pd.read_parquet(DATA_PATH + \"X_t0.parquet\")\n",
    "y_t0 = pd.read_parquet(DATA_PATH + \"y_t0.parquet\")\n",
    "\n",
    "df_t0 = pd.concat([X_t0, y_t0], axis=1)\n",
    "df_t0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t0.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se entrega una breve descripción de cada una de las 78 columnas:\n",
    "#### **Datos generales del wallet**\n",
    "- **`borrow_block_number`**: Número del bloque en el que ocurrió el préstamo más reciente asociado al monedero.\n",
    "- **`borrow_timestamp`**: Marca de tiempo (timestamp) del préstamo más reciente.\n",
    "- **`wallet_address`**: Dirección del monedero que identifica al usuario.\n",
    "- **`first_tx_timestamp`**: Timestamp de la primera transacción registrada para este monedero.\n",
    "- **`last_tx_timestamp`**: Timestamp de la última transacción registrada.\n",
    "- **`wallet_age`**: Tiempo total desde la primera transacción hasta la fecha actual, generalmente en días o meses.\n",
    "\n",
    "#### **Estadísticas de transacciones**\n",
    "- **`incoming_tx_count`**: Número total de transacciones entrantes al monedero.\n",
    "- **`outgoing_tx_count`**: Número total de transacciones salientes desde el monedero.\n",
    "- **`net_incoming_tx_count`**: Diferencia entre las transacciones entrantes y salientes.\n",
    "- **`total_gas_paid_eth`**: Cantidad total de gas pagado en ETH por todas las transacciones.\n",
    "- **`avg_gas_paid_per_tx_eth`**: Promedio de gas pagado por transacción, expresado en ETH.\n",
    "\n",
    "#### **Datos sobre transacciones riesgosas**\n",
    "- **`risky_tx_count`**: Número de transacciones clasificadas como riesgosas.\n",
    "- **`risky_unique_contract_count`**: Número de contratos únicos involucrados en transacciones riesgosas.\n",
    "- **`risky_first_tx_timestamp`**: Timestamp de la primera transacción riesgosa.\n",
    "- **`risky_last_tx_timestamp`**: Timestamp de la última transacción riesgosa.\n",
    "- **`risky_first_last_tx_timestamp_diff`**: Diferencia temporal entre la primera y la última transacción riesgosa.\n",
    "- **`risky_sum_outgoing_amount_eth`**: Suma de ETH enviados en transacciones riesgosas.\n",
    "\n",
    "#### **Estadísticas de ETH en el monedero**\n",
    "- **`outgoing_tx_sum_eth`**: Suma total de ETH enviados en todas las transacciones salientes.\n",
    "- **`incoming_tx_sum_eth`**: Suma total de ETH recibidos en todas las transacciones entrantes.\n",
    "- **`outgoing_tx_avg_eth`**: Promedio de ETH enviados por transacción saliente.\n",
    "- **`incoming_tx_avg_eth`**: Promedio de ETH recibidos por transacción entrante.\n",
    "- **`max_eth_ever`**: Máximo balance de ETH alcanzado en el monedero.\n",
    "- **`min_eth_ever`**: Mínimo balance de ETH registrado en el monedero.\n",
    "- **`total_balance_eth`**: Balance actual del monedero en ETH.\n",
    "- **`risk_factor`**: Indicador del nivel de riesgo asociado al monedero, basado en algún modelo de análisis.\n",
    "\n",
    "#### **Estadísticas de préstamos y colaterales**\n",
    "- **`total_collateral_eth`**: Suma total de ETH utilizados como colateral.\n",
    "- **`total_collateral_avg_eth`**: Promedio de ETH usados como colateral por préstamo.\n",
    "- **`total_available_borrows_eth`**: Monto total de ETH disponible para préstamo.\n",
    "- **`total_available_borrows_avg_eth`**: Promedio de ETH disponibles para préstamo.\n",
    "- **`avg_weighted_risk_factor`**: Factor de riesgo ponderado promedio.\n",
    "- **`risk_factor_above_threshold_daily_count`**: Número de días en los que el factor de riesgo estuvo por encima de un umbral predefinido.\n",
    "- **`avg_risk_factor`**: Promedio del factor de riesgo del monedero.\n",
    "- **`max_risk_factor`**: Máximo valor del factor de riesgo registrado.\n",
    "- **`borrow_amount_sum_eth`**: Suma total de ETH prestados.\n",
    "- **`borrow_amount_avg_eth`**: Promedio de ETH prestados por transacción.\n",
    "- **`borrow_count`**: Número total de transacciones de préstamo.\n",
    "- **`repay_amount_sum_eth`**: Suma total de ETH devueltos.\n",
    "- **`repay_amount_avg_eth`**: Promedio de ETH devueltos por transacción.\n",
    "- **`repay_count`**: Número total de transacciones de devolución.\n",
    "- **`borrow_repay_diff_eth`**: Diferencia entre ETH prestados y devueltos.\n",
    "\n",
    "#### **Estadísticas de depósitos y retiros**\n",
    "- **`deposit_count`**: Número de transacciones de depósito realizadas.\n",
    "- **`deposit_amount_sum_eth`**: Suma total de ETH depositados.\n",
    "- **`time_since_first_deposit`**: Tiempo transcurrido desde el primer depósito.\n",
    "- **`withdraw_amount_sum_eth`**: Suma total de ETH retirados.\n",
    "- **`withdraw_deposit_diff_if_positive_eth`**: Diferencia positiva entre ETH retirados y depositados.\n",
    "- **`liquidation_count`**: Número de veces que el monedero fue liquidado.\n",
    "- **`time_since_last_liquidated`**: Tiempo transcurrido desde la última liquidación.\n",
    "- **`liquidation_amount_sum_eth`**: Suma total de ETH liquidados.\n",
    "\n",
    "#### **Indicadores del mercado**\n",
    "- **`market_adx`, `market_adxr`, `market_apo`, etc.**: Indicadores técnicos basados en análisis del mercado, como fuerza direccional (ADX), Momentum, osciladores (Aroon), volatilidad (ATR), fuerza relativa (CCI), entre otros. Estos se usan comúnmente para evaluar tendencias o comportamientos del mercado.\n",
    "\n",
    "#### **Estadísticas adicionales**\n",
    "- **`unique_borrow_protocol_count`**: Número de protocolos de préstamos únicos utilizados.\n",
    "- **`unique_lending_protocol_count`**: Número de protocolos de préstamos ofrecidos.\n",
    "- **`target`**: Variable objetivo, posiblemente para un modelo de predicción (como riesgo de impago o clasificación)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Limpieza de los datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se remueve las columnas que corresponden a identificadores o no aportan información relevante al problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    'borrow_block_number',\n",
    "    'wallet_address',\n",
    "    'borrow_timestamp',\n",
    "    'first_tx_timestamp',\n",
    "    'last_tx_timestamp',\n",
    "    'risky_first_tx_timestamp',\n",
    "    'risky_last_tx_timestamp',\n",
    "    'unique_borrow_protocol_count',\n",
    "    'unique_lending_protocol_count',\n",
    "]\n",
    "\n",
    "df_t0_columns_dropped = df_t0.drop(columns=columns_to_drop, inplace=False)\n",
    "\n",
    "df_t0_columns_dropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vemos la existencia de nulos\n",
    "sum(df_t0_columns_dropped.isna().sum() > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t0_columns_dropped.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identificamos las columnas numericas, no hay categoricas\n",
    "numeric_features = df_t0_columns_dropped.select_dtypes(include=['int64', 'float64']).columns\n",
    "train_numeric_features = numeric_features.drop('target')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estadísticas generales de las columnas numéricas\n",
    "print(df_t0_columns_dropped.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver el desbalance de clases en la columna target \n",
    "df_t0_columns_dropped['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_numeric_histograms(df, numeric_features, n_cols=6, title=\"Análisis Univariado de las Variables Numéricas\"):\n",
    "    \"\"\"\n",
    "    Función para graficar histogramas de variables numéricas en un DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): El DataFrame que contiene las variables a graficar.\n",
    "    - numeric_features (list): Lista de nombres de las columnas numéricas a graficar.\n",
    "    - n_cols (int): Número de columnas en la cuadrícula de subplots.\n",
    "    - title (str): Título del gráfico.\n",
    "    \"\"\"\n",
    "    # Crear subplots dinámicamente según la cantidad de variables\n",
    "    n_rows = len(numeric_features) // n_cols + (1 if len(numeric_features) % n_cols != 0 else 0)\n",
    "    fig = make_subplots(rows=n_rows, cols=n_cols, subplot_titles=numeric_features)\n",
    "\n",
    "    for idx, col in enumerate(numeric_features):\n",
    "        row_idx = idx // n_cols + 1\n",
    "        col_idx = idx % n_cols + 1\n",
    "\n",
    "        hist = go.Histogram(x=df.loc[:, col], name=col, histnorm=\"probability\")\n",
    "        fig.add_trace(hist, row=row_idx, col=col_idx)\n",
    "    fig.update_layout(\n",
    "        height=400 * n_rows,  \n",
    "        title_text=title,\n",
    "        showlegend=False,\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = len(numeric_features) // 3\n",
    "numeric_features_1 = numeric_features[:chunk_size]\n",
    "numeric_features_2 = numeric_features[chunk_size:2*chunk_size]\n",
    "numeric_features_3 = numeric_features[2*chunk_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numeric_histograms(df_t0_columns_dropped, numeric_features_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numeric_histograms(df_t0_columns_dropped, numeric_features_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numeric_histograms(df_t0_columns_dropped, numeric_features_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se estudia la correlación de features\n",
    "correlation_matrix = df_t0_columns_dropped[numeric_features].corr()\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se obtiene las 10 variables más correlacionadas con 'target', pero su correlación es muy baja para considerarlas relevantes\n",
    "correlations = df_t0_columns_dropped.corr(numeric_only=True)['target'].dropna()\n",
    "correlations_sorted = correlations.abs().sort_values(ascending=False)\n",
    "top_10_correlated_variables = correlations_sorted.index[1:11]  \n",
    "print(\"10 variables más correlacionadas con 'target':\")\n",
    "print(correlations[top_10_correlated_variables])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t0_columns_dropped.columns[np.argmax(vars)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se estudiará la distribución de las 3 variables más correlacionadas con 'target'\n",
    "top_3_correlated_variables = correlations_sorted.index[1:4]\n",
    "def plot_distributions_grid_stacked(df, max_plots_per_row=3, hue=None, normalize=False, clip_percentiles=(1, 99)):\n",
    "    \"\"\"\n",
    "    Plots a grid of histograms with stacked bars and overlaid KDE lines for each column in a DataFrame.\n",
    "    Each plot has its own scale for both X and Y axes.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing the data to plot.\n",
    "    - max_plots_per_row (int): Maximum number of plots per row.\n",
    "    - hue (str): Column name to use for coloring the plots (optional).\n",
    "    - normalize (bool): Whether to normalize histograms for comparison.\n",
    "    - clip_percentiles (tuple): Percentiles to clip the data for better visualization.\n",
    "    \"\"\"\n",
    "    # Calculate the grid dimensions\n",
    "    num_columns = len(df.columns)\n",
    "    if hue in df.columns:\n",
    "        num_columns -= 1  # Exclude hue column from plotting\n",
    "    \n",
    "    num_rows = int(np.ceil(num_columns / max_plots_per_row))\n",
    "    \n",
    "    fig, axes = plt.subplots(num_rows, max_plots_per_row, figsize=(5 * max_plots_per_row, 4 * num_rows))\n",
    "    axes = axes.flatten()  # Flatten to make indexing easier\n",
    "    \n",
    "    columns_to_plot = [col for col in df.columns if col != hue]\n",
    "    \n",
    "    # Plot each column\n",
    "    for i, column in enumerate(columns_to_plot):\n",
    "        ax = axes[i]\n",
    "        # Clip data to remove outliers\n",
    "        lower, upper = np.percentile(df[column], clip_percentiles)\n",
    "        clipped_data = df[(df[column] >= lower) & (df[column] <= upper)]\n",
    "        \n",
    "        if hue and hue in df.columns:\n",
    "            # Plot stacked histogram\n",
    "            sns.histplot(data=clipped_data, x=column, hue=hue, kde=False, \n",
    "                         stat='density' if normalize else 'count', ax=ax, element=\"bars\", multiple=\"stack\")\n",
    "            # Add overlaid KDE lines\n",
    "            sns.kdeplot(data=clipped_data, x=column, hue=hue, ax=ax, common_norm=normalize, legend=False)\n",
    "        else:\n",
    "            sns.histplot(clipped_data[column], kde=True, stat='density' if normalize else 'count', ax=ax)\n",
    "        \n",
    "        ax.set_title(column)\n",
    "        ax.set_xlim(lower, upper)  # Set x-axis limits to clipped range\n",
    "    \n",
    "    # Remove unused subplots\n",
    "    for j in range(len(columns_to_plot), len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "to_plot_cols = top_3_correlated_variables.tolist() + ['target']\n",
    "plot_distributions_grid_stacked(df_t0_columns_dropped[to_plot_cols], max_plots_per_row=3, hue='target', normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen y conclusiones\n",
    "\n",
    "**Hay q escribirlo bonito y ver si hay algo más**\n",
    "No hay nulos ni blancos\n",
    "\n",
    "Clases balanceadas, por lo que si hay un desbalance de clase podría ser un target drift, esto sirve para responder la ultima parte\n",
    "\n",
    "no hay columnas categóricas, por lo que no se implementa one hot encoding ni na del estilo\n",
    "\n",
    "se grafica la distribución de todas las variables, hay muchísimas con poca varianza y centradas en 0, por lo que no se espera que aporten información, además, las que sí tienen más varianza no tienen distribución normal ni aproximada, por lo que para escalar se elegirá minmax scaler, hay q ver si hay outliers, según yo, no\n",
    "\n",
    "Para evitar columnas redundantes, se grafica la matriz de correlación y luego se observa que hay variables con una correlación relativamente alta, se obtienen y se elimina una\n",
    "\n",
    "También se busca las variables más correlacionadas a la columna target, no se obtiene valores muy altos pero aún así se grafica las 3 con más alta correlación, en un gráfico donde se muestra ambas distribuciones en conjunto para comparar visualmente los comportamientos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Entrenamiento de modelos de ML**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de un primer acercamiento a los datos, se procede a realizar un preprocesamiento de estos:\n",
    "   - Re-escalamiento de columnas.  \n",
    "   - Reducción de dimensionalidad.\n",
    "   - Otras transformaciones relevantes según los datos disponibles.  \n",
    "\n",
    "\n",
    "**Eso es lo recomendado, no estoy seguro si lo hicimos todo o aún no**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **División de los datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Preparación de pipelines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', MinMaxScaler())   # se escoge minmax scaler dado que los datos no tienen una distribución normal en ninguna feature\n",
    "\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features.drop('target'))\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_t0_columns_dropped.drop(columns='target')\n",
    "y = df_t0_columns_dropped['target']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size=0.7, stratify=y_t0, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, train_size=0.5, stratify=y_temp, random_state=42)\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "X_val = pd.DataFrame(X_val, columns=X.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X.columns)\n",
    "\n",
    "y_train = np.array(y_train).ravel()\n",
    "y_val = np.array(y_val).ravel()\n",
    "y_test = np.array(y_test).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "Se implementa un baseline, para ello se elige Decision Tree como modelo y se crea una función que englobe los Pipelines y retorne métricas de interés. En lo siguiente, consideraremos como métrica objetivo la **INSERTAR MÉTRICA** (DE MOMENTO CONSIDERO EL AUC, PUEDE SER OTRA)\n",
    "\n",
    "#### **Decision Tree Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import os\n",
    "import pickle\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def train_or_update_model(\n",
    "    model: BaseEstimator | str,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series | np.ndarray,\n",
    "    X_val: pd.DataFrame,\n",
    "    y_val: pd.Series | np.ndarray,\n",
    "    experiment_name: str,\n",
    "    numeric_features: pd.Index | List[str],\n",
    "    categorical_features: pd.Index | List[str],\n",
    "    save_model_path: str = \"\",\n",
    "    scaler=None,\n",
    "    use_pca=False,\n",
    "    pca_components=50,\n",
    "    retrain=False,\n",
    "    preprocessor=None,\n",
    "    fun_to_update_model=None,\n",
    "    use_optuna=False,\n",
    "    n_trials=50,\n",
    "):\n",
    "    \"\"\"\n",
    "    Entrena o reentrena un modelo con pipeline, manejando incrementalidad y optimización de hiperparámetros.\n",
    "\n",
    "    Args:\n",
    "    - model: Clasificador base (debe soportar `partial_fit` si `retrain=True`) o ruta a un modelo guardado.\n",
    "    - X_train, y_train: Datos de entrenamiento.\n",
    "    - X_val, y_val: Datos de validación.\n",
    "    - experiment_name: Nombre del experimento en MLflow.\n",
    "    - numeric_features: Lista de columnas numéricas.\n",
    "    - categorical_features: Lista de columnas categóricas.\n",
    "    - save_model_path: Ruta para guardar el modelo.\n",
    "    - scaler: Escalador para características numéricas (Default: StandardScaler).\n",
    "    - use_pca: Si True, aplica PCA a las características numéricas.\n",
    "    - pca_components: Número de componentes principales para PCA.\n",
    "    - retrain: Si True, reentrena un modelo existente.\n",
    "    - preprocessor: Preprocesador ya ajustado para reutilizar.\n",
    "    - fun_to_update_model (callable): Función para actualizar el modelo. Recibe (modelo, X, y).\n",
    "    - use_optuna: Si True, utiliza Optuna para optimizar hiperparámetros.\n",
    "    - n_trials: Número de iteraciones para Optuna.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple: (pipeline completo, clasificador base).\n",
    "    \"\"\"\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "    # Crear pipeline de preprocesamiento si no existe\n",
    "    if preprocessor is None:\n",
    "        numeric_transformer_steps = [(\"scaler\", scaler)]\n",
    "        if use_pca:\n",
    "            numeric_transformer_steps.append((\"pca\", PCA(n_components=pca_components)))\n",
    "        numeric_transformer = Pipeline(steps=numeric_transformer_steps)\n",
    "\n",
    "        categorical_transformer = Pipeline(\n",
    "            steps=[(\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))]\n",
    "        )\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"num\", numeric_transformer, numeric_features),\n",
    "                (\"cat\", categorical_transformer, categorical_features),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    # Optimización de hiperparámetros con Optuna\n",
    "    if use_optuna:\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "                \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            }\n",
    "\n",
    "            temp_model = model.set_params(**params)\n",
    "            pipeline = Pipeline(\n",
    "                steps=[(\"preprocessor\", preprocessor), (\"classifier\", temp_model)]\n",
    "            )\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            y_val_pred = pipeline.predict(X_val)\n",
    "            val_auc = roc_auc_score(y_val, pipeline.predict_proba(X_val)[:, 1])\n",
    "            return val_auc\n",
    "\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective, n_trials=n_trials)\n",
    "        best_params = study.best_params\n",
    "        model.set_params(**best_params)\n",
    "\n",
    "    pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"classifier\", model)])\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Registrar el nombre del modelo y parámetros\n",
    "        mlflow.log_param(\"model_name\", model.__class__.__name__)\n",
    "        if use_optuna:\n",
    "            mlflow.log_params(best_params)\n",
    "\n",
    "        # Evaluar el modelo\n",
    "        y_train_pred = pipeline.predict(X_train)\n",
    "        y_val_pred = pipeline.predict(X_val)\n",
    "\n",
    "        train_auc = roc_auc_score(y_train, pipeline.predict_proba(X_train)[:, 1])\n",
    "        val_auc = roc_auc_score(y_val, pipeline.predict_proba(X_val)[:, 1])\n",
    "\n",
    "        train_metrics = {\n",
    "            \"accuracy\": accuracy_score(y_train, y_train_pred),\n",
    "            \"precision\": precision_score(y_train, y_train_pred, average=\"binary\"),\n",
    "            \"recall\": recall_score(y_train, y_train_pred, average=\"binary\"),\n",
    "            \"f1_score\": f1_score(y_train, y_train_pred, average=\"binary\"),\n",
    "            \"auc\": train_auc,\n",
    "        }\n",
    "        mlflow.log_metrics({f\"train_{k}\": v for k, v in train_metrics.items()})\n",
    "\n",
    "        val_metrics = {\n",
    "            \"accuracy\": accuracy_score(y_val, y_val_pred),\n",
    "            \"precision\": precision_score(y_val, y_val_pred, average=\"binary\"),\n",
    "            \"recall\": recall_score(y_val, y_val_pred, average=\"binary\"),\n",
    "            \"f1_score\": f1_score(y_val, y_val_pred, average=\"binary\"),\n",
    "            \"auc\": val_auc,\n",
    "        }\n",
    "        mlflow.log_metrics({f\"val_{k}\": v for k, v in val_metrics.items()})\n",
    "\n",
    "        # Guardar el modelo\n",
    "        if save_model_path:\n",
    "            with open(save_model_path, \"wb\") as f:\n",
    "                pickle.dump(pipeline, f)\n",
    "\n",
    "        # Imprimir métricas\n",
    "        print(f\"Model to train/retrain: {model.__class__.__name__}\")\n",
    "        print(\"\\n--- Training Metrics ---\")\n",
    "        for k, v in train_metrics.items():\n",
    "            print(f\"{k.capitalize()}: {v}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(y_train, y_train_pred))\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "        print(\"\\n--- Validation Metrics ---\")\n",
    "        for k, v in val_metrics.items():\n",
    "            print(f\"{k.capitalize()}: {v}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_matrix(y_val, y_val_pred))\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "    return pipeline, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import os\n",
    "import pickle\n",
    "from typing import List\n",
    "\n",
    "def create_preprocessor(numeric_features, categorical_features, scaler, use_pca, pca_components):\n",
    "    \"\"\"Crea el preprocesador basado en las características numéricas y categóricas.\"\"\"\n",
    "    numeric_transformer_steps = [(\"scaler\", scaler)]\n",
    "    if use_pca:\n",
    "        numeric_transformer_steps.append((\"pca\", PCA(n_components=pca_components)))\n",
    "    numeric_transformer = Pipeline(steps=numeric_transformer_steps)\n",
    "\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[(\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))]\n",
    "    )\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features),\n",
    "        ]\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "def optimize_hyperparameters(model, X_train, y_train, X_val, y_val, preprocessor, n_trials):\n",
    "    \"\"\"Optimiza los hiperparámetros del modelo usando Optuna.\"\"\"\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        }\n",
    "\n",
    "        temp_model = model.set_params(**params)\n",
    "        pipeline = Pipeline(\n",
    "            steps=[(\"preprocessor\", preprocessor), (\"classifier\", temp_model)]\n",
    "        )\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        val_auc = roc_auc_score(y_val, pipeline.predict_proba(X_val)[:, 1])\n",
    "        return val_auc\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    return study.best_params\n",
    "\n",
    "def evaluate_model(pipeline, X, y, dataset_name=\"train\"):\n",
    "    \"\"\"Calcula y retorna las métricas del modelo.\"\"\"\n",
    "    y_pred = pipeline.predict(X)\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y, y_pred),\n",
    "        \"precision\": precision_score(y, y_pred, average=\"binary\"),\n",
    "        \"recall\": recall_score(y, y_pred, average=\"binary\"),\n",
    "        \"f1_score\": f1_score(y, y_pred, average=\"binary\"),\n",
    "        \"auc\": roc_auc_score(y, pipeline.predict_proba(X)[:, 1]),\n",
    "    }\n",
    "    print(f\"\\n--- {dataset_name.capitalize()} Metrics ---\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k.capitalize()}: {v}\")\n",
    "    return metrics\n",
    "\n",
    "def save_model(pipeline, save_model_path):\n",
    "    \"\"\"Guarda el modelo en la ruta especificada.\"\"\"\n",
    "    if save_model_path:\n",
    "        with open(save_model_path, \"wb\") as f:\n",
    "            pickle.dump(pipeline, f)\n",
    "\n",
    "def train_pipeline(model, preprocessor, X_train, y_train):\n",
    "    \"\"\"Entrena el pipeline completo.\"\"\"\n",
    "    pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"classifier\", model)])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    return pipeline\n",
    "\n",
    "def train_or_update_model(\n",
    "    model: BaseEstimator | str,\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series | np.ndarray,\n",
    "    X_val: pd.DataFrame,\n",
    "    y_val: pd.Series | np.ndarray,\n",
    "    experiment_name: str,\n",
    "    numeric_features: pd.Index | List[str],\n",
    "    categorical_features: pd.Index | List[str],\n",
    "    save_model_path: str = \"\",\n",
    "    scaler=None,\n",
    "    use_pca=False,\n",
    "    pca_components=50,\n",
    "    retrain=False,\n",
    "    preprocessor=None,\n",
    "    fun_to_update_model=None,\n",
    "    use_optuna=False,\n",
    "    n_trials=50,\n",
    "):\n",
    "    \"\"\"\n",
    "    Entrena o reentrena un modelo con pipeline, manejando incrementalidad y optimización de hiperparámetros.\n",
    "\n",
    "    Args:\n",
    "    - model: Clasificador base (debe soportar `partial_fit` si `retrain=True`) o ruta a un modelo guardado.\n",
    "    - X_train, y_train: Datos de entrenamiento.\n",
    "    - X_val, y_val: Datos de validación.\n",
    "    - experiment_name: Nombre del experimento en MLflow.\n",
    "    - numeric_features: Lista de columnas numéricas.\n",
    "    - categorical_features: Lista de columnas categóricas.\n",
    "    - save_model_path: Ruta para guardar el modelo.\n",
    "    - scaler: Escalador para características numéricas (Default: StandardScaler).\n",
    "    - use_pca: Si True, aplica PCA a las características numéricas.\n",
    "    - pca_components: Número de componentes principales para PCA.\n",
    "    - retrain: Si True, reentrena un modelo existente.\n",
    "    - preprocessor: Preprocesador ya ajustado para reutilizar.\n",
    "    - fun_to_update_model (callable): Función para actualizar el modelo. Recibe (modelo, X, y).\n",
    "    - use_optuna: Si True, utiliza Optuna para optimizar hiperparámetros.\n",
    "    - n_trials: Número de iteraciones para Optuna.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple: (pipeline completo, clasificador base).\n",
    "    \"\"\"\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "    if preprocessor is None:\n",
    "        preprocessor = create_preprocessor(\n",
    "            numeric_features, categorical_features, scaler, use_pca, pca_components\n",
    "        )\n",
    "\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    if use_optuna:\n",
    "        best_params = optimize_hyperparameters(\n",
    "            model, X_train, y_train, X_val, y_val, preprocessor, n_trials\n",
    "        )\n",
    "        model.set_params(**best_params)\n",
    "\n",
    "    pipeline = train_pipeline(model, preprocessor, X_train, y_train)\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"model_name\", model.__class__.__name__)\n",
    "        if use_optuna:\n",
    "            mlflow.log_params(best_params)\n",
    "\n",
    "        train_metrics = evaluate_model(pipeline, X_train, y_train, \"train\")\n",
    "        val_metrics = evaluate_model(pipeline, X_val, y_val, \"validation\")\n",
    "\n",
    "        mlflow.log_metrics({f\"train_{k}\": v for k, v in train_metrics.items()})\n",
    "        mlflow.log_metrics({f\"val_{k}\": v for k, v in val_metrics.items()})\n",
    "\n",
    "        save_model(pipeline, save_model_path)\n",
    "\n",
    "    return pipeline, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree_pipe, _ = train_or_update_model(\n",
    "    model=DecisionTreeClassifier(),\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    experiment_name=\"Basic Decision Tree\",\n",
    "    numeric_features=train_numeric_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the tree\n",
    "# tree = decision_tree_pipe.named_steps['classifier']\n",
    "## get the feature importances\n",
    "# importances = tree.feature_importances_\n",
    "## get the feature names\n",
    "# feature_names = X_train.columns\n",
    "# sort them\n",
    "# indices = np.argsort(importances)[::-1]#\n",
    "\n",
    "## plot the feature importances\n",
    "##plt.figure(figsize=(12, 6))\n",
    "##plt.title(\"Feature importances\")\n",
    "##preprocessed_X_train = decision_tree_pipe.named_steps['preprocessor'].transform(X_train)\n",
    "##plt.bar(range(preprocessed_X_train.shape[1]), importances[indices],\n",
    "##        align=\"center\")\n",
    "##\n",
    "# plt.xticks(range(preprocessed_X_train.shape[1]), np.array(feature_names)[indices], rotation=90)\n",
    "# plt.xlim([-1, preprocessed_X_train.shape[1]])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de ML\n",
    "\n",
    "Hecho el baseline, se implementará Random Forest, XGBoost, CatBoost y LGBoost.\n",
    "\n",
    "##### **Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest_pipe, _ = train_or_update_model(\n",
    "    model=RandomForestClassifier(),\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    experiment_name=\"Basic Random Forest\",\n",
    "    numeric_features=train_numeric_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the tree\n",
    "# tree = random_forest_pipe.named_steps['classifier']\n",
    "## get the feature importances\n",
    "# importances = tree.feature_importances_\n",
    "## get the feature names\n",
    "# feature_names = X_train.columns\n",
    "## sort them\n",
    "# indices = np.argsort(importances)[::-1]\n",
    "#\n",
    "## plot the feature importances\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.title(\"Feature importances\")\n",
    "# preprocessed_X_train = random_forest_pipe.named_steps['preprocessor'].transform(X_train)\n",
    "# plt.bar(range(preprocessed_X_train.shape[1]), importances[indices],\n",
    "#        align=\"center\")\n",
    "#\n",
    "# plt.xticks(range(preprocessed_X_train.shape[1]), np.array(feature_names)[indices], rotation=90)\n",
    "# plt.xlim([-1, preprocessed_X_train.shape[1]])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Entrenar el modelo con XGBoost\n",
    "xgboost_pipe, _ = train_or_update_model(\n",
    "    model=XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\"),\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    experiment_name=\"Basic XGBoost\",\n",
    "    numeric_features=train_numeric_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtener el modelo entrenado\n",
    "# xgb_model = xgboost_pipe.named_steps['classifier']\n",
    "#\n",
    "## Obtener importancias de características\n",
    "# importances = xgb_model.feature_importances_\n",
    "# feature_names = X_train.columns\n",
    "# indices = np.argsort(importances)[::-1]\n",
    "#\n",
    "## Graficar la importancia de las características\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.title(\"Feature Importances (XGBoost)\")\n",
    "# preprocessed_X_train = xgboost_pipe.named_steps['preprocessor'].transform(X_train)\n",
    "# plt.bar(range(preprocessed_X_train.shape[1]), importances[indices], align=\"center\")\n",
    "# plt.xticks(range(preprocessed_X_train.shape[1]), np.array(feature_names)[indices], rotation=90)\n",
    "# plt.xlim([-1, preprocessed_X_train.shape[1]])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **LGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Entrenar el modelo con LightGBM\n",
    "lightgbm_pipe, _ = train_or_update_model(\n",
    "    model=LGBMClassifier(),\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    experiment_name=\"Basic LightGBM\",\n",
    "    numeric_features=train_numeric_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtener el modelo entrenado\n",
    "# lgb_model = lightgbm_pipe.named_steps['classifier']\n",
    "#\n",
    "## Obtener importancias de características\n",
    "# importances = lgb_model.feature_importances_\n",
    "# feature_names = X_train.columns\n",
    "# indices = np.argsort(importances)[::-1]\n",
    "#\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.title(\"Feature Importances (LightGBM)\")\n",
    "# preprocessed_X_train = lightgbm_pipe.named_steps['preprocessor'].transform(X_train)\n",
    "# plt.bar(range(preprocessed_X_train.shape[1]), importances[indices], align=\"center\")\n",
    "# plt.xticks(range(preprocessed_X_train.shape[1]), np.array(feature_names)[indices], rotation=90)\n",
    "# plt.xlim([-1, preprocessed_X_train.shape[1]])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **CatBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "catboost_pipe, _ = train_or_update_model(\n",
    "    model=CatBoostClassifier(verbose=0),\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    experiment_name=\"Basic CatBoost\",\n",
    "    numeric_features=train_numeric_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_model = catboost_pipe.named_steps['classifier']\n",
    "#\n",
    "# importances = cat_model.feature_importances_\n",
    "# feature_names = X_train.columns\n",
    "# indices = np.argsort(importances)[::-1]\n",
    "#\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.title(\"Feature Importances (CatBoost)\")\n",
    "# preprocessed_X_train = catboost_pipe.named_steps['preprocessor'].transform(X_train)\n",
    "# plt.bar(range(preprocessed_X_train.shape[1]), importances[indices], align=\"center\")\n",
    "# plt.xticks(range(preprocessed_X_train.shape[1]), np.array(feature_names)[indices], rotation=90)\n",
    "# plt.xlim([-1, preprocessed_X_train.shape[1]])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Interpretabilidad del modelo con mejores resultados**  \n",
    "De lo anterior, el modelo con mejores resultados es Extra Trees Classifier en AUC, veamos la interpretabilidad del modelo para comprender en base a qué toma decisiones el modelo y así justificar sus resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "extra_tree_pipe, _ = train_or_update_model(\n",
    "    model=ExtraTreesClassifier(n_estimators=100, random_state=42),\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    experiment_name=\"Extra Trees: n_estimators=100, random_state=42\",\n",
    "    numeric_features=train_numeric_features\n",
    ")\n",
    "et_model = extra_tree_pipe.named_steps[\"classifier\"]\n",
    "importances = et_model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Feature Importances (Extra Trees)\")\n",
    "preprocessed_X_train = extra_tree_pipe.named_steps[\"preprocessor\"].transform(X_train)\n",
    "\n",
    "plt.bar(range(len(importances)), importances[indices], align=\"center\")\n",
    "plt.xticks(range(len(importances)), np.array(feature_names)[indices], rotation=90)\n",
    "plt.xlim([-1, len(importances)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrega Etapa 2 \n",
    "\n",
    "A continuación se realiza la optimización del mejor modelo encontrado en la primera entrega del proyecto, para ello se optimizará los hiperparámetros del modelo y de los preprocesadores utilizados.\n",
    "\n",
    "## Optimización de modelos\n",
    "\n",
    "Se utilizará `Optuna` para la búsqueda de hiperparámetros y también se probarán técnicas de selección de atributos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Primero el entrenamiento normal\n",
    "model_final = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "extra_tree_pipe_retrain, model_et_retrain = train_or_update_model(\n",
    "    model=model_final,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    experiment_name=\"Modelo Incremental\",\n",
    "    numeric_features=train_numeric_features,\n",
    "    save_model_path=\"extra_tree_model_pre_retrain.pkl\",\n",
    ")\n",
    "\n",
    "preprocessor_retrain = extra_tree_pipe_retrain.named_steps['preprocessor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t1_iter_1 = pd.read_parquet(DATA_PATH + \"X_t1_new.parquet\").reset_index(drop=True)\n",
    "y_t1_iter_1 = pd.read_parquet(DATA_PATH + \"y_t1.parquet\").reset_index(drop=True)\n",
    "\n",
    "df_t1_iter_1 = pd.concat([X_t1_iter_1, y_t1_iter_1], axis=1)\n",
    "\n",
    "df_t1_iter_1_columns_dropped = df_t1_iter_1.drop(columns=columns_to_drop, inplace=False)\n",
    "\n",
    "X_t1_iter_1 = df_t1_iter_1_columns_dropped.drop(columns='target')\n",
    "y_t1_iter_1 = df_t1_iter_1_columns_dropped['target']\n",
    "\n",
    "X_t1_iter_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits de nuevos datos\n",
    "X_train_iter_1, X_val_iter_1, y_train_iter_1, y_val_iter_1 = train_test_split(\n",
    "    X_t1_iter_1, y_t1_iter_1, train_size=0.7, stratify=y_t1_iter_1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_to_retrain_XGB(model_path, X_train, y_train):\n",
    "    import xgboost as xgb\n",
    "\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        model: xgb.XGBClassifier = pickle.load(f)\n",
    "    # We use save_model function of xgboost to save the model\n",
    "    model.save_model(\"tmp_model.model\")\n",
    "    # We use the train method of xgboost to update the model\n",
    "    xgb.train(\n",
    "        model.get_params(),\n",
    "        xgb.DMatrix(X_train, label=y_train),\n",
    "        num_boost_round=10,\n",
    "        xgb_model=\"tmp_model.model\",\n",
    "    )\n",
    "    # Remove the temporary model\n",
    "    os.remove(\"tmp_model.model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrained_pipe, retrained_model = train_or_update_model(\n",
    "    model=\"extra_tree_model_pre_retrain.pkl\",\n",
    "    X_train=X_train_iter_1,\n",
    "    y_train=y_train_iter_1,\n",
    "    X_val=X_val_iter_1,\n",
    "    y_val=y_val_iter_1,\n",
    "    experiment_name=\"Modelo Incremental\",\n",
    "    numeric_features=train_numeric_features,\n",
    "    retrain=True,\n",
    "    save_model_path=\"extra_tree_model_post_retrain.pkl\",\n",
    "    preprocessor=preprocessor_retrain,\n",
    "    fun_to_update_model=function_to_retrain_XGB,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Crear directorio para guardar los resultados\n",
    "output_dir = \"shap_results_xgb\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "model = extra_tree_pipe_retrain.named_steps[\"classifier\"] # Seleccionar el modelo entrenado\n",
    "X_val_transformed = preprocessor_retrain.transform(X_val_iter_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model.predict, X_val_transformed)\n",
    "shap_values = explainer(X_val_transformed) #se demora muchísimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values.values[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=2  #acorde a lo anterior modificar adecuadamente este indice\n",
    "shap.plots.waterfall(shap_values[idx,:], \n",
    "                     max_display=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenemos los shap values\n",
    "shap_values_abs = np.mean(np.abs(shap_values.values), axis=0)\n",
    "\n",
    "# Obtenemos los nombres ordenados de mayor a menor\n",
    "feature_importance_names = X_train.columns[shap_values_abs.argsort()[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = model.predict(X) # predicción para todo el conjunto de datos\n",
    "\n",
    "for name in feature_importance_names[:3]:\n",
    "    #shap.dependence_plot(name, shap_values.values, X)\n",
    "    shap.plots.scatter(shap_values[:,name], \n",
    "                       color=color, \n",
    "                       xmin=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "shap.summary_plot(shap_values, X_val_iter_1, plot_type=\"dot\", show=False)\n",
    "plt.title(\"SHAP Summary Plot (Dot)\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "shap.summary_plot(shap_values, X_val_iter_1, plot_type=\"violin\", show=False)\n",
    "plt.title(\"SHAP Beeswarm Plot\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "shap.summary_plot(shap_values, X_val_iter_1, plot_type=\"bar\", show=False)\n",
    "plt.title(\"SHAP Bar Plot\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking de experimentos con MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para visualizar la interfaz gráfica con el progreso de los experimentos, se debe ejecutar la siguiente celda que corre el comando `mlflow ui` en el directorio actual, y luego abrir el navegador en la dirección: `localhost:5000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoreo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp, mannwhitneyu\n",
    "from scipy.stats import cramervonmises_2samp\n",
    "\n",
    "def detect_drift(train_data, production_data, features, target_column, method='ks', alpha=0.05): #ks, mw, cv\n",
    "    results = []\n",
    "\n",
    "    for feature in features:\n",
    "        train_feature = train_data[feature]\n",
    "        prod_feature = production_data[feature]\n",
    "        \n",
    "        if method == 'ks':\n",
    "            # Prueba Kolmogorov-Smirnov\n",
    "            stat, p_value = ks_2samp(train_feature, prod_feature)\n",
    "        elif method == 'mw':\n",
    "            # Prueba Mann-Whitney U\n",
    "            stat, p_value = mannwhitneyu(train_feature, prod_feature, alternative='two-sided')\n",
    "        elif method == 'cv':\n",
    "            # Prueba Cramér-von Mises\n",
    "            stat, p_value = cramervonmises_2samp(train_feature, prod_feature)\n",
    "\n",
    "        drift_detected = p_value < alpha\n",
    "\n",
    "    # Detección de target drift\n",
    "    target_train = train_data[target_column]\n",
    "    target_prod = production_data[target_column]\n",
    "\n",
    "    if method == 'ks':\n",
    "        target_stat, target_p_value = ks_2samp(target_train, target_prod)\n",
    "    elif method == 'mw':\n",
    "        target_stat, target_p_value = mannwhitneyu(target_train, target_prod, alternative='two-sided')\n",
    "    elif method == 'cv':\n",
    "        target_stat, target_p_value = cramervonmises_2samp(target_train, target_prod)\n",
    "    \n",
    "    target_drift_detected = target_p_value < alpha\n",
    "    \n",
    "    \n",
    "    return drift_detected, target_drift_detected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_check = ['target'] #completar según correspondaaaa\n",
    "data_train = X_t0\n",
    "data_production = X_t1_iter_1\n",
    "for feature in features_to_check:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(data_train[feature], bins=30, alpha=0.5, label='Train Data', density=True)\n",
    "    plt.hist(data_production[feature], bins=30, alpha=0.5, label='Production Data', density=True)\n",
    "    plt.title(f\"Distribuciones de {feature}\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"Densidad\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
