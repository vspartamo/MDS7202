{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyPTffTLug7i"
   },
   "source": [
    "# **Laboratorio 11: LLM y Agentes Autónomos 🤖**\n",
    "\n",
    "MDS7202: Laboratorio de Programación Científica para Ciencia de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pbWVyntzbvL"
   },
   "source": [
    "### **Cuerpo Docente:**\n",
    "\n",
    "- Profesores: Ignacio Meza, Sebastián Tinoco\n",
    "- Auxiliar: Eduardo Moya\n",
    "- Ayudantes: Nicolás Ojeda, Melanie Peña, Valentina Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy6ikgVYzghB"
   },
   "source": [
    "### **Equipo: SUPER IMPORTANTE - notebooks sin nombre no serán revisados**\n",
    "\n",
    "- Nombre de alumno 1: Nicolas Herrera\n",
    "- Nombre de alumno 2: Lucas Carrasco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMJ-owchzjFf"
   },
   "source": [
    "### **Link de repositorio de GitHub:** [Repositorio](https://github.com/vspartamo/MDS7202)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUuwsXrKzmkK"
   },
   "source": [
    "## **Temas a tratar**\n",
    "\n",
    "- Reinforcement Learning\n",
    "- Large Language Models\n",
    "\n",
    "## **Reglas:**\n",
    "\n",
    "- **Grupos de 2 personas**\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente serán respondidos por este medio.\n",
    "- Prohibidas las copias.\n",
    "- Pueden usar cualquer matrial del curso que estimen conveniente.\n",
    "\n",
    "### **Objetivos principales del laboratorio**\n",
    "\n",
    "- Resolución de problemas secuenciales usando Reinforcement Learning\n",
    "- Habilitar un Chatbot para entregar respuestas útiles usando Large Language Models.\n",
    "\n",
    "El laboratorio deberá ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al máximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante más eficientes que los iteradores nativos sobre DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hmHHQ9BuyAG"
   },
   "source": [
    "## **1. Reinforcement Learning (2.0 puntos)**\n",
    "\n",
    "En esta sección van a usar métodos de RL para resolver dos problemas interesantes: `Blackjack` y `LunarLander`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gOcejYb6uzOO",
    "outputId": "cd24480a-0905-43d9-a9bf-e0be2e915605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/958.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m931.8/958.1 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m952.3/958.1 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -qqq gymnasium stable_baselines3\n",
    "!pip install -qqq swig\n",
    "!pip install -qqq gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBPet_Mq8dX9"
   },
   "source": [
    "### **1.1 Blackjack (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.recreoviral.com/wp-content/uploads/2016/08/s3.amazonaws.com-Math.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "La idea de esta subsección es que puedan implementar métodos de RL y así generar una estrategia para jugar el clásico juego Blackjack y de paso puedan ~~hacerse millonarios~~ aprender a resolver problemas mediante RL.\n",
    "\n",
    "Comencemos primero preparando el ambiente. El siguiente bloque de código transforma las observaciones del ambiente a `np.array`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vqeddmvKPfBt"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import MultiDiscrete\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LpZ8bBKk9ZlU"
   },
   "outputs": [],
   "source": [
    "class FlattenObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(FlattenObservation, self).__init__(env)\n",
    "        self.observation_space = MultiDiscrete(np.array([32, 11, 2]))\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.array(observation).flatten()\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "env = FlattenObservation(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJ6J1_-Y9nHO"
   },
   "source": [
    "#### **1.1.1 Descripción de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripción sobre el ambiente [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/) y su formulación en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5i1Wt1p770x"
   },
   "source": [
    "`escriba su respuesta acá`\n",
    "\n",
    "El ambiente Blackjack simula el conocido juego de cartas, formulado como un proceso de decisión de Markov (MDP). Primero se dará una breve descripción del juego y luego sus componentes como MDP:\n",
    "\n",
    "El objetivo del BlackJack es obtener una mano de cartas cuya suma sea lo más cercana posible a 21 sin pasarse. Sus reglas son:\n",
    "\n",
    "1. **Cartas y valores**:\n",
    "    - Las cartas numeradas del 2 al 10 tienen su valor nominal.\n",
    "    - Las cartas con figuras (J, Q, K) valen 10 puntos cada una.\n",
    "    - El As puede valer 1 u 11 puntos, dependiendo de cuál valor beneficie más al jugador.\n",
    "\n",
    "2. **El juego**:\n",
    "    - Cada jugador recibe dos cartas al inicio del juego, al igual que el dealer (la casa). Una de las cartas del dealer se muestra boca arriba y la otra boca abajo.\n",
    "    - Los jugadores pueden optar por \"pedir\" (hit) más cartas para acercarse a 21 o \"plantarse\" (stand) si creen que su mano es lo suficientemente fuerte.\n",
    "    - Si un jugador supera los 21 puntos, pierde automáticamente (esto se llama \"bust\").\n",
    "    - Después de que todos los jugadores han terminado sus turnos, el dealer revela su carta oculta y debe seguir ciertas reglas: debe pedir cartas hasta alcanzar al menos 17 puntos y plantarse si tiene 17 o más.\n",
    "\n",
    "3. **Ganador**:\n",
    "    - Si el jugador tiene una mano más cercana a 21 que la del dealer sin pasarse, gana.\n",
    "    - Si el dealer supera los 21 puntos, todos los jugadores que no se hayan pasado ganan.\n",
    "    - Si el jugador y el dealer tienen la misma cantidad de puntos, es un empate (push) y se devuelve la apuesta al jugador.\n",
    "\n",
    "Descripción como MDP:\n",
    "\n",
    "**Estados**\n",
    "El espacio de estados es un tupla que describe:\n",
    "1. **Suma actual del jugador**: Rango [4, 21] (sin superar 21).\n",
    "2. **Valor de la carta visible del dealer**: Rango [1, 10], donde 1 representa un As.\n",
    "3. **As utilizable**: Binario (0 o 1), indicando si el jugador tiene un As que puede contar como 11 sin pasarse de 21.\n",
    "\n",
    "Estado: `(player_sum, dealer_card, usable_ace)`\n",
    "\n",
    "**Acciones**\n",
    "El jugador tiene dos acciones discretas:\n",
    "- `0`: Stick (plantarse, no pedir más cartas).\n",
    "- `1`: Hit (pedir otra carta).\n",
    "\n",
    "Espacio de acciones: `Discrete(2)`\n",
    "\n",
    "**Recompensas**\n",
    "- `+1`: Si el jugador gana (sin blackjack natural).\n",
    "- `-1`: Si el jugador pierde.\n",
    "- `0`: Si hay empate (draw).\n",
    "- `+1.5`: Si el jugador gana con un blackjack natural (opcional según el parámetro `natural`).\n",
    "- Nota: Si el jugador excede 21, pierde inmediatamente (`-1`).\n",
    "\n",
    "**Dinámica del MDP**\n",
    "- Transición: Las acciones determinan si el jugador toma otra carta o se planta. Si toma otra carta y excede 21, el episodio termina.\n",
    "- El dealer sigue una estrategia fija: revela su carta oculta y continúa sacando hasta alcanzar un mínimo de 17.\n",
    "- Los episodios terminan cuando el jugador se planta, excede 21, o el dealer determina el resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmcX6bRC9agQ"
   },
   "source": [
    "#### **1.1.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulación 5000 veces y reporte el promedio y desviación de las recompensas. ¿Cómo calificaría el performance de esta política? ¿Cómo podría interpretar las recompensas obtenidas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9p2PrLLR9yju",
    "outputId": "60c4bedc-16c5-4b2e-b51e-8498ef7e3dc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de las recompensas: -0.3628\n",
      "Desviación estándar de las recompensas: 0.9088\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 5000\n",
    "rewards = []\n",
    "\n",
    "for _ in range(n_episodes):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample() #accion random\n",
    "        observation, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "print(f\"Promedio de las recompensas: {np.mean(rewards):.4f}\")\n",
    "print(f\"Desviación estándar de las recompensas: {np.std(rewards):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gg7sE3TLL-0v"
   },
   "source": [
    "Un promedio de recompensa de -0.3628 dice que jugar al azar tiende levemente a la perdida, hace sentido pues si no se tiene en consideración la suma actual de las cartas y se pide al azar, pareciera ser más probable pasarse de 21, además, la desviación estándar de 0.9088 dice que hay una alta variabilidad en los resultados, por lo que las recompensas obtenidad no parecieran tener alguna tendencia muy definida, por lo que no es una buena manera de maximizar las recompensas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEO_dY4x_SJu"
   },
   "source": [
    "#### **1.1.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `Blackjack`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xnze0KomMHku"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from gymnasium.spaces import MultiDiscrete\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m9JsFA1wGmnH",
    "outputId": "f22d6a00-ec3f-438d-ffb8-91fcb647197c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07     |\n",
      "|    ep_rew_mean        | -0.26    |\n",
      "| time/                 |          |\n",
      "|    fps                | 376      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.131   |\n",
      "|    explained_variance | 0.509    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.0137   |\n",
      "|    value_loss         | 0.54     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.02     |\n",
      "|    ep_rew_mean        | -0.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0649  |\n",
      "|    explained_variance | 0.0252   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.00424  |\n",
      "|    value_loss         | 1.14     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1         |\n",
      "|    ep_rew_mean        | -0.2      |\n",
      "| time/                 |           |\n",
      "|    fps                | 355       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.114    |\n",
      "|    explained_variance | -2.51e+14 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | 0.0105    |\n",
      "|    value_loss         | 0.818     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01     |\n",
      "|    ep_rew_mean        | -0.26    |\n",
      "| time/                 |          |\n",
      "|    fps                | 353      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0322  |\n",
      "|    explained_variance | -3.7e+14 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.00306 |\n",
      "|    value_loss         | 0.66     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.02     |\n",
      "|    ep_rew_mean        | -0.29    |\n",
      "| time/                 |          |\n",
      "|    fps                | 365      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0481  |\n",
      "|    explained_variance | -0.0731  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.16     |\n",
      "|    value_loss         | 1.34     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01     |\n",
      "|    ep_rew_mean        | -0.04    |\n",
      "| time/                 |          |\n",
      "|    fps                | 374      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.159   |\n",
      "|    explained_variance | 0.0542   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.00744 |\n",
      "|    value_loss         | 0.757    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.08      |\n",
      "|    ep_rew_mean        | -0.17     |\n",
      "| time/                 |           |\n",
      "|    fps                | 368       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.269    |\n",
      "|    explained_variance | -5.09e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -0.554    |\n",
      "|    value_loss         | 0.852     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04     |\n",
      "|    ep_rew_mean        | -0.07    |\n",
      "| time/                 |          |\n",
      "|    fps                | 374      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0234  |\n",
      "|    explained_variance | 0.594    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.000509 |\n",
      "|    value_loss         | 0.401    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.03     |\n",
      "|    ep_rew_mean        | -0.15    |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0215  |\n",
      "|    explained_variance | 0.818    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -0.00117 |\n",
      "|    value_loss         | 0.216    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04     |\n",
      "|    ep_rew_mean        | -0.28    |\n",
      "| time/                 |          |\n",
      "|    fps                | 383      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.139   |\n",
      "|    explained_variance | 0.594    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.095    |\n",
      "|    value_loss         | 0.396    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.06     |\n",
      "|    ep_rew_mean        | -0.14    |\n",
      "| time/                 |          |\n",
      "|    fps                | 386      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00851 |\n",
      "|    explained_variance | 0.332    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.000337 |\n",
      "|    value_loss         | 0.645    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11     |\n",
      "|    ep_rew_mean        | -0.21    |\n",
      "| time/                 |          |\n",
      "|    fps                | 388      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00406 |\n",
      "|    explained_variance | 0.266    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.000129 |\n",
      "|    value_loss         | 0.918    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04     |\n",
      "|    ep_rew_mean        | -0.09    |\n",
      "| time/                 |          |\n",
      "|    fps                | 383      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.17    |\n",
      "|    explained_variance | 0.65     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -0.0424  |\n",
      "|    value_loss         | 0.381    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09     |\n",
      "|    ep_rew_mean        | -0.25    |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0354  |\n",
      "|    explained_variance | 0.475    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.000968 |\n",
      "|    value_loss         | 0.549    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1        |\n",
      "|    ep_rew_mean        | -0.18    |\n",
      "| time/                 |          |\n",
      "|    fps                | 382      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 19       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.116   |\n",
      "|    explained_variance | 0.315    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.0167   |\n",
      "|    value_loss         | 0.696    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08     |\n",
      "|    ep_rew_mean        | -0.04    |\n",
      "| time/                 |          |\n",
      "|    fps                | 384      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0076  |\n",
      "|    explained_variance | -0.153   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.000873 |\n",
      "|    value_loss         | 1.21     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.06      |\n",
      "|    ep_rew_mean        | -0.17     |\n",
      "| time/                 |           |\n",
      "|    fps                | 387       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00398  |\n",
      "|    explained_variance | 0.595     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -5.44e-05 |\n",
      "|    value_loss         | 0.391     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12     |\n",
      "|    ep_rew_mean        | -0.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 381      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 23       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.223   |\n",
      "|    explained_variance | 0.427    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.2      |\n",
      "|    value_loss         | 0.558    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08     |\n",
      "|    ep_rew_mean        | -0.07    |\n",
      "| time/                 |          |\n",
      "|    fps                | 383      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 24       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0114  |\n",
      "|    explained_variance | -0.21    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.000238 |\n",
      "|    value_loss         | 1.22     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.06      |\n",
      "|    ep_rew_mean        | -0.36     |\n",
      "| time/                 |           |\n",
      "|    fps                | 381       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 26        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0089   |\n",
      "|    explained_variance | 0.902     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -0.000261 |\n",
      "|    value_loss         | 0.109     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.05     |\n",
      "|    ep_rew_mean        | -0.07    |\n",
      "| time/                 |          |\n",
      "|    fps                | 380      |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 27       |\n",
      "|    total_timesteps    | 10500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00471 |\n",
      "|    explained_variance | 0.639    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | 8.17e-06 |\n",
      "|    value_loss         | 0.45     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.02     |\n",
      "|    ep_rew_mean        | -0.34    |\n",
      "| time/                 |          |\n",
      "|    fps                | 372      |\n",
      "|    iterations         | 2200     |\n",
      "|    time_elapsed       | 29       |\n",
      "|    total_timesteps    | 11000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0333  |\n",
      "|    explained_variance | 0.54     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | -0.00151 |\n",
      "|    value_loss         | 0.4      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.06     |\n",
      "|    ep_rew_mean        | -0.06    |\n",
      "| time/                 |          |\n",
      "|    fps                | 359      |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 31       |\n",
      "|    total_timesteps    | 11500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00383 |\n",
      "|    explained_variance | 0.173    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | 1.69e-05 |\n",
      "|    value_loss         | 0.774    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.02     |\n",
      "|    ep_rew_mean        | -0.14    |\n",
      "| time/                 |          |\n",
      "|    fps                | 362      |\n",
      "|    iterations         | 2400     |\n",
      "|    time_elapsed       | 33       |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00159 |\n",
      "|    explained_variance | -0.492   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | 0.00016  |\n",
      "|    value_loss         | 1.5      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.08      |\n",
      "|    ep_rew_mean        | -0.15     |\n",
      "| time/                 |           |\n",
      "|    fps                | 364       |\n",
      "|    iterations         | 2500      |\n",
      "|    time_elapsed       | 34        |\n",
      "|    total_timesteps    | 12500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00128  |\n",
      "|    explained_variance | -0.204    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2499      |\n",
      "|    policy_loss        | -7.77e-05 |\n",
      "|    value_loss         | 1.17      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.09      |\n",
      "|    ep_rew_mean        | -0.06     |\n",
      "| time/                 |           |\n",
      "|    fps                | 366       |\n",
      "|    iterations         | 2600      |\n",
      "|    time_elapsed       | 35        |\n",
      "|    total_timesteps    | 13000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.173    |\n",
      "|    explained_variance | -7.25e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2599      |\n",
      "|    policy_loss        | 0.0229    |\n",
      "|    value_loss         | 1.2       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.12      |\n",
      "|    ep_rew_mean        | -0.32     |\n",
      "| time/                 |           |\n",
      "|    fps                | 368       |\n",
      "|    iterations         | 2700      |\n",
      "|    time_elapsed       | 36        |\n",
      "|    total_timesteps    | 13500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00914  |\n",
      "|    explained_variance | 0.627     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2699      |\n",
      "|    policy_loss        | -0.000868 |\n",
      "|    value_loss         | 0.392     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09     |\n",
      "|    ep_rew_mean        | -0.01    |\n",
      "| time/                 |          |\n",
      "|    fps                | 370      |\n",
      "|    iterations         | 2800     |\n",
      "|    time_elapsed       | 37       |\n",
      "|    total_timesteps    | 14000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00236 |\n",
      "|    explained_variance | 0.249    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | 0.000144 |\n",
      "|    value_loss         | 0.556    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.07      |\n",
      "|    ep_rew_mean        | -0.04     |\n",
      "| time/                 |           |\n",
      "|    fps                | 371       |\n",
      "|    iterations         | 2900      |\n",
      "|    time_elapsed       | 38        |\n",
      "|    total_timesteps    | 14500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00322  |\n",
      "|    explained_variance | 0.236     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2899      |\n",
      "|    policy_loss        | -3.36e-05 |\n",
      "|    value_loss         | 0.576     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.07      |\n",
      "|    ep_rew_mean        | -0.03     |\n",
      "| time/                 |           |\n",
      "|    fps                | 373       |\n",
      "|    iterations         | 3000      |\n",
      "|    time_elapsed       | 40        |\n",
      "|    total_timesteps    | 15000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00136  |\n",
      "|    explained_variance | 0.665     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2999      |\n",
      "|    policy_loss        | -6.28e-05 |\n",
      "|    value_loss         | 0.328     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.05     |\n",
      "|    ep_rew_mean        | -0.26    |\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 3100     |\n",
      "|    time_elapsed       | 41       |\n",
      "|    total_timesteps    | 15500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.104   |\n",
      "|    explained_variance | 0.408    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3099     |\n",
      "|    policy_loss        | -0.0446  |\n",
      "|    value_loss         | 0.491    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.07      |\n",
      "|    ep_rew_mean        | -0.28     |\n",
      "| time/                 |           |\n",
      "|    fps                | 374       |\n",
      "|    iterations         | 3200      |\n",
      "|    time_elapsed       | 42        |\n",
      "|    total_timesteps    | 16000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00367  |\n",
      "|    explained_variance | 0.633     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3199      |\n",
      "|    policy_loss        | -0.000216 |\n",
      "|    value_loss         | 0.368     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.15      |\n",
      "|    ep_rew_mean        | -0.21     |\n",
      "| time/                 |           |\n",
      "|    fps                | 371       |\n",
      "|    iterations         | 3300      |\n",
      "|    time_elapsed       | 44        |\n",
      "|    total_timesteps    | 16500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.27     |\n",
      "|    explained_variance | -1.14e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3299      |\n",
      "|    policy_loss        | -0.406    |\n",
      "|    value_loss         | 0.56      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.1       |\n",
      "|    ep_rew_mean        | -0.21     |\n",
      "| time/                 |           |\n",
      "|    fps                | 372       |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 45        |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000761 |\n",
      "|    explained_variance | 0.347     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | -4.96e-05 |\n",
      "|    value_loss         | 0.527     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.11      |\n",
      "|    ep_rew_mean        | -0.19     |\n",
      "| time/                 |           |\n",
      "|    fps                | 373       |\n",
      "|    iterations         | 3500      |\n",
      "|    time_elapsed       | 46        |\n",
      "|    total_timesteps    | 17500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000361 |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3499      |\n",
      "|    policy_loss        | -2.22e-05 |\n",
      "|    value_loss         | 0.5       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08     |\n",
      "|    ep_rew_mean        | -0.04    |\n",
      "| time/                 |          |\n",
      "|    fps                | 374      |\n",
      "|    iterations         | 3600     |\n",
      "|    time_elapsed       | 48       |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0866  |\n",
      "|    explained_variance | 0.0322   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3599     |\n",
      "|    policy_loss        | 0.0271   |\n",
      "|    value_loss         | 0.803    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1      |\n",
      "|    ep_rew_mean        | 0.01     |\n",
      "| time/                 |          |\n",
      "|    fps                | 376      |\n",
      "|    iterations         | 3700     |\n",
      "|    time_elapsed       | 49       |\n",
      "|    total_timesteps    | 18500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0111  |\n",
      "|    explained_variance | -2.45    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3699     |\n",
      "|    policy_loss        | 0.00105  |\n",
      "|    value_loss         | 0.651    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.06      |\n",
      "|    ep_rew_mean        | -0.06     |\n",
      "| time/                 |           |\n",
      "|    fps                | 375       |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 50        |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000804 |\n",
      "|    explained_variance | 0.653     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | -8.56e-05 |\n",
      "|    value_loss         | 0.365     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.06      |\n",
      "|    ep_rew_mean        | -0.17     |\n",
      "| time/                 |           |\n",
      "|    fps                | 376       |\n",
      "|    iterations         | 3900      |\n",
      "|    time_elapsed       | 51        |\n",
      "|    total_timesteps    | 19500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000355 |\n",
      "|    explained_variance | 0.64      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3899      |\n",
      "|    policy_loss        | -2.93e-05 |\n",
      "|    value_loss         | 0.653     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.08      |\n",
      "|    ep_rew_mean        | -0.16     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 53        |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000605 |\n",
      "|    explained_variance | -1.41     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3999      |\n",
      "|    policy_loss        | -2.38e-05 |\n",
      "|    value_loss         | 2.1       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07     |\n",
      "|    ep_rew_mean        | -0.11    |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 4100     |\n",
      "|    time_elapsed       | 54       |\n",
      "|    total_timesteps    | 20500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0461  |\n",
      "|    explained_variance | 0.724    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4099     |\n",
      "|    policy_loss        | -0.0122  |\n",
      "|    value_loss         | 0.537    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.06     |\n",
      "|    ep_rew_mean        | -0.08    |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 4200     |\n",
      "|    time_elapsed       | 55       |\n",
      "|    total_timesteps    | 21000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0483  |\n",
      "|    explained_variance | -0.38    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4199     |\n",
      "|    policy_loss        | -0.0115  |\n",
      "|    value_loss         | 1.11     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.07      |\n",
      "|    ep_rew_mean        | -0.06     |\n",
      "| time/                 |           |\n",
      "|    fps                | 376       |\n",
      "|    iterations         | 4300      |\n",
      "|    time_elapsed       | 57        |\n",
      "|    total_timesteps    | 21500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000121 |\n",
      "|    explained_variance | 0.103     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4299      |\n",
      "|    policy_loss        | -3.7e-06  |\n",
      "|    value_loss         | 0.503     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.15      |\n",
      "|    ep_rew_mean        | -0.29     |\n",
      "| time/                 |           |\n",
      "|    fps                | 376       |\n",
      "|    iterations         | 4400      |\n",
      "|    time_elapsed       | 58        |\n",
      "|    total_timesteps    | 22000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0309   |\n",
      "|    explained_variance | -1.58e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4399      |\n",
      "|    policy_loss        | -0.0071   |\n",
      "|    value_loss         | 1.37      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.11      |\n",
      "|    ep_rew_mean        | -0.19     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 59        |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000654 |\n",
      "|    explained_variance | 0.771     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4499      |\n",
      "|    policy_loss        | -8.47e-06 |\n",
      "|    value_loss         | 0.503     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07     |\n",
      "|    ep_rew_mean        | -0.02    |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 4600     |\n",
      "|    time_elapsed       | 60       |\n",
      "|    total_timesteps    | 23000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00102 |\n",
      "|    explained_variance | 0.275    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4599     |\n",
      "|    policy_loss        | 8.72e-05 |\n",
      "|    value_loss         | 0.464    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04     |\n",
      "|    ep_rew_mean        | 0.02     |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 4700     |\n",
      "|    time_elapsed       | 61       |\n",
      "|    total_timesteps    | 23500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0196  |\n",
      "|    explained_variance | 0.0767   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4699     |\n",
      "|    policy_loss        | 0.00649  |\n",
      "|    value_loss         | 1.73     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.15     |\n",
      "|    ep_rew_mean        | -0.36    |\n",
      "| time/                 |          |\n",
      "|    fps                | 380      |\n",
      "|    iterations         | 4800     |\n",
      "|    time_elapsed       | 63       |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0154  |\n",
      "|    explained_variance | 0.371    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4799     |\n",
      "|    policy_loss        | 0.000438 |\n",
      "|    value_loss         | 0.658    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.06      |\n",
      "|    ep_rew_mean        | -0.12     |\n",
      "| time/                 |           |\n",
      "|    fps                | 381       |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 64        |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000719 |\n",
      "|    explained_variance | 0.8       |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | -3.58e-06 |\n",
      "|    value_loss         | 0.216     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.03      |\n",
      "|    ep_rew_mean        | -0.19     |\n",
      "| time/                 |           |\n",
      "|    fps                | 382       |\n",
      "|    iterations         | 5000      |\n",
      "|    time_elapsed       | 65        |\n",
      "|    total_timesteps    | 25000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000611 |\n",
      "|    explained_variance | -0.00603  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4999      |\n",
      "|    policy_loss        | 3.23e-05  |\n",
      "|    value_loss         | 1.19      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.05     |\n",
      "|    ep_rew_mean        | -0.15    |\n",
      "| time/                 |          |\n",
      "|    fps                | 382      |\n",
      "|    iterations         | 5100     |\n",
      "|    time_elapsed       | 66       |\n",
      "|    total_timesteps    | 25500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00123 |\n",
      "|    explained_variance | 0.19     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5099     |\n",
      "|    policy_loss        | 2.61e-05 |\n",
      "|    value_loss         | 0.864    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.08      |\n",
      "|    ep_rew_mean        | 0.05      |\n",
      "| time/                 |           |\n",
      "|    fps                | 383       |\n",
      "|    iterations         | 5200      |\n",
      "|    time_elapsed       | 67        |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000482 |\n",
      "|    explained_variance | 0.231     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5199      |\n",
      "|    policy_loss        | 1.34e-05  |\n",
      "|    value_loss         | 0.685     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1      |\n",
      "|    ep_rew_mean        | -0.11    |\n",
      "| time/                 |          |\n",
      "|    fps                | 381      |\n",
      "|    iterations         | 5300     |\n",
      "|    time_elapsed       | 69       |\n",
      "|    total_timesteps    | 26500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00465 |\n",
      "|    explained_variance | 0.411    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5299     |\n",
      "|    policy_loss        | 0.000852 |\n",
      "|    value_loss         | 0.628    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.03     |\n",
      "|    ep_rew_mean        | -0.14    |\n",
      "| time/                 |          |\n",
      "|    fps                | 380      |\n",
      "|    iterations         | 5400     |\n",
      "|    time_elapsed       | 70       |\n",
      "|    total_timesteps    | 27000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00578 |\n",
      "|    explained_variance | -0.0439  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5399     |\n",
      "|    policy_loss        | 0.000112 |\n",
      "|    value_loss         | 0.839    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11     |\n",
      "|    ep_rew_mean        | -0.07    |\n",
      "| time/                 |          |\n",
      "|    fps                | 381      |\n",
      "|    iterations         | 5500     |\n",
      "|    time_elapsed       | 72       |\n",
      "|    total_timesteps    | 27500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00292 |\n",
      "|    explained_variance | -0.229   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5499     |\n",
      "|    policy_loss        | 0.000201 |\n",
      "|    value_loss         | 1.08     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.06      |\n",
      "|    ep_rew_mean        | -0.14     |\n",
      "| time/                 |           |\n",
      "|    fps                | 381       |\n",
      "|    iterations         | 5600      |\n",
      "|    time_elapsed       | 73        |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00689  |\n",
      "|    explained_variance | 0.161     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5599      |\n",
      "|    policy_loss        | -0.000303 |\n",
      "|    value_loss         | 1.04      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.04      |\n",
      "|    ep_rew_mean        | -0.14     |\n",
      "| time/                 |           |\n",
      "|    fps                | 382       |\n",
      "|    iterations         | 5700      |\n",
      "|    time_elapsed       | 74        |\n",
      "|    total_timesteps    | 28500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00069  |\n",
      "|    explained_variance | 0.207     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5699      |\n",
      "|    policy_loss        | -8.32e-06 |\n",
      "|    value_loss         | 0.549     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.05     |\n",
      "|    ep_rew_mean        | -0.06    |\n",
      "| time/                 |          |\n",
      "|    fps                | 382      |\n",
      "|    iterations         | 5800     |\n",
      "|    time_elapsed       | 75       |\n",
      "|    total_timesteps    | 29000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.139   |\n",
      "|    explained_variance | -0.0365  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5799     |\n",
      "|    policy_loss        | 0.0488   |\n",
      "|    value_loss         | 0.645    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.05      |\n",
      "|    ep_rew_mean        | -0.24     |\n",
      "| time/                 |           |\n",
      "|    fps                | 383       |\n",
      "|    iterations         | 5900      |\n",
      "|    time_elapsed       | 76        |\n",
      "|    total_timesteps    | 29500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000293 |\n",
      "|    explained_variance | 0.17      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5899      |\n",
      "|    policy_loss        | -5.46e-06 |\n",
      "|    value_loss         | 0.532     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.05      |\n",
      "|    ep_rew_mean        | -0.07     |\n",
      "| time/                 |           |\n",
      "|    fps                | 384       |\n",
      "|    iterations         | 6000      |\n",
      "|    time_elapsed       | 78        |\n",
      "|    total_timesteps    | 30000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000502 |\n",
      "|    explained_variance | -0.613    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5999      |\n",
      "|    policy_loss        | -5.37e-06 |\n",
      "|    value_loss         | 1.03      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.07      |\n",
      "|    ep_rew_mean        | -0.2      |\n",
      "| time/                 |           |\n",
      "|    fps                | 385       |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 79        |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000674 |\n",
      "|    explained_variance | 0.27      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6099      |\n",
      "|    policy_loss        | 7.59e-05  |\n",
      "|    value_loss         | 1.17      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.08      |\n",
      "|    ep_rew_mean        | -0.21     |\n",
      "| time/                 |           |\n",
      "|    fps                | 385       |\n",
      "|    iterations         | 6200      |\n",
      "|    time_elapsed       | 80        |\n",
      "|    total_timesteps    | 31000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000692 |\n",
      "|    explained_variance | -0.0769   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6199      |\n",
      "|    policy_loss        | 7.61e-05  |\n",
      "|    value_loss         | 1.34      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.04      |\n",
      "|    ep_rew_mean        | -0.35     |\n",
      "| time/                 |           |\n",
      "|    fps                | 384       |\n",
      "|    iterations         | 6300      |\n",
      "|    time_elapsed       | 81        |\n",
      "|    total_timesteps    | 31500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00147  |\n",
      "|    explained_variance | 0.821     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6299      |\n",
      "|    policy_loss        | -5.84e-05 |\n",
      "|    value_loss         | 0.16      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07     |\n",
      "|    ep_rew_mean        | -0.12    |\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 6400     |\n",
      "|    time_elapsed       | 85       |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0108  |\n",
      "|    explained_variance | 0.194    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6399     |\n",
      "|    policy_loss        | 0.00142  |\n",
      "|    value_loss         | 0.77     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.1       |\n",
      "|    ep_rew_mean        | -0.14     |\n",
      "| time/                 |           |\n",
      "|    fps                | 375       |\n",
      "|    iterations         | 6500      |\n",
      "|    time_elapsed       | 86        |\n",
      "|    total_timesteps    | 32500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000943 |\n",
      "|    explained_variance | 0.957     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6499      |\n",
      "|    policy_loss        | -3.32e-05 |\n",
      "|    value_loss         | 0.148     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09     |\n",
      "|    ep_rew_mean        | -0.26    |\n",
      "| time/                 |          |\n",
      "|    fps                | 376      |\n",
      "|    iterations         | 6600     |\n",
      "|    time_elapsed       | 87       |\n",
      "|    total_timesteps    | 33000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0997  |\n",
      "|    explained_variance | 0.128    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6599     |\n",
      "|    policy_loss        | -0.0568  |\n",
      "|    value_loss         | 0.73     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08     |\n",
      "|    ep_rew_mean        | -0.22    |\n",
      "| time/                 |          |\n",
      "|    fps                | 377      |\n",
      "|    iterations         | 6700     |\n",
      "|    time_elapsed       | 88       |\n",
      "|    total_timesteps    | 33500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.123   |\n",
      "|    explained_variance | 0.408    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6699     |\n",
      "|    policy_loss        | 0.0461   |\n",
      "|    value_loss         | 0.738    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.02      |\n",
      "|    ep_rew_mean        | -0.35     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 6800      |\n",
      "|    time_elapsed       | 90        |\n",
      "|    total_timesteps    | 34000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00533  |\n",
      "|    explained_variance | 0.656     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6799      |\n",
      "|    policy_loss        | -0.000415 |\n",
      "|    value_loss         | 0.192     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09     |\n",
      "|    ep_rew_mean        | -0.06    |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 6900     |\n",
      "|    time_elapsed       | 91       |\n",
      "|    total_timesteps    | 34500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00161 |\n",
      "|    explained_variance | 0.393    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6899     |\n",
      "|    policy_loss        | -1.9e-05 |\n",
      "|    value_loss         | 0.598    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.14      |\n",
      "|    ep_rew_mean        | -0.2      |\n",
      "| time/                 |           |\n",
      "|    fps                | 378       |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 92        |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00143  |\n",
      "|    explained_variance | 0.696     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | -0.000109 |\n",
      "|    value_loss         | 0.277     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09     |\n",
      "|    ep_rew_mean        | -0.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 7100     |\n",
      "|    time_elapsed       | 93       |\n",
      "|    total_timesteps    | 35500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0533  |\n",
      "|    explained_variance | 0.413    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7099     |\n",
      "|    policy_loss        | 0.00775  |\n",
      "|    value_loss         | 0.568    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12     |\n",
      "|    ep_rew_mean        | -0.17    |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 7200     |\n",
      "|    time_elapsed       | 94       |\n",
      "|    total_timesteps    | 36000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0149  |\n",
      "|    explained_variance | -0.942   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7199     |\n",
      "|    policy_loss        | 8.73e-05 |\n",
      "|    value_loss         | 1.3      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.11      |\n",
      "|    ep_rew_mean        | -0.08     |\n",
      "| time/                 |           |\n",
      "|    fps                | 378       |\n",
      "|    iterations         | 7300      |\n",
      "|    time_elapsed       | 96        |\n",
      "|    total_timesteps    | 36500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00442  |\n",
      "|    explained_variance | -0.198    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7299      |\n",
      "|    policy_loss        | -0.000327 |\n",
      "|    value_loss         | 1.27      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.06     |\n",
      "|    ep_rew_mean        | -0.05    |\n",
      "| time/                 |          |\n",
      "|    fps                | 377      |\n",
      "|    iterations         | 7400     |\n",
      "|    time_elapsed       | 97       |\n",
      "|    total_timesteps    | 37000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00222 |\n",
      "|    explained_variance | 0.331    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7399     |\n",
      "|    policy_loss        | 0.000191 |\n",
      "|    value_loss         | 0.652    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.1       |\n",
      "|    ep_rew_mean        | -0.02     |\n",
      "| time/                 |           |\n",
      "|    fps                | 375       |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 99        |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0019   |\n",
      "|    explained_variance | 0.299     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7499      |\n",
      "|    policy_loss        | -3.09e-07 |\n",
      "|    value_loss         | 0.673     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11     |\n",
      "|    ep_rew_mean        | -0.04    |\n",
      "| time/                 |          |\n",
      "|    fps                | 373      |\n",
      "|    iterations         | 7600     |\n",
      "|    time_elapsed       | 101      |\n",
      "|    total_timesteps    | 38000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00113 |\n",
      "|    explained_variance | -0.373   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7599     |\n",
      "|    policy_loss        | 3.62e-05 |\n",
      "|    value_loss         | 1.32     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09     |\n",
      "|    ep_rew_mean        | -0.09    |\n",
      "| time/                 |          |\n",
      "|    fps                | 374      |\n",
      "|    iterations         | 7700     |\n",
      "|    time_elapsed       | 102      |\n",
      "|    total_timesteps    | 38500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0261  |\n",
      "|    explained_variance | -0.142   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7699     |\n",
      "|    policy_loss        | 0.0028   |\n",
      "|    value_loss         | 1.1      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.06     |\n",
      "|    ep_rew_mean        | -0.11    |\n",
      "| time/                 |          |\n",
      "|    fps                | 374      |\n",
      "|    iterations         | 7800     |\n",
      "|    time_elapsed       | 104      |\n",
      "|    total_timesteps    | 39000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00117 |\n",
      "|    explained_variance | 0.269    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7799     |\n",
      "|    policy_loss        | 5.89e-05 |\n",
      "|    value_loss         | 0.544    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.07      |\n",
      "|    ep_rew_mean        | -0.23     |\n",
      "| time/                 |           |\n",
      "|    fps                | 375       |\n",
      "|    iterations         | 7900      |\n",
      "|    time_elapsed       | 105       |\n",
      "|    total_timesteps    | 39500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00357  |\n",
      "|    explained_variance | 0.175     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7899      |\n",
      "|    policy_loss        | -0.000157 |\n",
      "|    value_loss         | 0.549     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09     |\n",
      "|    ep_rew_mean        | -0.13    |\n",
      "| time/                 |          |\n",
      "|    fps                | 376      |\n",
      "|    iterations         | 8000     |\n",
      "|    time_elapsed       | 106      |\n",
      "|    total_timesteps    | 40000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.14    |\n",
      "|    explained_variance | 0.694    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7999     |\n",
      "|    policy_loss        | -0.136   |\n",
      "|    value_loss         | 0.311    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.08      |\n",
      "|    ep_rew_mean        | -0.24     |\n",
      "| time/                 |           |\n",
      "|    fps                | 376       |\n",
      "|    iterations         | 8100      |\n",
      "|    time_elapsed       | 107       |\n",
      "|    total_timesteps    | 40500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00534  |\n",
      "|    explained_variance | -1.15e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8099      |\n",
      "|    policy_loss        | -0.000185 |\n",
      "|    value_loss         | 0.118     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1      |\n",
      "|    ep_rew_mean        | -0.17    |\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 8200     |\n",
      "|    time_elapsed       | 109      |\n",
      "|    total_timesteps    | 41000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00958 |\n",
      "|    explained_variance | -0.385   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8199     |\n",
      "|    policy_loss        | -8.6e-05 |\n",
      "|    value_loss         | 0.843    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1      |\n",
      "|    ep_rew_mean        | -0.08    |\n",
      "| time/                 |          |\n",
      "|    fps                | 374      |\n",
      "|    iterations         | 8300     |\n",
      "|    time_elapsed       | 110      |\n",
      "|    total_timesteps    | 41500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.157   |\n",
      "|    explained_variance | 0.366    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8299     |\n",
      "|    policy_loss        | -0.139   |\n",
      "|    value_loss         | 0.611    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09     |\n",
      "|    ep_rew_mean        | -0.09    |\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 8400     |\n",
      "|    time_elapsed       | 111      |\n",
      "|    total_timesteps    | 42000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.016   |\n",
      "|    explained_variance | 0.108    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8399     |\n",
      "|    policy_loss        | 0.00256  |\n",
      "|    value_loss         | 1.25     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.06      |\n",
      "|    ep_rew_mean        | -0.4      |\n",
      "| time/                 |           |\n",
      "|    fps                | 375       |\n",
      "|    iterations         | 8500      |\n",
      "|    time_elapsed       | 113       |\n",
      "|    total_timesteps    | 42500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00836  |\n",
      "|    explained_variance | 0.191     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8499      |\n",
      "|    policy_loss        | -0.000316 |\n",
      "|    value_loss         | 0.9       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04     |\n",
      "|    ep_rew_mean        | -0.08    |\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 8600     |\n",
      "|    time_elapsed       | 114      |\n",
      "|    total_timesteps    | 43000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0973  |\n",
      "|    explained_variance | -0.143   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8599     |\n",
      "|    policy_loss        | -0.031   |\n",
      "|    value_loss         | 1.19     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04     |\n",
      "|    ep_rew_mean        | -0.36    |\n",
      "| time/                 |          |\n",
      "|    fps                | 376      |\n",
      "|    iterations         | 8700     |\n",
      "|    time_elapsed       | 115      |\n",
      "|    total_timesteps    | 43500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00209 |\n",
      "|    explained_variance | 0.0759   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8699     |\n",
      "|    policy_loss        | 7.44e-05 |\n",
      "|    value_loss         | 0.897    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.05      |\n",
      "|    ep_rew_mean        | -0.21     |\n",
      "| time/                 |           |\n",
      "|    fps                | 376       |\n",
      "|    iterations         | 8800      |\n",
      "|    time_elapsed       | 116       |\n",
      "|    total_timesteps    | 44000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00129  |\n",
      "|    explained_variance | 0.861     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8799      |\n",
      "|    policy_loss        | -4.78e-05 |\n",
      "|    value_loss         | 0.238     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.05      |\n",
      "|    ep_rew_mean        | -0.32     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 8900      |\n",
      "|    time_elapsed       | 117       |\n",
      "|    total_timesteps    | 44500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000779 |\n",
      "|    explained_variance | 0.733     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | -3.92e-05 |\n",
      "|    value_loss         | 0.282     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09     |\n",
      "|    ep_rew_mean        | -0.24    |\n",
      "| time/                 |          |\n",
      "|    fps                | 377      |\n",
      "|    iterations         | 9000     |\n",
      "|    time_elapsed       | 119      |\n",
      "|    total_timesteps    | 45000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.137   |\n",
      "|    explained_variance | 0.094    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8999     |\n",
      "|    policy_loss        | 0.0907   |\n",
      "|    value_loss         | 0.964    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.06      |\n",
      "|    ep_rew_mean        | -0.23     |\n",
      "| time/                 |           |\n",
      "|    fps                | 376       |\n",
      "|    iterations         | 9100      |\n",
      "|    time_elapsed       | 120       |\n",
      "|    total_timesteps    | 45500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00111  |\n",
      "|    explained_variance | 0.0485    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9099      |\n",
      "|    policy_loss        | -3.18e-05 |\n",
      "|    value_loss         | 0.618     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.07      |\n",
      "|    ep_rew_mean        | -0.24     |\n",
      "| time/                 |           |\n",
      "|    fps                | 374       |\n",
      "|    iterations         | 9200      |\n",
      "|    time_elapsed       | 122       |\n",
      "|    total_timesteps    | 46000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000648 |\n",
      "|    explained_variance | 0.159     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9199      |\n",
      "|    policy_loss        | -2.3e-05  |\n",
      "|    value_loss         | 0.811     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.11      |\n",
      "|    ep_rew_mean        | 0.01      |\n",
      "| time/                 |           |\n",
      "|    fps                | 373       |\n",
      "|    iterations         | 9300      |\n",
      "|    time_elapsed       | 124       |\n",
      "|    total_timesteps    | 46500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00335  |\n",
      "|    explained_variance | 0.176     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9299      |\n",
      "|    policy_loss        | -0.000239 |\n",
      "|    value_loss         | 0.792     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09     |\n",
      "|    ep_rew_mean        | -0.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 373      |\n",
      "|    iterations         | 9400     |\n",
      "|    time_elapsed       | 125      |\n",
      "|    total_timesteps    | 47000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0166  |\n",
      "|    explained_variance | 0.105    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9399     |\n",
      "|    policy_loss        | 0.00121  |\n",
      "|    value_loss         | 0.828    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.12      |\n",
      "|    ep_rew_mean        | -0.12     |\n",
      "| time/                 |           |\n",
      "|    fps                | 374       |\n",
      "|    iterations         | 9500      |\n",
      "|    time_elapsed       | 126       |\n",
      "|    total_timesteps    | 47500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000701 |\n",
      "|    explained_variance | 0.936     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9499      |\n",
      "|    policy_loss        | -1.91e-05 |\n",
      "|    value_loss         | 0.171     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.14      |\n",
      "|    ep_rew_mean        | -0.1      |\n",
      "| time/                 |           |\n",
      "|    fps                | 374       |\n",
      "|    iterations         | 9600      |\n",
      "|    time_elapsed       | 128       |\n",
      "|    total_timesteps    | 48000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000595 |\n",
      "|    explained_variance | -0.347    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9599      |\n",
      "|    policy_loss        | 2.78e-06  |\n",
      "|    value_loss         | 1.11      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.15     |\n",
      "|    ep_rew_mean        | -0.06    |\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 9700     |\n",
      "|    time_elapsed       | 129      |\n",
      "|    total_timesteps    | 48500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0242  |\n",
      "|    explained_variance | 0.534    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9699     |\n",
      "|    policy_loss        | 0.000727 |\n",
      "|    value_loss         | 0.458    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08     |\n",
      "|    ep_rew_mean        | -0.25    |\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 9800     |\n",
      "|    time_elapsed       | 130      |\n",
      "|    total_timesteps    | 49000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0369  |\n",
      "|    explained_variance | 0.102    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9799     |\n",
      "|    policy_loss        | 0.0105   |\n",
      "|    value_loss         | 1.49     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09     |\n",
      "|    ep_rew_mean        | -0.19    |\n",
      "| time/                 |          |\n",
      "|    fps                | 376      |\n",
      "|    iterations         | 9900     |\n",
      "|    time_elapsed       | 131      |\n",
      "|    total_timesteps    | 49500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00105 |\n",
      "|    explained_variance | 0.0668   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9899     |\n",
      "|    policy_loss        | 4.07e-05 |\n",
      "|    value_loss         | 1.7      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.11      |\n",
      "|    ep_rew_mean        | -0.01     |\n",
      "| time/                 |           |\n",
      "|    fps                | 376       |\n",
      "|    iterations         | 10000     |\n",
      "|    time_elapsed       | 132       |\n",
      "|    total_timesteps    | 50000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00445  |\n",
      "|    explained_variance | 0.588     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9999      |\n",
      "|    policy_loss        | -0.000234 |\n",
      "|    value_loss         | 0.398     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.08      |\n",
      "|    ep_rew_mean        | -0.34     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 10100     |\n",
      "|    time_elapsed       | 133       |\n",
      "|    total_timesteps    | 50500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0774   |\n",
      "|    explained_variance | -1.83e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10099     |\n",
      "|    policy_loss        | -0.0133   |\n",
      "|    value_loss         | 0.489     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07     |\n",
      "|    ep_rew_mean        | -0.11    |\n",
      "| time/                 |          |\n",
      "|    fps                | 377      |\n",
      "|    iterations         | 10200    |\n",
      "|    time_elapsed       | 135      |\n",
      "|    total_timesteps    | 51000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0299  |\n",
      "|    explained_variance | -0.0651  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10199    |\n",
      "|    policy_loss        | -0.00357 |\n",
      "|    value_loss         | 0.918    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.05      |\n",
      "|    ep_rew_mean        | -0.2      |\n",
      "| time/                 |           |\n",
      "|    fps                | 376       |\n",
      "|    iterations         | 10300     |\n",
      "|    time_elapsed       | 136       |\n",
      "|    total_timesteps    | 51500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0057   |\n",
      "|    explained_variance | 0.497     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10299     |\n",
      "|    policy_loss        | -0.000345 |\n",
      "|    value_loss         | 0.488     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07     |\n",
      "|    ep_rew_mean        | -0.04    |\n",
      "| time/                 |          |\n",
      "|    fps                | 376      |\n",
      "|    iterations         | 10400    |\n",
      "|    time_elapsed       | 138      |\n",
      "|    total_timesteps    | 52000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00241 |\n",
      "|    explained_variance | 0.147    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10399    |\n",
      "|    policy_loss        | 0.000338 |\n",
      "|    value_loss         | 1.19     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.15      |\n",
      "|    ep_rew_mean        | -0.19     |\n",
      "| time/                 |           |\n",
      "|    fps                | 376       |\n",
      "|    iterations         | 10500     |\n",
      "|    time_elapsed       | 139       |\n",
      "|    total_timesteps    | 52500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00269  |\n",
      "|    explained_variance | 0.945     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10499     |\n",
      "|    policy_loss        | -9.04e-05 |\n",
      "|    value_loss         | 0.164     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07     |\n",
      "|    ep_rew_mean        | -0.17    |\n",
      "| time/                 |          |\n",
      "|    fps                | 376      |\n",
      "|    iterations         | 10600    |\n",
      "|    time_elapsed       | 140      |\n",
      "|    total_timesteps    | 53000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0155  |\n",
      "|    explained_variance | 0.385    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10599    |\n",
      "|    policy_loss        | 0.00304  |\n",
      "|    value_loss         | 1.27     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04     |\n",
      "|    ep_rew_mean        | -0.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 377      |\n",
      "|    iterations         | 10700    |\n",
      "|    time_elapsed       | 141      |\n",
      "|    total_timesteps    | 53500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00877 |\n",
      "|    explained_variance | 0.27     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10699    |\n",
      "|    policy_loss        | 0.000605 |\n",
      "|    value_loss         | 0.736    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.13     |\n",
      "|    ep_rew_mean        | -0.32    |\n",
      "| time/                 |          |\n",
      "|    fps                | 377      |\n",
      "|    iterations         | 10800    |\n",
      "|    time_elapsed       | 143      |\n",
      "|    total_timesteps    | 54000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0216  |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10799    |\n",
      "|    policy_loss        | -0.00353 |\n",
      "|    value_loss         | 0.344    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.08      |\n",
      "|    ep_rew_mean        | -0.22     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 10900     |\n",
      "|    time_elapsed       | 144       |\n",
      "|    total_timesteps    | 54500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0141   |\n",
      "|    explained_variance | -2.87e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10899     |\n",
      "|    policy_loss        | -0.00071  |\n",
      "|    value_loss         | 0.245     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.1       |\n",
      "|    ep_rew_mean        | -0.09     |\n",
      "| time/                 |           |\n",
      "|    fps                | 378       |\n",
      "|    iterations         | 11000     |\n",
      "|    time_elapsed       | 145       |\n",
      "|    total_timesteps    | 55000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00242  |\n",
      "|    explained_variance | 0.467     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10999     |\n",
      "|    policy_loss        | -0.000146 |\n",
      "|    value_loss         | 0.475     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.1       |\n",
      "|    ep_rew_mean        | -0.12     |\n",
      "| time/                 |           |\n",
      "|    fps                | 378       |\n",
      "|    iterations         | 11100     |\n",
      "|    time_elapsed       | 146       |\n",
      "|    total_timesteps    | 55500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.032    |\n",
      "|    explained_variance | 0.734     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11099     |\n",
      "|    policy_loss        | -0.000535 |\n",
      "|    value_loss         | 0.144     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.07      |\n",
      "|    ep_rew_mean        | -0.41     |\n",
      "| time/                 |           |\n",
      "|    fps                | 378       |\n",
      "|    iterations         | 11200     |\n",
      "|    time_elapsed       | 147       |\n",
      "|    total_timesteps    | 56000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00446  |\n",
      "|    explained_variance | -0.259    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11199     |\n",
      "|    policy_loss        | -0.000236 |\n",
      "|    value_loss         | 0.841     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1      |\n",
      "|    ep_rew_mean        | -0.22    |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 11300    |\n",
      "|    time_elapsed       | 149      |\n",
      "|    total_timesteps    | 56500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00255 |\n",
      "|    explained_variance | 0.0782   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11299    |\n",
      "|    policy_loss        | 0.000142 |\n",
      "|    value_loss         | 0.895    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.05      |\n",
      "|    ep_rew_mean        | 0         |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 11400     |\n",
      "|    time_elapsed       | 151       |\n",
      "|    total_timesteps    | 57000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0076   |\n",
      "|    explained_variance | -0.213    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11399     |\n",
      "|    policy_loss        | -0.000346 |\n",
      "|    value_loss         | 1.23      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.04      |\n",
      "|    ep_rew_mean        | -0.26     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 11500     |\n",
      "|    time_elapsed       | 152       |\n",
      "|    total_timesteps    | 57500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00153  |\n",
      "|    explained_variance | -0.531    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11499     |\n",
      "|    policy_loss        | -7.53e-05 |\n",
      "|    value_loss         | 0.353     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.05     |\n",
      "|    ep_rew_mean        | 0        |\n",
      "| time/                 |          |\n",
      "|    fps                | 377      |\n",
      "|    iterations         | 11600    |\n",
      "|    time_elapsed       | 153      |\n",
      "|    total_timesteps    | 58000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00188 |\n",
      "|    explained_variance | 0.29     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11599    |\n",
      "|    policy_loss        | 0.000168 |\n",
      "|    value_loss         | 0.654    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.06      |\n",
      "|    ep_rew_mean        | -0.3      |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 11700     |\n",
      "|    time_elapsed       | 154       |\n",
      "|    total_timesteps    | 58500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0231   |\n",
      "|    explained_variance | -0.0367   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11699     |\n",
      "|    policy_loss        | -0.000995 |\n",
      "|    value_loss         | 1.01      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.11      |\n",
      "|    ep_rew_mean        | -0.33     |\n",
      "| time/                 |           |\n",
      "|    fps                | 378       |\n",
      "|    iterations         | 11800     |\n",
      "|    time_elapsed       | 155       |\n",
      "|    total_timesteps    | 59000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0395   |\n",
      "|    explained_variance | -1.01e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11799     |\n",
      "|    policy_loss        | -0.00212  |\n",
      "|    value_loss         | 0.589     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.24     |\n",
      "|    ep_rew_mean        | -0.04    |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 11900    |\n",
      "|    time_elapsed       | 157      |\n",
      "|    total_timesteps    | 59500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0862  |\n",
      "|    explained_variance | -0.537   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11899    |\n",
      "|    policy_loss        | 0.0049   |\n",
      "|    value_loss         | 1.1      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09     |\n",
      "|    ep_rew_mean        | -0.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 12000    |\n",
      "|    time_elapsed       | 158      |\n",
      "|    total_timesteps    | 60000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0421  |\n",
      "|    explained_variance | 0.401    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11999    |\n",
      "|    policy_loss        | -0.00315 |\n",
      "|    value_loss         | 0.221    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07     |\n",
      "|    ep_rew_mean        | -0.09    |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 12100    |\n",
      "|    time_elapsed       | 159      |\n",
      "|    total_timesteps    | 60500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.188   |\n",
      "|    explained_variance | -0.0665  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12099    |\n",
      "|    policy_loss        | 0.196    |\n",
      "|    value_loss         | 1.15     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11     |\n",
      "|    ep_rew_mean        | -0.25    |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 12200    |\n",
      "|    time_elapsed       | 160      |\n",
      "|    total_timesteps    | 61000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.232   |\n",
      "|    explained_variance | -0.203   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12199    |\n",
      "|    policy_loss        | -0.0935  |\n",
      "|    value_loss         | 0.255    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.14     |\n",
      "|    ep_rew_mean        | -0.18    |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 12300    |\n",
      "|    time_elapsed       | 162      |\n",
      "|    total_timesteps    | 61500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.213   |\n",
      "|    explained_variance | 0.314    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12299    |\n",
      "|    policy_loss        | 0.0647   |\n",
      "|    value_loss         | 1.86     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08     |\n",
      "|    ep_rew_mean        | -0.13    |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 12400    |\n",
      "|    time_elapsed       | 163      |\n",
      "|    total_timesteps    | 62000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0199  |\n",
      "|    explained_variance | -0.793   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12399    |\n",
      "|    policy_loss        | 0.00296  |\n",
      "|    value_loss         | 0.488    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.05     |\n",
      "|    ep_rew_mean        | -0.14    |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 12500    |\n",
      "|    time_elapsed       | 165      |\n",
      "|    total_timesteps    | 62500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00778 |\n",
      "|    explained_variance | 0.469    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12499    |\n",
      "|    policy_loss        | 0.00107  |\n",
      "|    value_loss         | 0.512    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.09      |\n",
      "|    ep_rew_mean        | -0.29     |\n",
      "| time/                 |           |\n",
      "|    fps                | 378       |\n",
      "|    iterations         | 12600     |\n",
      "|    time_elapsed       | 166       |\n",
      "|    total_timesteps    | 63000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000156 |\n",
      "|    explained_variance | 0.223     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12599     |\n",
      "|    policy_loss        | 3.91e-06  |\n",
      "|    value_loss         | 0.75      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.09      |\n",
      "|    ep_rew_mean        | 0.01      |\n",
      "| time/                 |           |\n",
      "|    fps                | 379       |\n",
      "|    iterations         | 12700     |\n",
      "|    time_elapsed       | 167       |\n",
      "|    total_timesteps    | 63500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00136  |\n",
      "|    explained_variance | 0.111     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12699     |\n",
      "|    policy_loss        | -4.78e-05 |\n",
      "|    value_loss         | 0.588     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09     |\n",
      "|    ep_rew_mean        | -0.13    |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 12800    |\n",
      "|    time_elapsed       | 168      |\n",
      "|    total_timesteps    | 64000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00419 |\n",
      "|    explained_variance | -0.261   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12799    |\n",
      "|    policy_loss        | 0.000734 |\n",
      "|    value_loss         | 1.23     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.15     |\n",
      "|    ep_rew_mean        | -0.22    |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 12900    |\n",
      "|    time_elapsed       | 169      |\n",
      "|    total_timesteps    | 64500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.124   |\n",
      "|    explained_variance | 0.522    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12899    |\n",
      "|    policy_loss        | 0.151    |\n",
      "|    value_loss         | 0.52     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07     |\n",
      "|    ep_rew_mean        | -0.25    |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 13000    |\n",
      "|    time_elapsed       | 171      |\n",
      "|    total_timesteps    | 65000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.053   |\n",
      "|    explained_variance | 0.809    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12999    |\n",
      "|    policy_loss        | -0.00181 |\n",
      "|    value_loss         | 0.288    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12     |\n",
      "|    ep_rew_mean        | 0.01     |\n",
      "| time/                 |          |\n",
      "|    fps                | 380      |\n",
      "|    iterations         | 13100    |\n",
      "|    time_elapsed       | 172      |\n",
      "|    total_timesteps    | 65500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0155  |\n",
      "|    explained_variance | -0.308   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13099    |\n",
      "|    policy_loss        | 0.00504  |\n",
      "|    value_loss         | 1.4      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11     |\n",
      "|    ep_rew_mean        | -0.19    |\n",
      "| time/                 |          |\n",
      "|    fps                | 380      |\n",
      "|    iterations         | 13200    |\n",
      "|    time_elapsed       | 173      |\n",
      "|    total_timesteps    | 66000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0756  |\n",
      "|    explained_variance | 0.57     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13199    |\n",
      "|    policy_loss        | 0.0235   |\n",
      "|    value_loss         | 0.385    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.08      |\n",
      "|    ep_rew_mean        | -0.05     |\n",
      "| time/                 |           |\n",
      "|    fps                | 379       |\n",
      "|    iterations         | 13300     |\n",
      "|    time_elapsed       | 175       |\n",
      "|    total_timesteps    | 66500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00014  |\n",
      "|    explained_variance | 0.368     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13299     |\n",
      "|    policy_loss        | -1.32e-06 |\n",
      "|    value_loss         | 0.608     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.17     |\n",
      "|    ep_rew_mean        | -0.06    |\n",
      "| time/                 |          |\n",
      "|    fps                | 377      |\n",
      "|    iterations         | 13400    |\n",
      "|    time_elapsed       | 177      |\n",
      "|    total_timesteps    | 67000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00836 |\n",
      "|    explained_variance | -0.427   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13399    |\n",
      "|    policy_loss        | -0.00143 |\n",
      "|    value_loss         | 1.14     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.13     |\n",
      "|    ep_rew_mean        | -0.13    |\n",
      "| time/                 |          |\n",
      "|    fps                | 377      |\n",
      "|    iterations         | 13500    |\n",
      "|    time_elapsed       | 178      |\n",
      "|    total_timesteps    | 67500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0606  |\n",
      "|    explained_variance | 0.239    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13499    |\n",
      "|    policy_loss        | -0.00326 |\n",
      "|    value_loss         | 0.765    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12     |\n",
      "|    ep_rew_mean        | -0.17    |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 13600    |\n",
      "|    time_elapsed       | 179      |\n",
      "|    total_timesteps    | 68000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0626  |\n",
      "|    explained_variance | -0.25    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13599    |\n",
      "|    policy_loss        | 0.0315   |\n",
      "|    value_loss         | 1.79     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12     |\n",
      "|    ep_rew_mean        | -0.31    |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 13700    |\n",
      "|    time_elapsed       | 181      |\n",
      "|    total_timesteps    | 68500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00379 |\n",
      "|    explained_variance | 0.161    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13699    |\n",
      "|    policy_loss        | 0.000547 |\n",
      "|    value_loss         | 0.992    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.07      |\n",
      "|    ep_rew_mean        | -0.17     |\n",
      "| time/                 |           |\n",
      "|    fps                | 378       |\n",
      "|    iterations         | 13800     |\n",
      "|    time_elapsed       | 182       |\n",
      "|    total_timesteps    | 69000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.65e-05 |\n",
      "|    explained_variance | 0.0533    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13799     |\n",
      "|    policy_loss        | 4.97e-07  |\n",
      "|    value_loss         | 0.812     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.14     |\n",
      "|    ep_rew_mean        | -0.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 13900    |\n",
      "|    time_elapsed       | 183      |\n",
      "|    total_timesteps    | 69500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.14    |\n",
      "|    explained_variance | -4.85    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13899    |\n",
      "|    policy_loss        | 0.0661   |\n",
      "|    value_loss         | 0.684    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.13      |\n",
      "|    ep_rew_mean        | -0.32     |\n",
      "| time/                 |           |\n",
      "|    fps                | 379       |\n",
      "|    iterations         | 14000     |\n",
      "|    time_elapsed       | 184       |\n",
      "|    total_timesteps    | 70000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00138  |\n",
      "|    explained_variance | -0.0136   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13999     |\n",
      "|    policy_loss        | -4.01e-05 |\n",
      "|    value_loss         | 0.701     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.02      |\n",
      "|    ep_rew_mean        | -0.04     |\n",
      "| time/                 |           |\n",
      "|    fps                | 379       |\n",
      "|    iterations         | 14100     |\n",
      "|    time_elapsed       | 185       |\n",
      "|    total_timesteps    | 70500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00628  |\n",
      "|    explained_variance | 0.41      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14099     |\n",
      "|    policy_loss        | -0.000461 |\n",
      "|    value_loss         | 0.582     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12     |\n",
      "|    ep_rew_mean        | -0.15    |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 14200    |\n",
      "|    time_elapsed       | 186      |\n",
      "|    total_timesteps    | 71000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0017  |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14199    |\n",
      "|    policy_loss        | 0.000314 |\n",
      "|    value_loss         | 0.647    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.14     |\n",
      "|    ep_rew_mean        | -0.12    |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 14300    |\n",
      "|    time_elapsed       | 188      |\n",
      "|    total_timesteps    | 71500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00712 |\n",
      "|    explained_variance | 0.369    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14299    |\n",
      "|    policy_loss        | 6.11e-05 |\n",
      "|    value_loss         | 0.656    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.05     |\n",
      "|    ep_rew_mean        | -0.11    |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 14400    |\n",
      "|    time_elapsed       | 189      |\n",
      "|    total_timesteps    | 72000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00091 |\n",
      "|    explained_variance | 0.834    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14399    |\n",
      "|    policy_loss        | 1.71e-05 |\n",
      "|    value_loss         | 0.167    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11     |\n",
      "|    ep_rew_mean        | -0.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 14500    |\n",
      "|    time_elapsed       | 191      |\n",
      "|    total_timesteps    | 72500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0425  |\n",
      "|    explained_variance | 0.292    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14499    |\n",
      "|    policy_loss        | -0.00167 |\n",
      "|    value_loss         | 0.869    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.09      |\n",
      "|    ep_rew_mean        | -0.33     |\n",
      "| time/                 |           |\n",
      "|    fps                | 379       |\n",
      "|    iterations         | 14600     |\n",
      "|    time_elapsed       | 192       |\n",
      "|    total_timesteps    | 73000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0023   |\n",
      "|    explained_variance | 0.831     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14599     |\n",
      "|    policy_loss        | -8.71e-05 |\n",
      "|    value_loss         | 0.264     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.14     |\n",
      "|    ep_rew_mean        | -0.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 14700    |\n",
      "|    time_elapsed       | 193      |\n",
      "|    total_timesteps    | 73500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00931 |\n",
      "|    explained_variance | -0.115   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14699    |\n",
      "|    policy_loss        | 0.00165  |\n",
      "|    value_loss         | 0.815    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08     |\n",
      "|    ep_rew_mean        | -0.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 14800    |\n",
      "|    time_elapsed       | 194      |\n",
      "|    total_timesteps    | 74000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00282 |\n",
      "|    explained_variance | 0.469    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14799    |\n",
      "|    policy_loss        | 0.000128 |\n",
      "|    value_loss         | 0.589    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09     |\n",
      "|    ep_rew_mean        | -0.14    |\n",
      "| time/                 |          |\n",
      "|    fps                | 380      |\n",
      "|    iterations         | 14900    |\n",
      "|    time_elapsed       | 196      |\n",
      "|    total_timesteps    | 74500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00462 |\n",
      "|    explained_variance | -0.927   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14899    |\n",
      "|    policy_loss        | 0.00109  |\n",
      "|    value_loss         | 1.43     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.09      |\n",
      "|    ep_rew_mean        | -0.12     |\n",
      "| time/                 |           |\n",
      "|    fps                | 380       |\n",
      "|    iterations         | 15000     |\n",
      "|    time_elapsed       | 197       |\n",
      "|    total_timesteps    | 75000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0108   |\n",
      "|    explained_variance | -0.496    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14999     |\n",
      "|    policy_loss        | -5.85e-05 |\n",
      "|    value_loss         | 1.11      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.1       |\n",
      "|    ep_rew_mean        | -0.22     |\n",
      "| time/                 |           |\n",
      "|    fps                | 380       |\n",
      "|    iterations         | 15100     |\n",
      "|    time_elapsed       | 198       |\n",
      "|    total_timesteps    | 75500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00115  |\n",
      "|    explained_variance | 0.19      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15099     |\n",
      "|    policy_loss        | -0.000103 |\n",
      "|    value_loss         | 0.853     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.21     |\n",
      "|    ep_rew_mean        | 0.01     |\n",
      "| time/                 |          |\n",
      "|    fps                | 381      |\n",
      "|    iterations         | 15200    |\n",
      "|    time_elapsed       | 199      |\n",
      "|    total_timesteps    | 76000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0197  |\n",
      "|    explained_variance | -0.114   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15199    |\n",
      "|    policy_loss        | 0.00361  |\n",
      "|    value_loss         | 0.703    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.1       |\n",
      "|    ep_rew_mean        | -0.16     |\n",
      "| time/                 |           |\n",
      "|    fps                | 381       |\n",
      "|    iterations         | 15300     |\n",
      "|    time_elapsed       | 200       |\n",
      "|    total_timesteps    | 76500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000191 |\n",
      "|    explained_variance | -0.92     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15299     |\n",
      "|    policy_loss        | 2.39e-05  |\n",
      "|    value_loss         | 1.82      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.07      |\n",
      "|    ep_rew_mean        | -0.3      |\n",
      "| time/                 |           |\n",
      "|    fps                | 381       |\n",
      "|    iterations         | 15400     |\n",
      "|    time_elapsed       | 202       |\n",
      "|    total_timesteps    | 77000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000199 |\n",
      "|    explained_variance | 0.671     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15399     |\n",
      "|    policy_loss        | -8.86e-06 |\n",
      "|    value_loss         | 0.326     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.13     |\n",
      "|    ep_rew_mean        | -0.16    |\n",
      "| time/                 |          |\n",
      "|    fps                | 380      |\n",
      "|    iterations         | 15500    |\n",
      "|    time_elapsed       | 203      |\n",
      "|    total_timesteps    | 77500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0529  |\n",
      "|    explained_variance | -0.0416  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15499    |\n",
      "|    policy_loss        | -0.00281 |\n",
      "|    value_loss         | 0.673    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.13      |\n",
      "|    ep_rew_mean        | -0.23     |\n",
      "| time/                 |           |\n",
      "|    fps                | 378       |\n",
      "|    iterations         | 15600     |\n",
      "|    time_elapsed       | 205       |\n",
      "|    total_timesteps    | 78000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -8.49e-05 |\n",
      "|    explained_variance | 0.00335   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15599     |\n",
      "|    policy_loss        | -4.56e-06 |\n",
      "|    value_loss         | 0.663     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.06      |\n",
      "|    ep_rew_mean        | -0.21     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 15700     |\n",
      "|    time_elapsed       | 208       |\n",
      "|    total_timesteps    | 78500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000284 |\n",
      "|    explained_variance | 0.669     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15699     |\n",
      "|    policy_loss        | -1.27e-05 |\n",
      "|    value_loss         | 0.579     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.05      |\n",
      "|    ep_rew_mean        | -0.25     |\n",
      "| time/                 |           |\n",
      "|    fps                | 375       |\n",
      "|    iterations         | 15800     |\n",
      "|    time_elapsed       | 210       |\n",
      "|    total_timesteps    | 79000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000757 |\n",
      "|    explained_variance | -0.138    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15799     |\n",
      "|    policy_loss        | 1.53e-06  |\n",
      "|    value_loss         | 0.645     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.12      |\n",
      "|    ep_rew_mean        | -0.2      |\n",
      "| time/                 |           |\n",
      "|    fps                | 375       |\n",
      "|    iterations         | 15900     |\n",
      "|    time_elapsed       | 211       |\n",
      "|    total_timesteps    | 79500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000185 |\n",
      "|    explained_variance | 0.767     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15899     |\n",
      "|    policy_loss        | -8.73e-06 |\n",
      "|    value_loss         | 0.424     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07     |\n",
      "|    ep_rew_mean        | -0.08    |\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 16000    |\n",
      "|    time_elapsed       | 212      |\n",
      "|    total_timesteps    | 80000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0837  |\n",
      "|    explained_variance | 0.346    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15999    |\n",
      "|    policy_loss        | -0.00682 |\n",
      "|    value_loss         | 0.176    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.34     |\n",
      "|    ep_rew_mean        | -0.18    |\n",
      "| time/                 |          |\n",
      "|    fps                | 376      |\n",
      "|    iterations         | 16100    |\n",
      "|    time_elapsed       | 214      |\n",
      "|    total_timesteps    | 80500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.237   |\n",
      "|    explained_variance | 0.746    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16099    |\n",
      "|    policy_loss        | -0.175   |\n",
      "|    value_loss         | 0.268    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33     |\n",
      "|    ep_rew_mean        | -0.23    |\n",
      "| time/                 |          |\n",
      "|    fps                | 376      |\n",
      "|    iterations         | 16200    |\n",
      "|    time_elapsed       | 215      |\n",
      "|    total_timesteps    | 81000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.16    |\n",
      "|    explained_variance | 0.0502   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16199    |\n",
      "|    policy_loss        | -0.391   |\n",
      "|    value_loss         | 1.23     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.45     |\n",
      "|    ep_rew_mean        | -0.12    |\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 16300    |\n",
      "|    time_elapsed       | 216      |\n",
      "|    total_timesteps    | 81500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0541  |\n",
      "|    explained_variance | 0.691    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16299    |\n",
      "|    policy_loss        | 0.00529  |\n",
      "|    value_loss         | 0.239    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.6      |\n",
      "|    ep_rew_mean        | -0.17    |\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 16400    |\n",
      "|    time_elapsed       | 218      |\n",
      "|    total_timesteps    | 82000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.155   |\n",
      "|    explained_variance | 0.908    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16399    |\n",
      "|    policy_loss        | -0.12    |\n",
      "|    value_loss         | 0.349    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.59     |\n",
      "|    ep_rew_mean        | 0.01     |\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 16500    |\n",
      "|    time_elapsed       | 219      |\n",
      "|    total_timesteps    | 82500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0357  |\n",
      "|    explained_variance | -0.37    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16499    |\n",
      "|    policy_loss        | -0.00577 |\n",
      "|    value_loss         | 0.351    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.55      |\n",
      "|    ep_rew_mean        | -0.19     |\n",
      "| time/                 |           |\n",
      "|    fps                | 376       |\n",
      "|    iterations         | 16600     |\n",
      "|    time_elapsed       | 220       |\n",
      "|    total_timesteps    | 83000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0115   |\n",
      "|    explained_variance | -0.0576   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16599     |\n",
      "|    policy_loss        | -0.000661 |\n",
      "|    value_loss         | 0.649     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.58     |\n",
      "|    ep_rew_mean        | -0.02    |\n",
      "| time/                 |          |\n",
      "|    fps                | 376      |\n",
      "|    iterations         | 16700    |\n",
      "|    time_elapsed       | 221      |\n",
      "|    total_timesteps    | 83500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0666  |\n",
      "|    explained_variance | -0.0641  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16699    |\n",
      "|    policy_loss        | 0.0188   |\n",
      "|    value_loss         | 0.912    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.69      |\n",
      "|    ep_rew_mean        | -0.06     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 16800     |\n",
      "|    time_elapsed       | 222       |\n",
      "|    total_timesteps    | 84000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00252  |\n",
      "|    explained_variance | 0.8       |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16799     |\n",
      "|    policy_loss        | -0.000125 |\n",
      "|    value_loss         | 0.14      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.44      |\n",
      "|    ep_rew_mean        | -0.1      |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 16900     |\n",
      "|    time_elapsed       | 223       |\n",
      "|    total_timesteps    | 84500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00163  |\n",
      "|    explained_variance | -0.0818   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16899     |\n",
      "|    policy_loss        | -0.000115 |\n",
      "|    value_loss         | 0.714     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.47      |\n",
      "|    ep_rew_mean        | -0.07     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 17000     |\n",
      "|    time_elapsed       | 225       |\n",
      "|    total_timesteps    | 85000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.03e-05 |\n",
      "|    explained_variance | 0.396     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16999     |\n",
      "|    policy_loss        | -2.44e-06 |\n",
      "|    value_loss         | 0.564     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.61      |\n",
      "|    ep_rew_mean        | -0.09     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 17100     |\n",
      "|    time_elapsed       | 226       |\n",
      "|    total_timesteps    | 85500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000209 |\n",
      "|    explained_variance | 0.394     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17099     |\n",
      "|    policy_loss        | -2.03e-07 |\n",
      "|    value_loss         | 0.596     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.7      |\n",
      "|    ep_rew_mean        | -0.13    |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 17200    |\n",
      "|    time_elapsed       | 227      |\n",
      "|    total_timesteps    | 86000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0964  |\n",
      "|    explained_variance | -0.23    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17199    |\n",
      "|    policy_loss        | 0.0325   |\n",
      "|    value_loss         | 1.16     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.52     |\n",
      "|    ep_rew_mean        | -0.12    |\n",
      "| time/                 |          |\n",
      "|    fps                | 377      |\n",
      "|    iterations         | 17300    |\n",
      "|    time_elapsed       | 228      |\n",
      "|    total_timesteps    | 86500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0939  |\n",
      "|    explained_variance | 0.651    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17299    |\n",
      "|    policy_loss        | -0.0126  |\n",
      "|    value_loss         | 0.256    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.53      |\n",
      "|    ep_rew_mean        | -0.04     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 17400     |\n",
      "|    time_elapsed       | 230       |\n",
      "|    total_timesteps    | 87000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000261 |\n",
      "|    explained_variance | 0.848     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17399     |\n",
      "|    policy_loss        | -1.07e-05 |\n",
      "|    value_loss         | 0.174     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.63      |\n",
      "|    ep_rew_mean        | -0.09     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 17500     |\n",
      "|    time_elapsed       | 231       |\n",
      "|    total_timesteps    | 87500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00911  |\n",
      "|    explained_variance | 0.207     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17499     |\n",
      "|    policy_loss        | -0.000562 |\n",
      "|    value_loss         | 0.658     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.43     |\n",
      "|    ep_rew_mean        | -0.03    |\n",
      "| time/                 |          |\n",
      "|    fps                | 377      |\n",
      "|    iterations         | 17600    |\n",
      "|    time_elapsed       | 232      |\n",
      "|    total_timesteps    | 88000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.108   |\n",
      "|    explained_variance | 0.59     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17599    |\n",
      "|    policy_loss        | -0.175   |\n",
      "|    value_loss         | 0.0827   |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.54      |\n",
      "|    ep_rew_mean        | -0.12     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 17700     |\n",
      "|    time_elapsed       | 234       |\n",
      "|    total_timesteps    | 88500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00402  |\n",
      "|    explained_variance | 0.487     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17699     |\n",
      "|    policy_loss        | -0.000106 |\n",
      "|    value_loss         | 0.293     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.47     |\n",
      "|    ep_rew_mean        | -0.21    |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 17800    |\n",
      "|    time_elapsed       | 235      |\n",
      "|    total_timesteps    | 89000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0946  |\n",
      "|    explained_variance | 0.919    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17799    |\n",
      "|    policy_loss        | -0.015   |\n",
      "|    value_loss         | 0.117    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.65      |\n",
      "|    ep_rew_mean        | 0.01      |\n",
      "| time/                 |           |\n",
      "|    fps                | 378       |\n",
      "|    iterations         | 17900     |\n",
      "|    time_elapsed       | 236       |\n",
      "|    total_timesteps    | 89500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.51e-05 |\n",
      "|    explained_variance | 0.0103    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17899     |\n",
      "|    policy_loss        | -1.48e-07 |\n",
      "|    value_loss         | 1.01      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.58      |\n",
      "|    ep_rew_mean        | -0.09     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 18000     |\n",
      "|    time_elapsed       | 238       |\n",
      "|    total_timesteps    | 90000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000155 |\n",
      "|    explained_variance | 0.176     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17999     |\n",
      "|    policy_loss        | -1.88e-05 |\n",
      "|    value_loss         | 0.703     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.59      |\n",
      "|    ep_rew_mean        | 0.02      |\n",
      "| time/                 |           |\n",
      "|    fps                | 375       |\n",
      "|    iterations         | 18100     |\n",
      "|    time_elapsed       | 240       |\n",
      "|    total_timesteps    | 90500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.57e-05 |\n",
      "|    explained_variance | -3.3e+03  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18099     |\n",
      "|    policy_loss        | -5.4e-06  |\n",
      "|    value_loss         | 0.535     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.51     |\n",
      "|    ep_rew_mean        | -0.25    |\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 18200    |\n",
      "|    time_elapsed       | 242      |\n",
      "|    total_timesteps    | 91000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.229   |\n",
      "|    explained_variance | -130     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18199    |\n",
      "|    policy_loss        | -0.0444  |\n",
      "|    value_loss         | 0.173    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.49      |\n",
      "|    ep_rew_mean        | 0.02      |\n",
      "| time/                 |           |\n",
      "|    fps                | 373       |\n",
      "|    iterations         | 18300     |\n",
      "|    time_elapsed       | 244       |\n",
      "|    total_timesteps    | 91500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000124 |\n",
      "|    explained_variance | 0.459     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18299     |\n",
      "|    policy_loss        | 4.19e-06  |\n",
      "|    value_loss         | 0.937     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.49     |\n",
      "|    ep_rew_mean        | -0.06    |\n",
      "| time/                 |          |\n",
      "|    fps                | 372      |\n",
      "|    iterations         | 18400    |\n",
      "|    time_elapsed       | 246      |\n",
      "|    total_timesteps    | 92000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.106   |\n",
      "|    explained_variance | -0.491   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18399    |\n",
      "|    policy_loss        | 0.319    |\n",
      "|    value_loss         | 1.65     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.56      |\n",
      "|    ep_rew_mean        | -0.23     |\n",
      "| time/                 |           |\n",
      "|    fps                | 372       |\n",
      "|    iterations         | 18500     |\n",
      "|    time_elapsed       | 248       |\n",
      "|    total_timesteps    | 92500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00101  |\n",
      "|    explained_variance | 0.853     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18499     |\n",
      "|    policy_loss        | -6.37e-05 |\n",
      "|    value_loss         | 0.18      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.56     |\n",
      "|    ep_rew_mean        | -0.02    |\n",
      "| time/                 |          |\n",
      "|    fps                | 373      |\n",
      "|    iterations         | 18600    |\n",
      "|    time_elapsed       | 249      |\n",
      "|    total_timesteps    | 93000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0106  |\n",
      "|    explained_variance | 0.46     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18599    |\n",
      "|    policy_loss        | 0.00114  |\n",
      "|    value_loss         | 0.467    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.58      |\n",
      "|    ep_rew_mean        | -0.16     |\n",
      "| time/                 |           |\n",
      "|    fps                | 373       |\n",
      "|    iterations         | 18700     |\n",
      "|    time_elapsed       | 250       |\n",
      "|    total_timesteps    | 93500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000498 |\n",
      "|    explained_variance | 0.433     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18699     |\n",
      "|    policy_loss        | -9.6e-06  |\n",
      "|    value_loss         | 0.637     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.35     |\n",
      "|    ep_rew_mean        | -0.29    |\n",
      "| time/                 |          |\n",
      "|    fps                | 373      |\n",
      "|    iterations         | 18800    |\n",
      "|    time_elapsed       | 251      |\n",
      "|    total_timesteps    | 94000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00594 |\n",
      "|    explained_variance | 0.167    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18799    |\n",
      "|    policy_loss        | 0.0012   |\n",
      "|    value_loss         | 0.804    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.5       |\n",
      "|    ep_rew_mean        | -0.01     |\n",
      "| time/                 |           |\n",
      "|    fps                | 373       |\n",
      "|    iterations         | 18900     |\n",
      "|    time_elapsed       | 252       |\n",
      "|    total_timesteps    | 94500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000341 |\n",
      "|    explained_variance | 0.305     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18899     |\n",
      "|    policy_loss        | 3.64e-05  |\n",
      "|    value_loss         | 0.732     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.51     |\n",
      "|    ep_rew_mean        | -0.23    |\n",
      "| time/                 |          |\n",
      "|    fps                | 373      |\n",
      "|    iterations         | 19000    |\n",
      "|    time_elapsed       | 254      |\n",
      "|    total_timesteps    | 95000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.132   |\n",
      "|    explained_variance | 0.295    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18999    |\n",
      "|    policy_loss        | 0.266    |\n",
      "|    value_loss         | 0.561    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.58      |\n",
      "|    ep_rew_mean        | -0.14     |\n",
      "| time/                 |           |\n",
      "|    fps                | 374       |\n",
      "|    iterations         | 19100     |\n",
      "|    time_elapsed       | 255       |\n",
      "|    total_timesteps    | 95500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000157 |\n",
      "|    explained_variance | -0.0784   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19099     |\n",
      "|    policy_loss        | -3.85e-06 |\n",
      "|    value_loss         | 0.6       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.46     |\n",
      "|    ep_rew_mean        | -0.08    |\n",
      "| time/                 |          |\n",
      "|    fps                | 374      |\n",
      "|    iterations         | 19200    |\n",
      "|    time_elapsed       | 256      |\n",
      "|    total_timesteps    | 96000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00135 |\n",
      "|    explained_variance | 0.261    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19199    |\n",
      "|    policy_loss        | 0.000116 |\n",
      "|    value_loss         | 0.198    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.6      |\n",
      "|    ep_rew_mean        | -0.05    |\n",
      "| time/                 |          |\n",
      "|    fps                | 373      |\n",
      "|    iterations         | 19300    |\n",
      "|    time_elapsed       | 258      |\n",
      "|    total_timesteps    | 96500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.261   |\n",
      "|    explained_variance | 0.556    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19299    |\n",
      "|    policy_loss        | 0.0584   |\n",
      "|    value_loss         | 0.316    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.65     |\n",
      "|    ep_rew_mean        | 0        |\n",
      "| time/                 |          |\n",
      "|    fps                | 373      |\n",
      "|    iterations         | 19400    |\n",
      "|    time_elapsed       | 259      |\n",
      "|    total_timesteps    | 97000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0195  |\n",
      "|    explained_variance | 0.0236   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19399    |\n",
      "|    policy_loss        | -0.00213 |\n",
      "|    value_loss         | 0.841    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.67     |\n",
      "|    ep_rew_mean        | -0.11    |\n",
      "| time/                 |          |\n",
      "|    fps                | 373      |\n",
      "|    iterations         | 19500    |\n",
      "|    time_elapsed       | 260      |\n",
      "|    total_timesteps    | 97500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0321  |\n",
      "|    explained_variance | 0.686    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19499    |\n",
      "|    policy_loss        | -0.0025  |\n",
      "|    value_loss         | 0.379    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.44      |\n",
      "|    ep_rew_mean        | -0.34     |\n",
      "| time/                 |           |\n",
      "|    fps                | 374       |\n",
      "|    iterations         | 19600     |\n",
      "|    time_elapsed       | 261       |\n",
      "|    total_timesteps    | 98000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0035   |\n",
      "|    explained_variance | 0.861     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19599     |\n",
      "|    policy_loss        | -0.000233 |\n",
      "|    value_loss         | 0.368     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.39     |\n",
      "|    ep_rew_mean        | -0.02    |\n",
      "| time/                 |          |\n",
      "|    fps                | 374      |\n",
      "|    iterations         | 19700    |\n",
      "|    time_elapsed       | 263      |\n",
      "|    total_timesteps    | 98500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0226  |\n",
      "|    explained_variance | 0.576    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19699    |\n",
      "|    policy_loss        | -0.00446 |\n",
      "|    value_loss         | 0.446    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.49      |\n",
      "|    ep_rew_mean        | -0.05     |\n",
      "| time/                 |           |\n",
      "|    fps                | 374       |\n",
      "|    iterations         | 19800     |\n",
      "|    time_elapsed       | 264       |\n",
      "|    total_timesteps    | 99000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000527 |\n",
      "|    explained_variance | -0.143    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19799     |\n",
      "|    policy_loss        | 7.99e-05  |\n",
      "|    value_loss         | 0.988     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.35     |\n",
      "|    ep_rew_mean        | -0.11    |\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 19900    |\n",
      "|    time_elapsed       | 265      |\n",
      "|    total_timesteps    | 99500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00252 |\n",
      "|    explained_variance | 0.181    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19899    |\n",
      "|    policy_loss        | 0.000124 |\n",
      "|    value_loss         | 0.337    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.43      |\n",
      "|    ep_rew_mean        | 0.03      |\n",
      "| time/                 |           |\n",
      "|    fps                | 375       |\n",
      "|    iterations         | 20000     |\n",
      "|    time_elapsed       | 266       |\n",
      "|    total_timesteps    | 100000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00219  |\n",
      "|    explained_variance | 0.791     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19999     |\n",
      "|    policy_loss        | -0.000154 |\n",
      "|    value_loss         | 0.388     |\n",
      "-------------------------------------\n",
      "Promedio de recompensa después del entrenamiento: -0.1000\n",
      "Desviación estándar de recompensa: 0.9434\n"
     ]
    }
   ],
   "source": [
    "class FlattenObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(FlattenObservation, self).__init__(env)\n",
    "        self.observation_space = MultiDiscrete(np.array([32, 11, 2]))\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.array(observation).flatten()\n",
    "\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "env = FlattenObservation(env)\n",
    "\n",
    "vec_env = make_vec_env(lambda: env, n_envs=1)\n",
    "\n",
    "model = A2C(\"MlpPolicy\", vec_env, verbose=1) #se escoge A2C\n",
    "\n",
    "model.learn(total_timesteps=100000)\n",
    "\n",
    "model.save(\"a2c_blackjack_model\")\n",
    "\n",
    "episodes = 100\n",
    "rewards = []\n",
    "for _ in range(episodes):\n",
    "    obs = vec_env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, _ = vec_env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "print(f\"Promedio de recompensa después del entrenamiento: {np.mean(rewards):.4f}\")\n",
    "print(f\"Desviación estándar de recompensa: {np.std(rewards):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-bpdb8wZID1"
   },
   "source": [
    "#### **1.1.4 Evaluación de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.1.2 pero utilizando el modelo entrenado. ¿Cómo es el performance de su agente? ¿Es mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-d7d8GFf7F6",
    "outputId": "7ebbf515-5713-45d5-9403-f9c6045eeda6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de las recompensas: -0.1092\n",
      "Desviación estándar de las recompensas: 0.9483\n"
     ]
    }
   ],
   "source": [
    "episodes = 5000\n",
    "rewards = []\n",
    "\n",
    "for _ in range(episodes):\n",
    "    obs = vec_env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, _= vec_env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "print(f\"Promedio de las recompensas: {np.mean(rewards):.4f}\")\n",
    "print(f\"Desviación estándar de las recompensas: {np.std(rewards):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Es9c6d-8MTby"
   },
   "source": [
    "Como era de esperar, el promedio de recompensas aumentó, pero lo que no era tan predecible es que la desviación estándar no disminuyera, por lo que se tiene que el modelo no alcanza una estrategia consistente en sus resultados. De este tenemos que hay mejores formas de jugar que al azar, pero no está claro si haya una forma de consistentemente ganar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO-EsAaPAYEm"
   },
   "source": [
    "#### **1.1.5 Estudio de acciones (0.2 puntos)**\n",
    "\n",
    "Genere una función que reciba un estado y retorne la accion del agente. Luego, use esta función para entregar la acción escogida frente a los siguientes escenarios:\n",
    "\n",
    "- Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene tiene un as\n",
    "- Suma de cartas del agente es 19, dealer muestra un 3, agente tiene tiene un as\n",
    "\n",
    "¿Son coherentes sus acciones con las reglas del juego?\n",
    "\n",
    "Hint: ¿A que clase de python pertenecen los estados? Pruebe a usar el método `.reset` para saberlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fh8XlGyzwtRp",
    "outputId": "ef953f5e-5d8f-4104-cacd-82281d68e908"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acción para el escenario 1: 1 (0: Stick, 1: Hit)\n",
      "Acción para el escenario 2: 0 (0: Stick, 1: Hit)\n"
     ]
    }
   ],
   "source": [
    "def get_action(agent, state):\n",
    "    \"\"\"\n",
    "    Función para obtener la acción del agente dado un estado específico.\n",
    "\n",
    "    Args:\n",
    "        agent: Modelo entrenado (e.g., A2C).\n",
    "        state: Estado del ambiente (tuple con la suma del jugador, carta del dealer y si tiene un as usable).\n",
    "\n",
    "    Returns:\n",
    "        Acción (0: Stick, 1: Hit).\n",
    "    \"\"\"\n",
    "    state = np.array(state).reshape(1, -1)\n",
    "    action, _ = agent.predict(state, deterministic=True)\n",
    "    return action[0]\n",
    "\n",
    "scenario_1 = (6, 7, 0)   # suma del agente: 6, dealer muestra: 7, no tiene as\n",
    "scenario_2 = (19, 3, 1)  # suma del agente: 19, dealer muestra: 3, tiene un as usable\n",
    "\n",
    "action_1 = get_action(model, scenario_1)\n",
    "action_2 = get_action(model, scenario_2)\n",
    "\n",
    "print(f\"Acción para el escenario 1: {action_1} (0: Stick, 1: Hit)\")\n",
    "print(f\"Acción para el escenario 2: {action_2} (0: Stick, 1: Hit)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EN1SoMCdMkP8"
   },
   "source": [
    "Parecen ser razonables, veamos cada acción:\n",
    "\n",
    "1) Estado: Agente tiene una suma baja (6), mientras que el dealer muestra un 7.\n",
    "En este caso lo esperado esq el agente debería pedir (hit, 1) para intentar mejorar su suma, ya que pararse (stick, 0) con 6 sería casi seguro una pérdida.\n",
    "\n",
    "2) Estado: Agente tiene 19 con un as usable, y el dealer muestra un 3.\n",
    "Se espera que el agente se quedara (stick, 0), ya que 19 es un puntaje fuerte y arriesgarse con un hit podría llevarlo a pasarse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEqCTqqroh03"
   },
   "source": [
    "### **1.2 LunarLander**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://i.redd.it/097t6tk29zf51.jpg\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la sección 2.1, en esta sección usted se encargará de implementar una gente de RL que pueda resolver el ambiente `LunarLander`.\n",
    "\n",
    "Comencemos preparando el ambiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nvQUyuZ_FtZ4"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\", continuous = True) # notar el parámetro continuous = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBU4lGX3wpN6"
   },
   "source": [
    "Noten que se especifica el parámetro `continuous = True`. ¿Que implicancias tiene esto sobre el ambiente?\n",
    "\n",
    "Además, se le facilita la función `export_gif` para el ejercicio 2.2.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "bRiWpSo9yfr9"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "def export_gif(model, n = 5):\n",
    "  '''\n",
    "  función que exporta a gif el comportamiento del agente en n episodios\n",
    "  '''\n",
    "  images = []\n",
    "  for episode in range(n):\n",
    "    obs = model.env.reset()\n",
    "    img = model.env.render()\n",
    "    done = False\n",
    "    while not done:\n",
    "      images.append(img)\n",
    "      action, _ = model.predict(obs)\n",
    "      obs, reward, done, info = model.env.step(action)\n",
    "      img = model.env.render(mode=\"rgb_array\")\n",
    "\n",
    "  imageio.mimsave(\"agent_performance.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sk5VJVppXh3N"
   },
   "source": [
    "#### **1.2.1 Descripción de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripción sobre el ambiente [LunarLander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) y su formulación en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas. ¿Como se distinguen las acciones de este ambiente en comparación a `Blackjack`?\n",
    "\n",
    "Nota: recuerde que se especificó el parámetro `continuous = True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yb-u9LUE8O9a"
   },
   "source": [
    "`escriba su respuesta acá`\n",
    "\n",
    "El ambiente **LunarLander** modela el problema de optimización de trayectorias de un cohete para aterrizar en una plataforma. Está formulado como un MDP:\n",
    "\n",
    "- **Estados**:  \n",
    "  El espacio de observación es continuo, representado por un vector de 8 dimensiones:  \n",
    "  1. Coordenadas `x` e `y` del cohete.  \n",
    "  2. Velocidades lineales en `x` e `y`.  \n",
    "  3. Ángulo y velocidad angular.  \n",
    "  4. Dos booleanos indicando si las patas del cohete están en contacto con el suelo.  \n",
    "\n",
    "- **Acciones**:  \n",
    "  Con el parámetro `continuous=True`, el espacio de acciones es continuo y consiste en un vector de dos dimensiones:  \n",
    "  1. Intensidad del motor principal (valor entre -1 y 1).  \n",
    "  2. Intensidad de los propulsores laterales (valor entre -1 y 1).  \n",
    "\n",
    "  Las acciones determinan el uso de los motores:  \n",
    "  - Motor principal solo funciona con al menos el 50% de potencia.  \n",
    "  - Propulsores laterales no se activan entre valores de -0.5 y 0.5.  \n",
    "\n",
    "- **Recompensas**:  \n",
    "  Recompensas se otorgan según:  \n",
    "  - Cercanía del cohete a la plataforma.  \n",
    "  - Reducción de velocidad (lineal y angular).  \n",
    "  - Estabilidad en el ángulo.  \n",
    "  - Contacto de las patas con el suelo (+10 por cada pata).  \n",
    "  - Penalizaciones por el uso de motores (0.03 por cada frame para propulsores laterales y 0.3 para el motor principal).  \n",
    "  - Recompensas grandes (+100 por aterrizaje exitoso, -100 por accidente).  \n",
    "\n",
    "- **Término**:  \n",
    "  El episodio termina si:  \n",
    "  - El cohete se estrella.  \n",
    "  - Sale del área visible.  \n",
    "  - Aterriza con éxito y se estabiliza.  \n",
    "\n",
    "\n",
    "Notemos que a diferencia de BlackJack, en este caso se tiene que el espacio de estados es diferente ya que en el primer caso, el BlackJack tiene una combinación finita de cartas y estados (i.e espacio discreto) mientras que LunarLander cuenta con un espacio continuo de 8 dimensiones. Por otro lado, el espacio de acciones es diferente en cada caso, en BlackJack el espacio de acciones es discreto y cuenta con dos posibilidades, pararse o pedir, LunarLander tiene un espacio de acciones continuo bidimensional para regular la potencia de los motores. Por último, las recompensas son distintas ya que en el BlackJack las recompensas se entregan únicamente al final del episodio y en LunarLander se entregan acumulativamente, dándose en cada paso del episodio según el desempeño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YChodtNQwzG2"
   },
   "source": [
    "#### **1.2.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulación 10 veces y reporte el promedio y desviación de las recompensas. ¿Cómo calificaría el performance de esta política?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5bwc3A0GX7a8",
    "outputId": "d0ffd971-2be7-4725-9eb2-43a3ebbbc92b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensas: -240.96\n",
      "Desviación estándar de recompensas: 99.86\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10\n",
    "rewards = []\n",
    "\n",
    "# con acciones aleatorias\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # acción al azar\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "print(f\"Promedio de recompensas: {np.mean(rewards):.2f}\")\n",
    "print(f\"Desviación estándar de recompensas: {np.std(rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyySYhUjNrwn"
   },
   "source": [
    "Esta estrategia es muy mala, la recompensa promedio negativa indica que en general se llega a acciodentes o que se hizo muchas cosas negativas para lo buscado, el cohete en la mayoría de casos no aterriza y en general la recompensa final es negativa, ya que pese a que la desviación estándar es alta, no es lo suficiente como para que esta estrategia se acerque consistentemente a resultados de recompensa positiva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQrZVQflX_5f"
   },
   "source": [
    "#### **1.2.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `LunarLander` **usando 10000 timesteps de entrenamiento**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_6Ia9uoF7Hs",
    "outputId": "89238d00-28dc-4b33-f975-b5e4f709b4bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 115      |\n",
      "|    ep_rew_mean        | -219     |\n",
      "| time/                 |          |\n",
      "|    fps                | 219      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.85    |\n",
      "|    explained_variance | -0.00148 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 26.8     |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 80.4     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 145      |\n",
      "|    ep_rew_mean        | -342     |\n",
      "| time/                 |          |\n",
      "|    fps                | 297      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.85    |\n",
      "|    explained_variance | 0.00867  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -20.2    |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 65.5     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 143      |\n",
      "|    ep_rew_mean        | -384     |\n",
      "| time/                 |          |\n",
      "|    fps                | 339      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.83    |\n",
      "|    explained_variance | 0.00136  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -21.9    |\n",
      "|    std                | 0.996    |\n",
      "|    value_loss         | 115      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 144      |\n",
      "|    ep_rew_mean        | -383     |\n",
      "| time/                 |          |\n",
      "|    fps                | 351      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.83    |\n",
      "|    explained_variance | 0.000435 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -27.2    |\n",
      "|    std                | 0.998    |\n",
      "|    value_loss         | 107      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 146      |\n",
      "|    ep_rew_mean        | -410     |\n",
      "| time/                 |          |\n",
      "|    fps                | 350      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.82    |\n",
      "|    explained_variance | -0.0016  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -9.97    |\n",
      "|    std                | 0.993    |\n",
      "|    value_loss         | 15.3     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 144      |\n",
      "|    ep_rew_mean        | -415     |\n",
      "| time/                 |          |\n",
      "|    fps                | 360      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.82    |\n",
      "|    explained_variance | -0.0192  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -1.51    |\n",
      "|    std                | 0.991    |\n",
      "|    value_loss         | 1.87     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 144       |\n",
      "|    ep_rew_mean        | -419      |\n",
      "| time/                 |           |\n",
      "|    fps                | 373       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.82     |\n",
      "|    explained_variance | -0.000148 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 6.15      |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 5.2       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 143      |\n",
      "|    ep_rew_mean        | -397     |\n",
      "| time/                 |          |\n",
      "|    fps                | 382      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.82    |\n",
      "|    explained_variance | -0.00194 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -109     |\n",
      "|    std                | 0.99     |\n",
      "|    value_loss         | 3.28e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 143      |\n",
      "|    ep_rew_mean        | -411     |\n",
      "| time/                 |          |\n",
      "|    fps                | 390      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.83    |\n",
      "|    explained_variance | -1.9e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -73.1    |\n",
      "|    std                | 0.993    |\n",
      "|    value_loss         | 498      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 139      |\n",
      "|    ep_rew_mean        | -401     |\n",
      "| time/                 |          |\n",
      "|    fps                | 396      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.8     |\n",
      "|    explained_variance | -0.00243 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -78.8    |\n",
      "|    std                | 0.981    |\n",
      "|    value_loss         | 1.73e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 134      |\n",
      "|    ep_rew_mean        | -391     |\n",
      "| time/                 |          |\n",
      "|    fps                | 401      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.81    |\n",
      "|    explained_variance | 0.00116  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 9.63     |\n",
      "|    std                | 0.984    |\n",
      "|    value_loss         | 16.4     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 132      |\n",
      "|    ep_rew_mean        | -402     |\n",
      "| time/                 |          |\n",
      "|    fps                | 406      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.81    |\n",
      "|    explained_variance | 0.000318 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -9.03    |\n",
      "|    std                | 0.988    |\n",
      "|    value_loss         | 20.1     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 129       |\n",
      "|    ep_rew_mean        | -396      |\n",
      "| time/                 |           |\n",
      "|    fps                | 410       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.8      |\n",
      "|    explained_variance | -0.000118 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 5.19      |\n",
      "|    std                | 0.982     |\n",
      "|    value_loss         | 2.84      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 127       |\n",
      "|    ep_rew_mean        | -417      |\n",
      "| time/                 |           |\n",
      "|    fps                | 413       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.8      |\n",
      "|    explained_variance | -2.26e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -55.2     |\n",
      "|    std                | 0.983     |\n",
      "|    value_loss         | 458       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 123      |\n",
      "|    ep_rew_mean        | -460     |\n",
      "| time/                 |          |\n",
      "|    fps                | 413      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.8     |\n",
      "|    explained_variance | 0.000726 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -7.77    |\n",
      "|    std                | 0.981    |\n",
      "|    value_loss         | 13.4     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 120       |\n",
      "|    ep_rew_mean        | -475      |\n",
      "| time/                 |           |\n",
      "|    fps                | 408       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.82     |\n",
      "|    explained_variance | -2.98e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -18.2     |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 130       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 119       |\n",
      "|    ep_rew_mean        | -477      |\n",
      "| time/                 |           |\n",
      "|    fps                | 409       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.8      |\n",
      "|    explained_variance | -2.05e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 14.6      |\n",
      "|    std                | 0.983     |\n",
      "|    value_loss         | 38.1      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 119      |\n",
      "|    ep_rew_mean        | -472     |\n",
      "| time/                 |          |\n",
      "|    fps                | 411      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.8     |\n",
      "|    explained_variance | 1.19e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 30.2     |\n",
      "|    std                | 0.979    |\n",
      "|    value_loss         | 228      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 118      |\n",
      "|    ep_rew_mean        | -463     |\n",
      "| time/                 |          |\n",
      "|    fps                | 414      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 22       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.78    |\n",
      "|    explained_variance | -0.00017 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -1.02    |\n",
      "|    std                | 0.973    |\n",
      "|    value_loss         | 0.359    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 117      |\n",
      "|    ep_rew_mean        | -466     |\n",
      "| time/                 |          |\n",
      "|    fps                | 416      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 24       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.81    |\n",
      "|    explained_variance | 0.000282 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 3.07     |\n",
      "|    std                | 0.984    |\n",
      "|    value_loss         | 1.99     |\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v3\", continuous=True)\n",
    "\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model.save(\"a2c_lunar_lander\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3z-oIUSrlAsY"
   },
   "source": [
    "#### **1.2.4 Evaluación de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.2.2 pero utilizando el modelo entrenado. ¿Cómo es el performance de su agente? ¿Es mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ophyU3KrWrwl",
    "outputId": "65e694f8-b4aa-4d64-e19d-9630901da2cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensas: -448.96739943018366\n",
      "Desviación estándar de recompensas: 123.7272590595172\n"
     ]
    }
   ],
   "source": [
    "model = A2C.load(\"a2c_lunar_lander\")\n",
    "\n",
    "rewards = []\n",
    "for episode in range(10):\n",
    "    obs = env.reset()[0]\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "    rewards.append(episode_reward)\n",
    "\n",
    "print(f\"Promedio de recompensas: {np.mean(rewards)}\")\n",
    "print(f\"Desviación estándar de recompensas: {np.std(rewards)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXTbkSgiN9z2"
   },
   "source": [
    "Dada la naturaleza del problema, lo esperable es que mejoraran los resultados ya que se puede pensar que existen estrategias ganadoras, y en consecuencia, que el modelo sea capaz de encontrar alguna, esto quizás se pueda alcanzar con un alto número de iteraciones pero también aumenta mucho el tiempo de cómputo, por lo que al quedarnos con un modelo de 10000 iteraciones, se obtiene resultados similares a escoger las acciones al azar, de hecho, con este modelo la desviación estándar aumenta, lo que hace que sea peor que el baseline, ya que la estrategia obtenida no muestra resultados consistentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6Xw4YHT3P5d"
   },
   "source": [
    "#### **1.2.5 Optimización de modelo (0.2 puntos)**\n",
    "\n",
    "Repita los ejercicios 1.2.3 y 1.2.4 hasta obtener un nivel de recompensas promedio mayor a 50. Para esto, puede cambiar manualmente parámetros como:\n",
    "- `total_timesteps`\n",
    "- `learning_rate`\n",
    "- `batch_size`\n",
    "\n",
    "Una vez optimizado el modelo, use la función `export_gif` para estudiar el comportamiento de su agente en la resolución del ambiente y comente sobre sus resultados.\n",
    "\n",
    "Adjunte el gif generado en su entrega (mejor aún si además adjuntan el gif en el markdown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TBi5hNcnbyrH",
    "outputId": "abb88ce6-03d3-4466-a3cc-5a487224565e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 88.8     |\n",
      "|    ep_rew_mean        | -397     |\n",
      "| time/                 |          |\n",
      "|    fps                | 358      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.074    |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.911    |\n",
      "|    value_loss         | 5.07     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 124      |\n",
      "|    ep_rew_mean        | -496     |\n",
      "| time/                 |          |\n",
      "|    fps                | 314      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00851 |\n",
      "|    explained_variance | 0.00626  |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0.0259  |\n",
      "|    value_loss         | 773      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 120      |\n",
      "|    ep_rew_mean        | -534     |\n",
      "| time/                 |          |\n",
      "|    fps                | 329      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.974   |\n",
      "|    explained_variance | -0.00178 |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0.326   |\n",
      "|    value_loss         | 1.19     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 127      |\n",
      "|    ep_rew_mean        | -550     |\n",
      "| time/                 |          |\n",
      "|    fps                | 338      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.517   |\n",
      "|    explained_variance | -17.4    |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -17.3    |\n",
      "|    value_loss         | 5.64e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 165      |\n",
      "|    ep_rew_mean        | -510     |\n",
      "| time/                 |          |\n",
      "|    fps                | 336      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.484   |\n",
      "|    explained_variance | -0.00135 |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.62     |\n",
      "|    value_loss         | 25.3     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 161      |\n",
      "|    ep_rew_mean        | -471     |\n",
      "| time/                 |          |\n",
      "|    fps                | 341      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.652   |\n",
      "|    explained_variance | -0.01    |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.365   |\n",
      "|    value_loss         | 0.432    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 168      |\n",
      "|    ep_rew_mean        | -444     |\n",
      "| time/                 |          |\n",
      "|    fps                | 339      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.495   |\n",
      "|    explained_variance | -0.0247  |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 3.44     |\n",
      "|    value_loss         | 54       |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 190      |\n",
      "|    ep_rew_mean        | -413     |\n",
      "| time/                 |          |\n",
      "|    fps                | 341      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00239 |\n",
      "|    explained_variance | -0.0243  |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.0215  |\n",
      "|    value_loss         | 1.19e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 183      |\n",
      "|    ep_rew_mean        | -388     |\n",
      "| time/                 |          |\n",
      "|    fps                | 346      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.94    |\n",
      "|    explained_variance | -0.187   |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -0.549   |\n",
      "|    value_loss         | 0.948    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 196      |\n",
      "|    ep_rew_mean        | -384     |\n",
      "| time/                 |          |\n",
      "|    fps                | 336      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.798   |\n",
      "|    explained_variance | 0.393    |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 1.14     |\n",
      "|    value_loss         | 3.9      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 206      |\n",
      "|    ep_rew_mean        | -384     |\n",
      "| time/                 |          |\n",
      "|    fps                | 327      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0142  |\n",
      "|    explained_variance | -0.00476 |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.0143   |\n",
      "|    value_loss         | 127      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 203      |\n",
      "|    ep_rew_mean        | -388     |\n",
      "| time/                 |          |\n",
      "|    fps                | 331      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0385  |\n",
      "|    explained_variance | -0.167   |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.204   |\n",
      "|    value_loss         | 904      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 197      |\n",
      "|    ep_rew_mean        | -394     |\n",
      "| time/                 |          |\n",
      "|    fps                | 335      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 19       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.225   |\n",
      "|    explained_variance | -0.709   |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -1.28    |\n",
      "|    value_loss         | 40.6     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 204      |\n",
      "|    ep_rew_mean        | -382     |\n",
      "| time/                 |          |\n",
      "|    fps                | 336      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.505   |\n",
      "|    explained_variance | 0.0178   |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -0.664   |\n",
      "|    value_loss         | 4.82     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 201      |\n",
      "|    ep_rew_mean        | -374     |\n",
      "| time/                 |          |\n",
      "|    fps                | 335      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 22       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.361   |\n",
      "|    explained_variance | 0.828    |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.553    |\n",
      "|    value_loss         | 3.88     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 215      |\n",
      "|    ep_rew_mean        | -361     |\n",
      "| time/                 |          |\n",
      "|    fps                | 335      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 23       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.326   |\n",
      "|    explained_variance | -0.284   |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.81     |\n",
      "|    value_loss         | 15.5     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 218      |\n",
      "|    ep_rew_mean        | -357     |\n",
      "| time/                 |          |\n",
      "|    fps                | 336      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 25       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.396   |\n",
      "|    explained_variance | -1.27    |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -2.01    |\n",
      "|    value_loss         | 9.86     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 217      |\n",
      "|    ep_rew_mean        | -335     |\n",
      "| time/                 |          |\n",
      "|    fps                | 337      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 26       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.403   |\n",
      "|    explained_variance | 0.615    |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -0.542   |\n",
      "|    value_loss         | 4.82     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 215      |\n",
      "|    ep_rew_mean        | -315     |\n",
      "| time/                 |          |\n",
      "|    fps                | 333      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 28       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0149  |\n",
      "|    explained_variance | -0.522   |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -0.0138  |\n",
      "|    value_loss         | 100      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 215      |\n",
      "|    ep_rew_mean        | -315     |\n",
      "| time/                 |          |\n",
      "|    fps                | 330      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 30       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.407   |\n",
      "|    explained_variance | 0.867    |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -1.53    |\n",
      "|    value_loss         | 1.85     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x7b0ad8ba0ac0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "env = make_vec_env(\"LunarLander-v3\", n_envs=1)\n",
    "\n",
    "model1 = A2C(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=0.005,\n",
    "    verbose=1,\n",
    ")\n",
    "model1.learn(total_timesteps=10000)  # aumentar timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sspxKWszORfF",
    "outputId": "7cf66911-e158-403f-a040-a51ef60e0b29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensas optimizado: 54.099287352489775\n",
      "Desviación estándar optimizado: 106.20646700784887\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "for episode in range(10):\n",
    "    obs = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = model1.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward[0]\n",
    "        done = done[0]\n",
    "    rewards.append(episode_reward)\n",
    "\n",
    "print(f\"Promedio de recompensas optimizado: {np.mean(rewards)}\")\n",
    "print(f\"Desviación estándar optimizado: {np.std(rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Accm6jAIcirX"
   },
   "outputs": [],
   "source": [
    "export_gif(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPUY-Ktgf2BO"
   },
   "source": [
    "## **2. Large Language Models (4.0 puntos)**\n",
    "\n",
    "En esta sección se enfocarán en habilitar un Chatbot que nos permita responder preguntas útiles a través de LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQ4fPRRihGLe"
   },
   "source": [
    "### **2.0 Configuración Inicial**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/uqAs9atZH58AAAAd/config-config-issue.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Como siempre, cargamos todas nuestras API KEY al entorno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ud2Xm_k-hFJn"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "\n",
    "if \"TAVILY_API_KEY\" not in os.environ:\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIbdy9arK40P",
    "outputId": "0b045509-84b1-48b9-b810-01434974b8e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), temperature=0.0, max_retries=2, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000002025B731F40>, default_metadata=())"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rj9JvQUsgZZJ"
   },
   "source": [
    "### **2.1 Retrieval Augmented Generation (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://y.yarn.co/218aaa02-c47e-4ec9-b1c9-07792a06a88f_text.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsección es que habiliten un chatbot que pueda responder preguntas usando información contenida en documentos PDF a través de **Retrieval Augmented Generation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrxOQroVnaZ5"
   },
   "source": [
    "#### **2.1.1 Reunir Documentos (0 puntos)**\n",
    "\n",
    "Reuna documentos PDF sobre los que hacer preguntas siguiendo las siguientes instrucciones:\n",
    "  - 2 documentos .pdf como mínimo.\n",
    "  - 50 páginas de contenido como mínimo entre todos los documentos.\n",
    "  - Ideas para documentos: Documentos relacionados a temas académicos, laborales o de ocio. Aprovechen este ejercicio para construir algo útil y/o relevante para ustedes!\n",
    "  - Deben ocupar documentos reales, no pueden utilizar los mismos de la clase.\n",
    "  - Deben registrar sus documentos en la siguiente [planilla](https://docs.google.com/spreadsheets/d/1Hy1w_dOiG2UCHJ8muyxhdKPZEPrrL7BNHm6E90imIIM/edit?usp=sharing). **NO PUEDEN USAR LOS MISMOS DOCUMENTOS QUE OTRO GRUPO**\n",
    "  - **Recuerden adjuntar los documentos en su entrega**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5D1tIRCi4oJJ",
    "outputId": "39f6d4fc-63cb-4b9b-d48f-48d60df25ee5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzq2TjWCnu15"
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "doc_paths = ['AttentionIsAllYouNeed.pdf', 'BERT.pdf', 'ChainOfThought.pdf'] # rellenar con los path a sus documentos\n",
    "\n",
    "assert len(doc_paths) >= 2, \"Deben adjuntar un mínimo de 2 documentos\"\n",
    "\n",
    "total_paginas = sum(len(PyPDF2.PdfReader(open(doc, \"rb\")).pages) for doc in doc_paths)\n",
    "assert total_paginas >= 50, f\"Páginas insuficientes: {total_paginas}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yymp25NBK40U",
    "outputId": "52733d58-ba66-4e74-df8b-f577bce42c51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 1}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 2}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 3}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 4}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "docs = []\n",
    "for path in doc_paths:\n",
    "    loader = PyPDFLoader(path)\n",
    "    docs += loader.load()\n",
    "\n",
    "docs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r811-P71nizA"
   },
   "source": [
    "#### **2.1.2 Vectorizar Documentos (0.2 puntos)**\n",
    "\n",
    "Vectorice los documentos y almacene sus representaciones de manera acorde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-yXAdCSn4JM",
    "outputId": "5a502aa7-353f-4b77-f8fe-bbe79568648b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 0}, page_content='University of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 0}, page_content='based solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 0}, page_content='our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 0}, page_content='the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "splits[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EEXGEMD4K40W",
    "outputId": "aee57976-ed37-410c-d5c2-cdef9ff01934"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x202b6690140>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embedding)\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAUkP5zrnyBK"
   },
   "source": [
    "#### **2.1.3 Habilitar RAG (0.3 puntos)**\n",
    "\n",
    "Habilite la solución RAG a través de una *chain* y guárdela en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPIySdDFn99l"
   },
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3},\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "retriever_chain = retriever | format_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycg5S5i_n-kL"
   },
   "source": [
    "#### **2.1.4 Verificación de respuestas (0.5 puntos)**\n",
    "\n",
    "Genere un listado de 3 tuplas (\"pregunta\", \"respuesta correcta\") y analice la respuesta de su solución para cada una. ¿Su solución RAG entrega las respuestas que esperaba?\n",
    "\n",
    "Ejemplo de tupla:\n",
    "- Pregunta: ¿Quién es el presidente de Chile?\n",
    "- Respuesta correcta: El presidente de Chile es Gabriel Boric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r8LLuBxQK40Y"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# noten como ahora existe el parámetro de context!\n",
    "rag_template = '''\n",
    "Eres un asistente experto en investigación sobre LLMs.\n",
    "Tu único rol es contestar preguntas del usuario a partir de información relevante que te sea proporcionada.\n",
    "Responde siempre de la forma más completa posible y usando toda la información entregada.\n",
    "Responde sólo lo que te pregunten a partir de la información relevante, NUNCA inventes una respuesta.\n",
    "\n",
    "Información relevante: {context}\n",
    "Pregunta: {question}\n",
    "Respuesta:\n",
    "'''\n",
    "\n",
    "rag_prompt = PromptTemplate.from_template(rag_template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever_chain,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    } # Los parámetros de la plantilla\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_UiEn1hoZYR",
    "outputId": "4006d104-ecaa-4ecf-e0cd-477024a3bddd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta esperada:  Un Transformer es un tipo de arquitectura de red neuronal que usa un encoder-decoder para modelar relaciones en datos secuenciales.\n",
      "Respuesta obtenita:  Un Transformer es un modelo de transducción que utiliza únicamente auto-atención para calcular representaciones de su entrada y salida, sin usar RNNs alineados con la secuencia o convoluciones.  Su arquitectura se compone de un codificador y un decodificador, ambos formados por una pila de N=6 capas idénticas. Cada capa tiene dos subcapas: un mecanismo de auto-atención multi-cabeza y una capa de conexión completamente conectada posicional.  El modelo utiliza codificaciones posicionales tanto en el codificador como en el decodificador, y un dropout de Pdrop = 0.1 en el modelo base. Durante el entrenamiento, se empleó suavizado de etiquetas con un valor de ϵls = 0.1.\n",
      "\n",
      "\n",
      "Respuesta esperada:  Se puede descomponer el problema en partes más pequeñas y resolver cada parte por separado.\n",
      "Respuesta obtenita:  Según la información proporcionada, la técnica Chain of Thought (pensamiento en cadena) se puede usar para resolver problemas complejos con un LLM.  Esta técnica permite descomponer problemas de múltiples pasos en subproblemas más pequeños y manejables,  imitando un proceso de pensamiento paso a paso para llegar a la respuesta.  Además,  los modelos de lenguaje pueden generar cadenas de pensamiento si se proporcionan demostraciones de razonamiento de pensamiento en cadena en los ejemplos para la indicación de pocos disparos (few-shot prompting).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question_answer_list = [\n",
    "    (\n",
    "        \"Qué es un Transformer?\",\n",
    "        \"Un Transformer es un tipo de arquitectura de red neuronal que usa un encoder-decoder para modelar relaciones en datos secuenciales.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Según Chain of Thought, ¿qué técnica se puede usar para resolver un problema complejo con un LLM?\",\n",
    "        \"Se puede descomponer el problema en partes más pequeñas y resolver cada parte por separado.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "for question_answer in question_answer_list:\n",
    "    res = rag_chain.invoke(question_answer[0])\n",
    "    print(\"Respuesta esperada: \", question_answer[1])\n",
    "    print(\"Respuesta obtenita: \", res)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzhKlq6DK40a"
   },
   "source": [
    "**RESPUESTA**: La solución de RAG entrega respuestas correctas, y bastante parecidas a las esperadas. Al ser preguntas no simples, era difícil que la respuesta fuera exactamente igual, pero en general, la respuesta entregada por RAG es correcta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8d5zTMHoUgF"
   },
   "source": [
    "#### **2.1.5 Sensibilidad de Hiperparámetros (0.5 puntos)**\n",
    "\n",
    "Extienda el análisis del punto 2.1.4 analizando cómo cambian las respuestas entregadas cambiando los siguientes hiperparámetros:\n",
    "- `Tamaño del chunk`. (*¿Cómo repercute que los chunks sean mas grandes o chicos?*)\n",
    "- `La cantidad de chunks recuperados`. (*¿Qué pasa si se devuelven muchos/pocos chunks?*)\n",
    "- `El tipo de búsqueda`. (*¿Cómo afecta el tipo de búsqueda a las respuestas de mi RAG?*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDh_QgeXLGHc"
   },
   "outputs": [],
   "source": [
    "def try_other_config(\n",
    "    chunk_size: int = 500,\n",
    "    ammount_of_chunks: int = 3,\n",
    "    search_type: str = 'similarity'\n",
    "):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=50)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\") # inicializamos los embeddings\n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=embedding) # vectorizacion y almacenamiento\n",
    "\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=search_type, # método de búsqueda\n",
    "        search_kwargs={\"k\": ammount_of_chunks}, # n° documentos a recuperar\n",
    "    )\n",
    "\n",
    "    retriever_chain = retriever | format_docs\n",
    "\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": retriever_chain,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | rag_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    for question_answer in question_answer_list:\n",
    "        res = rag_chain.invoke(question_answer[0])\n",
    "        print(\"Respuesta esperada: \", question_answer[1])\n",
    "        print(\"Respuesta obtenita: \", res)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOBE4zrBK40b"
   },
   "source": [
    "\n",
    "**PROBANDO CAMBIAR TAMAÑO DE CHUNK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0bwpPm07K40c",
    "outputId": "3cd64f73-ab5b-4bab-c49e-217753786cfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta esperada:  Un Transformer es un tipo de arquitectura de red neuronal que usa un encoder-decoder para modelar relaciones en datos secuenciales.\n",
      "Respuesta obtenita:  Basado en la información proporcionada, un Transformer es un modelo que puede ser usado para tareas de traducción.  También se menciona que existe una versión del Transformer referida como \"Transformer\".\n",
      "\n",
      "\n",
      "Respuesta esperada:  Se puede descomponer el problema en partes más pequeñas y resolver cada parte por separado.\n",
      "Respuesta obtenita:  Según la información proporcionada, la técnica \"chain of thought\" es particularmente adecuada para resolver problemas complejos con un LLM.  El ejemplo dado muestra que un modelo que produce un \"chain of thought\" puede resolver un problema.  Sin embargo, la información no describe la técnica en sí misma, solo indica su idoneidad para este propósito.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_other_config(chunk_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqByizLRK40c"
   },
   "source": [
    "**RESPUESTA**: Al achicar el tamaño de los chunks se obtienen respuestas especificas pero distintas a lo que buscabamos. La respuesta es incompleta y difiere de lo que se esperaba. Esto se debe a que al achicar el tamaño de los chunks, se obtiene menos información y contexto para responder la pregunta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_SoU4TQK40d",
    "outputId": "d1b16df3-1c93-48b4-8056-b5b21b35066d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta esperada:  Un Transformer es un tipo de arquitectura de red neuronal que usa un encoder-decoder para modelar relaciones en datos secuenciales.\n",
      "Respuesta obtenita:  Un Transformer es una nueva arquitectura de red simple basada únicamente en mecanismos de atención, que prescinde por completo de la recurrencia y las convoluciones.  Está compuesta por un codificador y un decodificador, ambos con capas apiladas de auto-atención y capas totalmente conectadas puntuales.  El codificador tiene una pila de N=6 capas idénticas, cada una con dos subcapas: un mecanismo de auto-atención multi-cabeza y una red de avance totalmente conectada posicional simple.  Se utiliza una conexión residual alrededor de cada una de las dos subcapas, seguida de una normalización de capa.  Se aplica abandono (dropout) a la salida de cada subcapa, antes de que se agregue a la entrada de la subcapa y se normalice.  También se aplica abandono a las sumas de las incrustaciones y las codificaciones posicionales tanto en el codificador como en el decodificador.  Durante el entrenamiento, se empleó un suavizado de etiquetas.\n",
      "\n",
      "\n",
      "Respuesta esperada:  Se puede descomponer el problema en partes más pequeñas y resolver cada parte por separado.\n",
      "Respuesta obtenita:  Según la información proporcionada, la técnica para resolver un problema complejo con un LLM es el *Chain-of-Thought prompting*.  Este método consiste en proporcionar ejemplos de razonamiento paso a paso (cadenas de pensamiento) en los ejemplos para el prompting de pocos disparos.  Esto permite que los modelos de lenguaje generen sus propias cadenas de pensamiento para resolver el problema.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_other_config(chunk_size=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUsAwGlFK40d"
   },
   "source": [
    "**RESPUESTA**: Al aumentar el tamaño de los chunks se obtiene información muchisimo más completa. Al tener tanta información, y dejar que el LLM la procese, se obtiene una respuesta mucho más completa y correcta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anUc1p2cK40e"
   },
   "source": [
    "**PROBANDO CAMBIAR CANTIDAD DE CHUNKS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ld6Ok-JNK40e",
    "outputId": "7fa1c77e-6d8b-40ce-814c-6ce50ca38ec9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta esperada:  Un Transformer es un tipo de arquitectura de red neuronal que usa un encoder-decoder para modelar relaciones en datos secuenciales.\n",
      "Respuesta obtenita:  Basado en la información proporcionada, un Transformer es un modelo de arquitectura que utiliza capas apiladas de auto-atención y capas totalmente conectadas puntuales, tanto para el codificador como para el decodificador.  El codificador está compuesto por una pila de N=6 capas idénticas, cada una con dos subcapas: una de mecanismo de auto-atención multi-cabeza y otra, una capa totalmente conectada simple y posicional.  La figura 1 muestra la arquitectura completa del modelo, con el codificador en la mitad izquierda y el decodificador en la mitad derecha.\n",
      "\n",
      "\n",
      "Respuesta esperada:  Se puede descomponer el problema en partes más pequeñas y resolver cada parte por separado.\n",
      "Respuesta obtenita:  Según la información proporcionada, Chain of Thought es una técnica que se puede usar para resolver problemas complejos con un LLM.  La información indica que es robusta a diferentes órdenes de ejemplos y a un número variable de ellos.  Además, aunque es particularmente adecuada para problemas matemáticos, su naturaleza basada en el lenguaje la hace aplicable a una amplia gama de problemas de razonamiento de sentido común que involucran el razonamiento sobre interacciones físicas y humanas bajo la presunción de conocimiento general.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_other_config(ammount_of_chunks=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxedNNPwK40e"
   },
   "source": [
    "**RESPUESTA**: Al disminuir la cantidad de chunks recuperados, se obtiene una respuesta más general y menos precisa, ya que el modelo obtieen menos contexto de diferentes partes del documento. Sin embargo, la respuesta en este caso es bastante buena, probablemente debido a que el tamaño de los chunks es lo suficientemente grande para que el modelo pueda responder correctamente, y que el chunk que se obtiene es realmente suficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHBwHTyjK40f",
    "outputId": "4e7f670f-70bd-4fb0-ce53-51c0ccf7df38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta esperada:  Un Transformer es un tipo de arquitectura de red neuronal que usa un encoder-decoder para modelar relaciones en datos secuenciales.\n",
      "Respuesta obtenita:  El Transformer es una nueva y simple arquitectura de red propuesta como modelo de transducción de secuencias.  A diferencia de los modelos dominantes basados en redes neuronales recurrentes o convolucionales complejas, el Transformer se basa completamente en la auto-atención para calcular las representaciones de su entrada y salida, sin usar RNNs o convoluciones alineados con la secuencia.  Es el primer modelo de transducción que hace esto.  Su arquitectura utiliza capas apiladas de auto-atención y capas totalmente conectadas puntuales, tanto para el codificador como para el decodificador.  El codificador está compuesto por una pila de N=6 capas idénticas, cada una con dos subcapas: un mecanismo de auto-atención multi-cabeza y una capa de conexión totalmente conectada posicional.  El modelo también utiliza codificaciones posicionales tanto en el codificador como en el decodificador, y un suavizado de etiquetas durante el entrenamiento.\n",
      "\n",
      "\n",
      "Respuesta esperada:  Se puede descomponer el problema en partes más pequeñas y resolver cada parte por separado.\n",
      "Respuesta obtenita:  Según la información proporcionada, la técnica Chain-of-Thought (pensamiento en cadena) se puede usar para resolver problemas complejos con un LLM.  Esta técnica implica descomponer el problema en pasos intermedios y resolver cada uno antes de dar la respuesta final, imitando un proceso de pensamiento paso a paso.  Además, es robusta a diferentes órdenes de ejemplos y a un número variable de ellos.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_other_config(ammount_of_chunks=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiBvyNg0K40f"
   },
   "source": [
    "**RESPUESTA**: Al aumentar la cantidad de chunks recuperados, se obtiene una respuesta más completa y precisa, ya que el modelo obtiene más contexto de diferentes partes del documento. Esto se cumple en este caso, entregando una respuesta bastante completa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8YsRDLxK40g"
   },
   "source": [
    "**PROBANDO CAMBIAR TIPO DE BUSQUEDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_qHrXwaK40h",
    "outputId": "48bfe10a-c364-474f-e112-89b7171c5922"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta esperada:  Un Transformer es un tipo de arquitectura de red neuronal que usa un encoder-decoder para modelar relaciones en datos secuenciales.\n",
      "Respuesta obtenita:  Un Transformer es un modelo de arquitectura que utiliza capas apiladas de auto-atención y capas totalmente conectadas punto a punto, tanto para el codificador como para el decodificador.  El codificador está compuesto por una pila de N=6 capas idénticas, cada una con dos subcapas: un mecanismo de auto-atención multi-cabeza y una capa de conexión totalmente conectada posicional.  Existen diferentes tamaños de Transformers,  por ejemplo, uno con (L=6, H=1024, A=16) y 100M parámetros, y otro más grande (L=64, H=512, A=2) con 235M parámetros.  Se ha demostrado que aumentar el tamaño del modelo lleva a mejoras continuas en tareas a gran escala como la traducción automática y el modelado del lenguaje.\n",
      "\n",
      "\n",
      "Respuesta esperada:  Se puede descomponer el problema en partes más pequeñas y resolver cada parte por separado.\n",
      "Respuesta obtenita:  Según la información proporcionada, la técnica Chain of Thought (CoT) se puede usar para resolver problemas complejos con un LLM.  La información indica que CoT es robusta a diferentes órdenes de ejemplos y a un número variable de ellos.  Además, aunque es particularmente adecuada para problemas matemáticos, su naturaleza basada en el lenguaje la hace aplicable a una amplia gama de problemas de razonamiento de sentido común.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_other_config(search_type='mmr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V45wtV4JK40h"
   },
   "source": [
    "**RESPUESTA**: Al cambiar similarity por MMR (Maximal Marginal Relevance), se obtiene una respuesta más completa gracias a la diversidad de los chunks recuperados. Esto dado a que MMR busca chunks que sean diferentes entre sí, lo que permite que el modelo tenga más contexto para responder la pregunta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENJiPPM0giX8"
   },
   "source": [
    "### **2.2 Agentes (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/rcqnN2aJCSEAAAAd/secret-agent-man.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la sección anterior, en esta sección se busca habilitar **Agentes** para obtener información a través de tools y así responder la pregunta del usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V47l7Mjfrk0N"
   },
   "source": [
    "#### **2.2.1 Tool de Tavily (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas al motor de búsqueda **Tavily**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6SLKwcWr0AG"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SonB1A-9rtRq"
   },
   "source": [
    "#### **2.2.2 Tool de Wikipedia (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas a **Wikipedia**.\n",
    "\n",
    "*Hint: Le puede ser de ayuda el siguiente [link](https://python.langchain.com/v0.1/docs/modules/tools/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-_2NwU1K40k",
    "outputId": "52552943-2392-4c32-e9b4-ad45957a7a6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehJJpoqsr26-"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(lang='es', top_k_results=1, doc_content_chars_max=100)\n",
    "wiki_tool = WikipediaQueryRun(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvUIMdX6r0ne"
   },
   "source": [
    "#### **2.2.3 Crear Agente (0.3 puntos)**\n",
    "\n",
    "Crear un agente que pueda responder preguntas preguntas usando las *tools* antes generadas. Asegúrese que su agente responda en español. Por último, guarde el agente en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VO9cE0TQK40l",
    "outputId": "aa9908f8-2fb3-449a-c09d-0dede14f699d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\langsmith\\client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "{tools}\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [{tool_names}]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "react_prompt = hub.pull(\"hwchase17/react\") # template de ReAct\n",
    "print(react_prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pD1_n0wrsDI5",
    "outputId": "d793a1ba-d15b-49b6-c8e5-3da6517ed783"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentExecutor(verbose=True, agent=RunnableAgent(runnable=RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_log_to_str(x['intermediate_steps']))\n",
       "})\n",
       "| PromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={}, partial_variables={'tools': 'tavily_search_results_json - A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.\\nwikipedia - A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'tool_names': 'tavily_search_results_json, wikipedia'}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'react', 'lc_hub_commit_hash': 'd15fe3c426f1c4b3f37c9198853e4a86e20c425ca7f4752ec0c9b0e97ca7ea4d'}, template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}')\n",
       "| RunnableBinding(bound=ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), temperature=0.0, max_retries=2, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000002025B731F40>, default_metadata=()), kwargs={'stop': ['\\nObservation']}, config={}, config_factories=[])\n",
       "| ReActSingleInputOutputParser(), input_keys_arg=[], return_keys_arg=[], stream_runnable=True), tools=[TavilySearchResults(max_results=1, api_wrapper=TavilySearchAPIWrapper(tavily_api_key=SecretStr('**********'))), WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'c:\\\\Users\\\\lucas\\\\python-envs\\\\nb-env\\\\Lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='es', load_all_available_meta=False, doc_content_chars_max=100))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "\n",
    "tools = [tavily_tool, wiki_tool]\n",
    "\n",
    "agent = create_react_agent(llm, tools, react_prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6EW2E_MbK40m",
    "outputId": "fa58a55b-7b3e-4c3d-8fd9-d8ee0b4fec08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To answer this question, I need to find a list of professors from the Faculty of Physics and Mathematics (FCFM) at the University of Chile.  A Wikipedia search might be a good starting point, but it might not be completely up-to-date.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: \"Faculty of Physics and Mathematics, University of Chile\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Universidad de California en Berkeley\n",
      "Summary: La Universidad de California en Berkeley (en id\u001b[0m\u001b[32;1m\u001b[1;3mThought: That was the wrong Wikipedia page. I need to try a different search term.  Let's try searching directly for the faculty's website.  If that doesn't work, I'll try a different approach.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: \"Facultad de Ciencias Físicas y Matemáticas, Universidad de Chile\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Facultad de Ciencias Físicas y Matemáticas de la Universidad de Chile\n",
      "Summary: La Facultad de \u001b[0m\u001b[32;1m\u001b[1;3mThought: The Wikipedia page for the Facultad de Ciencias Físicas y Matemáticas (FCFM) of the University of Chile should list faculty members, or at least provide a link to a page with that information.  Let's see if the page provides a list of professors or a link to a faculty directory.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: \"Facultad de Ciencias Físicas y Matemáticas, Universidad de Chile\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Facultad de Ciencias Físicas y Matemáticas de la Universidad de Chile\n",
      "Summary: La Facultad de \u001b[0m\u001b[32;1m\u001b[1;3mThought: The Wikipedia page doesn't seem to have a readily available list of current professors.  I'll try searching directly for the FCFM's website using a search engine.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"Facultad de Ciencias Físicas y Matemáticas Universidad de Chile profesores\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://ingenieria.uchile.cl/sobre-la-fcfm', 'content': 'La Facultad de Ciencias Físicas y Matemáticas se orienta a la enseñanza superior y al desarrollo de las ciencias básicas, ingeniería, aplicaciones tecnológicas e innovación. ... Profesores/as Eméritos/as FCFM-U. de Chile. Francisco Brieva Rodríguez ... Internacional; Sustentabilidad; Biblioteca; Facultad de Ciencias Físicas y'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: The search result provides a link to the FCFM website, but doesn't list professors directly.  I need to navigate to that website to find a faculty directory.  Since I can't directly browse websites, I'll have to rely on the information I can gather from the search results and potentially further searches.  The search result mentions \"Profesores/as Eméritos/as FCFM-U. de Chile. Francisco Brieva Rodríguez\", so I have at least one name.\n",
      "\n",
      "Thought: I now know the final answer.  I can't provide a comprehensive list, but I have at least one name.\n",
      "\n",
      "Final Answer: At least one professor at the FCFM of the University of Chile is Francisco Brieva Rodríguez.  A more complete list would require accessing and navigating the FCFM website, which is beyond my current capabilities.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "At least one professor at the FCFM of the University of Chile is Francisco Brieva Rodríguez.  A more complete list would require accessing and navigating the FCFM website, which is beyond my current capabilities.\n"
     ]
    }
   ],
   "source": [
    "def call_agent(input: str):\n",
    "    response = agent_executor.invoke({\"input\": input})\n",
    "    print(response[\"output\"])\n",
    "\n",
    "call_agent(\"Dame el nombre de algunos profesores de la FCFM de la Universidad de Chile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKV0JxK3r-XG"
   },
   "source": [
    "#### **2.2.4 Verificación de respuestas (0.3 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente y asegúrese que el agente esté ocupando correctamente las tools disponibles. ¿En qué casos el agente debería ocupar la tool de Tavily? ¿En qué casos debería ocupar la tool de Wikipedia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55H0LYWXK40o",
    "outputId": "522e3f52-8080-45b4-8f75-59f33914e471"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find information about Barack Obama.  Wikipedia is a good resource for biographical information.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]​ ( en inglés estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Quién es Obama?\n",
      "Thought: I need to find information about Barack Obama.  Wikipedia is a good resource for biographical information.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]​ ( en inglés estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Quién es Obama?\n",
      "Thought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]​ ( en inglés estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Quién es Obama?\n",
      "Thought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]​ ( en inglés estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Quién es Obama?\n",
      "Thought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]​ ( en inglés estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Quién es Obama?\n",
      "Thought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]​ ( en inglés estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Quién es Obama?\n",
      "Thought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]​ ( en inglés estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Quién es Obama?\n",
      "Thought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]​ ( en inglés estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Quién es Obama?\n",
      "Thought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]​ ( en inglés estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]​ ( en inglés estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to get a full summary of Barack Obama's life from Wikipedia to answer the question properly.  The previous attempts only showed a snippet.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]​ ( en inglés estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mThought: The previous attempts only returned a truncated summary. I need to access the full Wikipedia page to get a complete answer.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]​ ( en inglés estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mThought: The repeated attempts to access the Wikipedia page for Barack Obama only yielded truncated summaries.  I need a different approach to get a complete answer.  I will try again, specifying that I need the full text.  If that fails, I will resort to a different strategy.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama  (Requesting full page content)\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Drake (músico)\n",
      "Summary: Aubrey Drake Graham (Toronto, 24 de octubre de 1986) conocido simpleme\u001b[0m\u001b[32;1m\u001b[1;3mThought: The Wikipedia calls are failing to return the full page content.  I will try a different approach.  I will use a search engine to find a reliable source with a biography of Barack Obama.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"Barack Obama biography\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.whitehouse.gov/about-the-white-house/presidents/barack-obama/', 'content': 'Mobile Menu Overlay\\nThe White House\\n1600 Pennsylvania Ave NW\\nWashington, DC 20500\\nBarack Obama\\nThe 44th President of the United States\\nNavigate this Section\\nThe biography for President Obama and past presidents is courtesy of the White House Historical Association.\\n In the last year of his second term, Obama spoke at two events that clearly moved him—the 50th anniversary of the civil rights march from Selma to Montgomery, and the dedication of the National Museum of African American History and Culture. His story is the American story — values from the heartland, a middle-class upbringing in a strong family, hard work and education as the means of getting ahead, and the conviction that a life so blessed should be lived in service to others.\\n Following graduation in 1983, Obama worked in New York City, then became a community organizer on the South Side of Chicago, coordinating with churches to improve housing conditions and set up job-training programs in a community hit hard by steel mill closures. “And that’s why we celebrate,” he told those attending the museum opening in Washington, “mindful that our work is not yet done.”\\nLearn more about Barack Obama’s spouse, Michelle Obama.\\n'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now have a concise biography of Barack Obama from a reliable source.\n",
      "\n",
      "Final Answer: Barack Obama was the 44th President of the United States.  He had a middle-class upbringing and worked as a community organizer in Chicago before entering politics.  His life has been dedicated to public service.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Barack Obama was the 44th President of the United States.  He had a middle-class upbringing and worked as a community organizer in Chicago before entering politics.  His life has been dedicated to public service.\n"
     ]
    }
   ],
   "source": [
    "call_agent(\"Quién es Obama?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56bVbo06K40o",
    "outputId": "c1e4f995-c989-4a04-829b-d2ab9732e8e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To answer this question about FC Barcelona's performance in the Spanish league, I need current standings and possibly recent results.  A search engine like tavily_search_results_json should provide up-to-date information.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"FC Barcelona La Liga standings\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.foxsports.com/soccer/barcelona-team-standings', 'content': 'Barcelona Laliga Standings | FOX Sports SPORTS & TEAMS SPORTS SPORTS & TEAMS SPORTS NFL NCAA FB MLB NBA NCAA BK NASCAR Soccer UFL NCAAW BK NHL Golf Premier Boxing Champions WWE UFC WNBA Tennis Motor Sports Professional Bowlers Association Horse Racing Westminster Kennel Club FIBA Olympics World Baseball Classic NRL The Basketball Tournament BARCELONA 9-0-1 · 1ST IN LALIGA · CHAMPIONS LEAGUE 9-0-1 · 1ST IN LALIGA · CHAMPIONS LEAGUE Next Match vs Real Madrid (A) · Sat 7:00PM BARCELONA LALIGA STANDINGS uefa champions league uefa europa league When games are live, standings update in real-time. QUALIFICATION/RELEGATION Champions League Europa League Relegation Real Madrid Top Leagues FOX Sports Soccer Leagues and Tournaments  2024 MLB Playoff Bracket  2024 NFL Power Rankings  2024 World Series'}]\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Cómo le está yendo al FC Barcelona en la liga española?\n",
      "Thought:To answer this question about FC Barcelona's performance in the Spanish league, I need current standings and possibly recent results.  A search engine like tavily_search_results_json should provide up-to-date information.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"FC Barcelona La Liga standings\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.foxsports.com/soccer/barcelona-team-standings', 'content': 'Barcelona Laliga Standings | FOX Sports SPORTS & TEAMS SPORTS SPORTS & TEAMS SPORTS NFL NCAA FB MLB NBA NCAA BK NASCAR Soccer UFL NCAAW BK NHL Golf Premier Boxing Champions WWE UFC WNBA Tennis Motor Sports Professional Bowlers Association Horse Racing Westminster Kennel Club FIBA Olympics World Baseball Classic NRL The Basketball Tournament BARCELONA 9-0-1 · 1ST IN LALIGA · CHAMPIONS LEAGUE 9-0-1 · 1ST IN LALIGA · CHAMPIONS LEAGUE Next Match vs Real Madrid (A) · Sat 7:00PM BARCELONA LALIGA STANDINGS uefa champions league uefa europa league When games are live, standings update in real-time. QUALIFICATION/RELEGATION Champions League Europa League Relegation Real Madrid Top Leagues FOX Sports Soccer Leagues and Tournaments  2024 MLB Playoff Bracket  2024 NFL Power Rankings  2024 World Series'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought:The observation shows that FC Barcelona is currently 1st in La Liga with a record of 9 wins, 0 losses, and 1 draw.\n",
      "\n",
      "Thought:I now know the final answer.\n",
      "\n",
      "Final Answer: Al FC Barcelona le está yendo muy bien en la liga española.  Actualmente están en primer lugar con un récord de 9 victorias, 0 derrotas y 1 empate.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Al FC Barcelona le está yendo muy bien en la liga española.  Actualmente están en primer lugar con un récord de 9 victorias, 0 derrotas y 1 empate.\n"
     ]
    }
   ],
   "source": [
    "call_agent(\"Cómo le está yendo al FC Barcelona en la liga española?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNF3CkQjK40p"
   },
   "source": [
    "En general debería usar Wikipedia para información especifica que suele estar contenida en wikipedia, mientras que en cualquier otro caso más general debería usar Tavily. Que tipo de información debería buscar en wikipedia? Cuando se le pregunta el nombre de una persona, una entidad economica, etc. Es posible que incluso en esos casos se use la de Tavily, pero ese sería el criterio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZbDTYiogquv"
   },
   "source": [
    "### **2.3 Multi Agente (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/r7QMJLxU4BoAAAAd/this-is-getting-out-of-hand-star-wars.gif\"\n",
    "\" width=\"450\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsección es encapsular las funcionalidades creadas en una solución multiagente con un **supervisor**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-iUfH0WvI6m"
   },
   "source": [
    "#### **2.3.1 Generando Tools (0.5 puntos)**\n",
    "\n",
    "Transforme la solución RAG de la sección 2.1 y el agente de la sección 2.2 a *tools* (una tool por cada uno)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lV4VrNiEK40r"
   },
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def call_rag(question: str) -> str:\n",
    "    \"\"\"Call the RAG model to answer a question\"\"\"\n",
    "    return rag_chain.invoke(question)\n",
    "\n",
    "@tool\n",
    "def call_react(input: str) -> str:\n",
    "    \"\"\"Call the ReAct model to answer a question\"\"\"\n",
    "    return agent_executor.invoke({\"input\": input})[\"output\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQYNjT_0vPCg"
   },
   "source": [
    "#### **2.3.2 Agente Supervisor (0.5 puntos)**\n",
    "\n",
    "Habilite un agente que tenga acceso a las tools del punto anterior y pueda responder preguntas relacionadas. Almacene este agente en una variable llamada supervisor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pw1cfTtvv1AZ"
   },
   "outputs": [],
   "source": [
    "supervisor_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Eres un agente enrrutador de preguntas.\n",
    "    Tu rol es decidir que acción tomar para poder contestar la pregunta de la mejor manera posible:\n",
    "    - 'llm': Cuando tengas que usar un sistema RAG para extraer información sobre LLMs. Esto incluye información de BERT, Transformers, CoT, etc.\n",
    "    - 'general': Cuando la pregunta sea relacionada a cosas que puedan estar en internet o personas o entidades que conoce wikipedia.\n",
    "    - 'otro': Todo aquella pregunta que no esté contenida en las categorías anteriores.\n",
    "    - 'fin': Si crees que la pregunta ya puede ser contestada con la información actual.\n",
    "\n",
    "    No respondas con más de una palabra y no incluyas.\n",
    "\n",
    "    Pregunta:\n",
    "    {question}\n",
    "\n",
    "\n",
    "    Información actual:\n",
    "    {context}\n",
    "\n",
    "\n",
    "    Categoría:\"\"\"\n",
    ")\n",
    "\n",
    "supervisor_chain = (\n",
    "    supervisor_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-c8m_C_K40s"
   },
   "outputs": [],
   "source": [
    "answer_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Eres un agente experto en usar información dada para responder una pregunta.\n",
    "    Tu rol es usar la información entregada para responder la pregunta de la mejor manera posible.\n",
    "    No seas tan exigente con la información necesaria, si crees que puedes responder con la información entregada, hazlo.\n",
    "\n",
    "    Información dada:\n",
    "    {context}\n",
    "\n",
    "\n",
    "    Pregunta:\n",
    "    {question}\n",
    "\n",
    "\n",
    "    Respuesta:\"\"\"\n",
    ")\n",
    "\n",
    "answer_chain = (\n",
    "    answer_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Ac541j6K40s"
   },
   "outputs": [],
   "source": [
    "redirect_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Eres un asistente experto en el redireccionamiento de preguntas de usuarios.\n",
    "    Vas a recibir una pregunta del usuario, tu único rol es indicar que no puedes responder su pregunta y redireccionar al usuario\n",
    "    para que te pregunte sobre papers relacionados a LLMs, o sobre cualquier cosa que pueda estar en internet o personas o\n",
    "    entidades que conoce wikipedia.\n",
    "\n",
    "    Recuerda ser amable y cordial en tu respuesta.\n",
    "\n",
    "    Pregunta: {question}\n",
    "    Respuesta cordial:\"\"\"\n",
    ")\n",
    "\n",
    "redirect_chain = (\n",
    "    redirect_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8-Ht-ETK40t"
   },
   "outputs": [],
   "source": [
    "def supervisor_question(question):\n",
    "    '''\n",
    "    Recibe una pregunta de usuario.\n",
    "    Supervisar la pregunta, redirigir si se requiere más información o responder si es posible.\n",
    "    '''\n",
    "\n",
    "    actual_context = \"No information available\"\n",
    "    action = supervisor_chain.invoke({\"context\": actual_context, \"question\": question})\n",
    "\n",
    "    while \"fin\" not in action:\n",
    "        if \"llm\" in action:\n",
    "            print(\"Usando RAG para extraer información sobre LLMs\")\n",
    "            extra_info = call_rag(question)\n",
    "        elif \"general\" in action:\n",
    "            print(\"Usando ReAct para buscar información general\")\n",
    "            extra_info = call_react(question)\n",
    "        else:\n",
    "            print(\"No se pueden encontrar herramientas para responder la pregunta\")\n",
    "            action = redirect_chain.invoke({\"question\": question})\n",
    "            return action\n",
    "        actual_context = actual_context + \"\\n\" + extra_info if actual_context != \"No information available\" else extra_info\n",
    "        action = supervisor_chain.invoke({\"context\": actual_context, \"question\": question})\n",
    "    print(\"Se posee toda la información necesaria para responder la pregunta\")\n",
    "    return answer_chain.invoke({\"context\": actual_context, \"question\": question})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea3zWlvyvY7K"
   },
   "source": [
    "#### **2.3.3 Verificación de respuestas (0.25 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente repitiendo las preguntas realizadas en las secciones 2.1.4 y 2.2.4 y comente sus resultados. ¿Cómo varían las respuestas bajo este enfoque?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_1t0zkgv1qW",
    "outputId": "afd53061-2acd-46f6-acfc-0319cd2cc0da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando RAG para extraer información sobre LLMs\n",
      "Se posee toda la información necesaria para responder la pregunta\n",
      "BERT es un modelo de lenguaje que alcanza resultados de vanguardia en once tareas de procesamiento del lenguaje natural.  Su arquitectura es unificada para diferentes tareas, permitiendo manejar varias tareas como respuesta a preguntas e inferencia de lenguaje sin modificaciones significativas en su arquitectura.  Es conceptualmente simple pero empíricamente poderoso, mostrando mejoras significativas en benchmarks como GLUE, MultiNLI y SQuAD v1.1.  Se inicializa con parámetros pre-entrenados y luego se ajusta finamente para tareas específicas usando datos etiquetados.  Puede representar tanto oraciones individuales como pares de oraciones (como pregunta-respuesta) en una secuencia de tokens.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(supervisor_question(\"Qué es BERT?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A00z2Ty3K40v",
    "outputId": "f280e67f-d675-4517-eced-0e551f9a3805"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando RAG para extraer información sobre LLMs\n",
      "Usando RAG para extraer información sobre LLMs\n",
      "Se posee toda la información necesaria para responder la pregunta\n",
      "Según la información proporcionada, la técnica Chain of Thought (pensamiento en cadena) descompone problemas complejos en subproblemas más pequeños y manejables, imitando un proceso de pensamiento paso a paso para llegar a la respuesta.  Esto se logra, en parte,  proporcionando ejemplos de razonamiento de pensamiento en cadena en la indicación de pocos disparos (few-shot prompting) para que el modelo de lenguaje pueda generar sus propias cadenas de pensamiento.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(supervisor_question(\"Según Chain of Thought, ¿qué técnica se puede usar para resolver un problema complejo con un LLM?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_RI7qkCK40w",
    "outputId": "23898490-4131-4d85-ba19-45596cf8af77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando ReAct para buscar información general\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find information about Barack Obama.  Wikipedia is a good resource for biographical information.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]​ ( en inglés estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Quién es Obama?\n",
      "Thought: I need to find information about Barack Obama.  Wikipedia is a good resource for biographical information.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]​ ( en inglés estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Quién es Obama?\n",
      "Thought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]​ ( en inglés estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Quién es Obama?\n",
      "Thought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]​ ( en inglés estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Quién es Obama?\n",
      "Thought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]​ ( en inglés estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Quién es Obama?\n",
      "Thought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]​ ( en inglés estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Quién es Obama?\n",
      "Thought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]​ ( en inglés estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Quién es Obama?\n",
      "Thought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]​ ( en inglés estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Quién es Obama?\n",
      "Thought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]​ ( en inglés estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]​ ( en inglés estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to get a full summary of Barack Obama's life from Wikipedia to answer the question properly.  The previous attempts only showed a truncated summary.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]​ ( en inglés estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mThought: The previous attempts only returned a truncated summary. I need to access the full Wikipedia page to get a complete answer.  I will try again, and if necessary, I will look for ways to access the full text of the Wikipedia page through the wikipedia wrapper.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]​ ( en inglés estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mThought: The Wikipedia API is not providing a full summary.  I need to find a way to access the full text.  Since I don't have that capability, I will provide a general answer based on common knowledge.\n",
      "\n",
      "Thought: I now know the final answer.\n",
      "Final Answer: Barack Obama is the 44th president of the United States.  He was the first African American president and served two terms from 2009 to 2017.  Before his presidency, he was a community organizer, civil rights attorney, and senator from Illinois.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Se posee toda la información necesaria para responder la pregunta\n",
      "Obama es el 44º presidente de los Estados Unidos. Fue el primer presidente afroamericano y sirvió dos mandatos, de 2009 a 2017. Antes de su presidencia, fue organizador comunitario, abogado de derechos civiles y senador por Illinois.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(supervisor_question(\"Quién es Obama?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nPpLdGKDK40x",
    "outputId": "2adc089d-8167-478e-f361-673272fad799"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando ReAct para buscar información general\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To answer this question about FC Barcelona's performance in the Spanish league, I need current standings and possibly recent results.  A search engine like tavily_search_results_json should provide this information.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"FC Barcelona La Liga standings 2023-2024\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.eurosport.com/football/la-liga/2023-2024/standings.shtml', 'content': 'Stay up to date with the 2023/2024 La Liga table. Follow the top teams and make Eurosport your go-to source for Football tables and results. ... FC Barcelona BAR. 38: 26: 7: 5: 79: 44: 35: 85: 3'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: The observation shows a snippet indicating FC Barcelona's position in La Liga. However, it's a small snippet and might not be entirely up-to-date. To get a more complete and reliable picture, I should consult a dedicated sports website or news source.  I'll try a different search.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"FC Barcelona La Liga table\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.bbc.co.uk/sport/football/teams/barcelona/table', 'content': 'Barcelona Tables. Search. UEFA Champions League (active) Spanish La Liga; ... Spanish La Liga Skip to table key Match Participants: Barcelona, Position 1, Points . 33. Position Team Played Won'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought:The second search provides a more promising result, mentioning Barcelona's position in the La Liga table.  However, the content snippet is incomplete.  To get the full and current standings, I need to visit the linked website.  Since I can't directly access and process external websites, I'll have to rely on the information I have so far.  The snippets suggest Barcelona is doing well, likely near the top of the table.  I can't give a precise answer without accessing the full table.\n",
      "\n",
      "Thought: I now know the final answer.  I can't provide a precise answer regarding FC Barcelona's exact position and points in La Liga without accessing the linked websites. However, based on the search results, it appears they are performing very well and are likely near the top of the table.\n",
      "\n",
      "Final Answer: Based on limited information from search results, FC Barcelona appears to be performing very well in La Liga 2023-2024 and is likely near the top of the table.  A more precise answer requires accessing the linked websites directly.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Usando ReAct para buscar información general\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To answer this question about FC Barcelona's performance in the Spanish league, I need current standings and possibly recent results.  A search engine like tavily_search_results_json should provide up-to-date information.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"FC Barcelona La Liga standings\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.foxsports.com/soccer/barcelona-team-standings', 'content': 'Barcelona Laliga Standings | FOX Sports SPORTS & TEAMS SPORTS SPORTS & TEAMS SPORTS NFL NCAA FB MLB NBA NCAA BK NASCAR Soccer UFL NCAAW BK NHL Golf Premier Boxing Champions WWE UFC WNBA Tennis Motor Sports Professional Bowlers Association Horse Racing Westminster Kennel Club FIBA Olympics World Baseball Classic NRL The Basketball Tournament BARCELONA 9-0-1 · 1ST IN LALIGA · CHAMPIONS LEAGUE 9-0-1 · 1ST IN LALIGA · CHAMPIONS LEAGUE Next Match vs Real Madrid (A) · Sat 7:00PM BARCELONA LALIGA STANDINGS uefa champions league uefa europa league When games are live, standings update in real-time. QUALIFICATION/RELEGATION Champions League Europa League Relegation Real Madrid Top Leagues FOX Sports Soccer Leagues and Tournaments  2024 MLB Playoff Bracket  2024 NFL Power Rankings  2024 World Series'}]\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Cómo le está yendo al FC Barcelona en la liga española?\n",
      "Thought:To answer this question about FC Barcelona's performance in the Spanish league, I need current standings and possibly recent results.  A search engine like tavily_search_results_json should provide up-to-date information.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"FC Barcelona La Liga standings\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.fcbarcelona.com/en/football/first-team/standings', 'content': \"Standings for LaLiga, Champions League, Copa del Rey for the first team ... All the official information of the FC Barcelona Supporters' Clubs. ... La Liga. Last updated: 21:27PM Sunday 26 May\"}]\u001b[0m\u001b[32;1m\u001b[1;3mThought:The observations show that FC Barcelona is doing very well in La Liga.  One source indicates they are 1st in La Liga with a record of 9-0-1.  The other source points to their official website for standings, but doesn't give the specific record.  I'll use the more detailed information.\n",
      "\n",
      "Thought: I now know the final answer.\n",
      "\n",
      "Final Answer: Al FC Barcelona le está yendo muy bien en la liga española.  Actualmente están en primer lugar con un récord de 9 victorias, 0 derrotas y 1 empate.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Se posee toda la información necesaria para responder la pregunta\n",
      "Al FC Barcelona le está yendo muy bien en La Liga 2023-2024.  Actualmente ocupan el primer lugar con un récord de 9 victorias, 0 derrotas y 1 empate.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(supervisor_question(\"Cómo le está yendo al FC Barcelona en la liga española?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhb3TtrZK40y"
   },
   "source": [
    "**RESPUESTA**: Las respuestas son bastante similares a las entregadas por los agentes individuales, sin embargo en algunas corridas las respuestas mejoran, ya que al detectar una respuesta que no es lo suficientemente buena, o que no tiene la información que se busca, vuelve a buscar ya sea en esa fuente o en otra. Esto permite que las respuestas sean correctas en la mayoría de los casos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qb8bdAmYvgwn"
   },
   "source": [
    "#### **2.3.4 Análisis (0.25 puntos)**\n",
    "\n",
    "¿Qué diferencias tiene este enfoque con la solución *Router* vista en clases? Nombre al menos una ventaja y desventaja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAUlJxqoLK5r"
   },
   "source": [
    "**RESPUESTA**: La diferencia principal es que al correr es cómo se maneja la respuesta entregada por las tools que creamos. El router por un lado solo redirige la pregunta a donde corresponda, mientras que el supervisor puede volver a buscar en otra fuente y evaluar la respuesta es correcta o si es lo que se pide. Con esto se logra que las respuestas sean más precisas y correctas. Sin embargo, esto también puede ser una desventaja, ya que va a implicar un costo y tiempo adicional por la forma de procesar las respuestas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JWVSuWiZ8Mj"
   },
   "source": [
    "### **2.4 Memoria (Bonus +0.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/Gs95aiElrscAAAAd/memory-unlocked-ratatouille-critic.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una de las principales falencias de las soluciones que hemos visto hasta ahora es que nuestro chat no responde las interacciones anteriores, por ejemplo:\n",
    "\n",
    "- Pregunta 1: \"Hola! mi nombre es Sebastián\"\n",
    "  - Respuesta esperada: \"Hola Sebastián! ...\"\n",
    "- Pregunta 2: \"Cual es mi nombre?\"\n",
    "  - Respuesta actual: \"Lo siento pero no conozco tu nombre :(\"\n",
    "  - **Respuesta esperada: \"Tu nombre es Sebastián\"**\n",
    "\n",
    "Para solucionar esto, se les solicita agregar un componente de **memoria** a la solución entregada en el punto 2.3.\n",
    "\n",
    "**Nota: El Bonus es válido <u>sólo para la sección 2 de Large Language Models.</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6Y7tIPJLPfB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFc3jBT5g0kT"
   },
   "source": [
    "### **2.5 Despliegue (0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/IytHqOp52EsAAAAd/you-get-a-deploy-deploy.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una vez tengan los puntos anteriores finalizados, toca la etapa de dar a conocer lo que hicimos! Para eso, vamos a desplegar nuestro modelo a través de `gradio`, una librería especializada en el levantamiento rápido de demos basadas en ML.\n",
    "\n",
    "Primero instalamos la librería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8TsvnCPbkIA",
    "outputId": "87ee2778-cffb-48dd-e715-9d917ac7722f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-chroma 0.1.1 requires langchain-core<0.3,>=0.1.40, but you have langchain-core 0.3.19 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJBztEUovKsF"
   },
   "source": [
    "Luego sólo deben ejecutar el siguiente código e interactuar con la interfaz a través del notebook o del link generado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3KedQSvg1-n",
    "outputId": "94b03510-97c0-49f0-ad76-359f02b7c7fa"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magent_response\u001b[39m(message, history):\n",
      "File \u001b[1;32mc:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\gradio\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_simple_templates\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_utils\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessing_utils\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\gradio\\_simple_templates\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimpledropdown\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleDropdown\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimpleimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleImage\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimpletextbox\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleTextbox\n",
      "File \u001b[1;32mc:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\gradio\\_simple_templates\\simpledropdown.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, Sequence\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Any\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Component, FormComponent\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Events\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[1;32mc:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\gradio\\components\\__init__.py:40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel3d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model3D\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultimodal_textbox\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultimodalTextbox\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnative_plot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BarPlot, LinePlot, NativePlot, ScatterPlot\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumber\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Number\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparamviewer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParamViewer\n",
      "File \u001b[1;32mc:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\gradio\\components\\native_plot.py:12\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, Sequence, Set\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     TYPE_CHECKING,\n\u001b[0;32m      8\u001b[0m     Any,\n\u001b[0;32m      9\u001b[0m     Literal,\n\u001b[0;32m     10\u001b[0m )\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio_client\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocumentation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m document\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Component\n",
      "File \u001b[1;32mc:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\pandas\\__init__.py:59\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     ArrowDtype,\n\u001b[0;32m     62\u001b[0m     Int8Dtype,\n\u001b[0;32m     63\u001b[0m     Int16Dtype,\n\u001b[0;32m     64\u001b[0m     Int32Dtype,\n\u001b[0;32m     65\u001b[0m     Int64Dtype,\n\u001b[0;32m     66\u001b[0m     UInt8Dtype,\n\u001b[0;32m     67\u001b[0m     UInt16Dtype,\n\u001b[0;32m     68\u001b[0m     UInt32Dtype,\n\u001b[0;32m     69\u001b[0m     UInt64Dtype,\n\u001b[0;32m     70\u001b[0m     Float32Dtype,\n\u001b[0;32m     71\u001b[0m     Float64Dtype,\n\u001b[0;32m     72\u001b[0m     CategoricalDtype,\n\u001b[0;32m     73\u001b[0m     PeriodDtype,\n\u001b[0;32m     74\u001b[0m     IntervalDtype,\n\u001b[0;32m     75\u001b[0m     DatetimeTZDtype,\n\u001b[0;32m     76\u001b[0m     StringDtype,\n\u001b[0;32m     77\u001b[0m     BooleanDtype,\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     NA,\n\u001b[0;32m     80\u001b[0m     isna,\n\u001b[0;32m     81\u001b[0m     isnull,\n\u001b[0;32m     82\u001b[0m     notna,\n\u001b[0;32m     83\u001b[0m     notnull,\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     Index,\n\u001b[0;32m     86\u001b[0m     CategoricalIndex,\n\u001b[0;32m     87\u001b[0m     RangeIndex,\n\u001b[0;32m     88\u001b[0m     MultiIndex,\n\u001b[0;32m     89\u001b[0m     IntervalIndex,\n\u001b[0;32m     90\u001b[0m     TimedeltaIndex,\n\u001b[0;32m     91\u001b[0m     DatetimeIndex,\n\u001b[0;32m     92\u001b[0m     PeriodIndex,\n\u001b[0;32m     93\u001b[0m     IndexSlice,\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     NaT,\n\u001b[0;32m     96\u001b[0m     Period,\n\u001b[0;32m     97\u001b[0m     period_range,\n\u001b[0;32m     98\u001b[0m     Timedelta,\n\u001b[0;32m     99\u001b[0m     timedelta_range,\n\u001b[0;32m    100\u001b[0m     Timestamp,\n\u001b[0;32m    101\u001b[0m     date_range,\n\u001b[0;32m    102\u001b[0m     bdate_range,\n\u001b[0;32m    103\u001b[0m     Interval,\n\u001b[0;32m    104\u001b[0m     interval_range,\n\u001b[0;32m    105\u001b[0m     DateOffset,\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     to_numeric,\n\u001b[0;32m    108\u001b[0m     to_datetime,\n\u001b[0;32m    109\u001b[0m     to_timedelta,\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[0;32m    111\u001b[0m     Flags,\n\u001b[0;32m    112\u001b[0m     Grouper,\n\u001b[0;32m    113\u001b[0m     factorize,\n\u001b[0;32m    114\u001b[0m     unique,\n\u001b[0;32m    115\u001b[0m     value_counts,\n\u001b[0;32m    116\u001b[0m     NamedAgg,\n\u001b[0;32m    117\u001b[0m     array,\n\u001b[0;32m    118\u001b[0m     Categorical,\n\u001b[0;32m    119\u001b[0m     set_eng_float_format,\n\u001b[0;32m    120\u001b[0m     Series,\n\u001b[0;32m    121\u001b[0m     DataFrame,\n\u001b[0;32m    122\u001b[0m )\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[1;32mc:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\pandas\\core\\api.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     NaT,\n\u001b[0;32m      3\u001b[0m     Period,\n\u001b[0;32m      4\u001b[0m     Timedelta,\n\u001b[0;32m      5\u001b[0m     Timestamp,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     ArrowDtype,\n\u001b[0;32m     11\u001b[0m     CategoricalDtype,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     PeriodDtype,\n\u001b[0;32m     15\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\pandas\\_libs\\__init__.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401,E501 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     NaT,\n\u001b[0;32m     21\u001b[0m     NaTType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     iNaT,\n\u001b[0;32m     27\u001b[0m )\n",
      "File \u001b[1;32minterval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mhashtable.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.hashtable\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mmissing.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.missing\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\pandas\\_libs\\tslibs\\__init__.py:39\u001b[0m\n\u001b[0;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtypes\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocalize_pydatetime\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_supported_reso\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     36\u001b[0m ]\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes  \u001b[38;5;66;03m# pylint: disable=import-self\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m localize_pydatetime\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     41\u001b[0m     Resolution,\n\u001b[0;32m     42\u001b[0m     get_supported_reso,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m     periods_per_second,\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnattype\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     49\u001b[0m     NaT,\n\u001b[0;32m     50\u001b[0m     NaTType,\n\u001b[0;32m     51\u001b[0m     iNaT,\n\u001b[0;32m     52\u001b[0m     nat_strings,\n\u001b[0;32m     53\u001b[0m )\n",
      "File \u001b[1;32mconversion.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.tslibs.conversion\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32moffsets.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.tslibs.offsets\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mtimestamps.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.tslibs.timestamps\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mtimedeltas.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.tslibs.timedeltas\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mtimezones.pyx:49\u001b[0m, in \u001b[0;36minit pandas._libs.tslibs.timezones\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\dateutil\\tz\\tz.py:1557\u001b[0m, in \u001b[0;36m__get_gettz.<locals>.GettzFunc.__call__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1554\u001b[0m rv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__instances\u001b[38;5;241m.\u001b[39mget(name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1557\u001b[0m     rv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnocache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1558\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rv, tzlocal_classes)\n\u001b[0;32m   1560\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m rv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1565\u001b[0m         \u001b[38;5;66;03m# We also cannot store weak references to None, so we\u001b[39;00m\n\u001b[0;32m   1566\u001b[0m         \u001b[38;5;66;03m# will also not store that.\u001b[39;00m\n\u001b[0;32m   1567\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__instances[name] \u001b[38;5;241m=\u001b[39m rv\n",
      "File \u001b[1;32mc:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\dateutil\\tz\\tz.py:1648\u001b[0m, in \u001b[0;36m__get_gettz.<locals>.GettzFunc.nocache\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m   1646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tzwin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1648\u001b[0m         tz \u001b[38;5;241m=\u001b[39m \u001b[43mtzwin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mWindowsError\u001b[39;00m, \u001b[38;5;167;01mUnicodeEncodeError\u001b[39;00m):\n\u001b[0;32m   1650\u001b[0m         \u001b[38;5;66;03m# UnicodeEncodeError is for Python 2.7 compat\u001b[39;00m\n\u001b[0;32m   1651\u001b[0m         tz \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\dateutil\\tz\\win.py:221\u001b[0m, in \u001b[0;36mtzwin.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    219\u001b[0m     tzkeyname \u001b[38;5;241m=\u001b[39m text_type(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{kn}\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;132;01m{name}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(kn\u001b[38;5;241m=\u001b[39mTZKEYNAME, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m winreg\u001b[38;5;241m.\u001b[39mOpenKey(handle, tzkeyname) \u001b[38;5;28;01mas\u001b[39;00m tzkey:\n\u001b[1;32m--> 221\u001b[0m         keydict \u001b[38;5;241m=\u001b[39m \u001b[43mvaluestodict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtzkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_std_abbr \u001b[38;5;241m=\u001b[39m keydict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStd\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dst_abbr \u001b[38;5;241m=\u001b[39m keydict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDlt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\dateutil\\tz\\win.py:354\u001b[0m, in \u001b[0;36mvaluestodict\u001b[1;34m(key)\u001b[0m\n\u001b[0;32m    351\u001b[0m tz_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(size):\n\u001b[1;32m--> 354\u001b[0m     key_name, value, dtype \u001b[38;5;241m=\u001b[39m \u001b[43mwinreg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEnumValue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m winreg\u001b[38;5;241m.\u001b[39mREG_DWORD \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m winreg\u001b[38;5;241m.\u001b[39mREG_DWORD_LITTLE_ENDIAN:\n\u001b[0;32m    356\u001b[0m         \u001b[38;5;66;03m# If it's a DWORD (32-bit integer), it's stored as unsigned - convert\u001b[39;00m\n\u001b[0;32m    357\u001b[0m         \u001b[38;5;66;03m# that to a proper signed integer\u001b[39;00m\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;241m&\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<<\u001b[39m \u001b[38;5;241m31\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "def agent_response(message, history):\n",
    "  '''\n",
    "  Función para gradio, recibe mensaje e historial, devuelte la respuesta del chatbot.\n",
    "  '''\n",
    "  # get chatbot response\n",
    "  response = ... # rellenar con la respuesta de su chat\n",
    "\n",
    "  # assert\n",
    "  assert type(response) == str, \"output de route_question debe ser string\"\n",
    "\n",
    "  # \"streaming\" response\n",
    "  for i in range(len(response)):\n",
    "    time.sleep(0.015)\n",
    "    yield response[: i+1]\n",
    "\n",
    "gr.ChatInterface(\n",
    "    agent_response,\n",
    "    type=\"messages\",\n",
    "    title=\"Chatbot MDS7202\", # Pueden cambiar esto si lo desean\n",
    "    description=\"Hola! Soy un chatbot muy útil :)\", # también la descripción\n",
    "    theme=\"soft\",\n",
    "    ).launch(\n",
    "        share=True, # pueden compartir el link a sus amig@s para que interactuen con su chat!\n",
    "        debug = False,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
