{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyPTffTLug7i"
   },
   "source": [
    "# **Laboratorio 11: LLM y Agentes Aut√≥nomos ü§ñ**\n",
    "\n",
    "MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pbWVyntzbvL"
   },
   "source": [
    "### **Cuerpo Docente:**\n",
    "\n",
    "- Profesores: Ignacio Meza, Sebasti√°n Tinoco\n",
    "- Auxiliar: Eduardo Moya\n",
    "- Ayudantes: Nicol√°s Ojeda, Melanie Pe√±a, Valentina Rojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy6ikgVYzghB"
   },
   "source": [
    "### **Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser√°n revisados**\n",
    "\n",
    "- Nombre de alumno 1: Nicolas Herrera\n",
    "- Nombre de alumno 2: Lucas Carrasco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMJ-owchzjFf"
   },
   "source": [
    "### **Link de repositorio de GitHub:** [Repositorio](https://github.com/vspartamo/MDS7202)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUuwsXrKzmkK"
   },
   "source": [
    "## **Temas a tratar**\n",
    "\n",
    "- Reinforcement Learning\n",
    "- Large Language Models\n",
    "\n",
    "## **Reglas:**\n",
    "\n",
    "- **Grupos de 2 personas**\n",
    "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser√°n respondidos por este medio.\n",
    "- Prohibidas las copias.\n",
    "- Pueden usar cualquer matrial del curso que estimen conveniente.\n",
    "\n",
    "### **Objetivos principales del laboratorio**\n",
    "\n",
    "- Resoluci√≥n de problemas secuenciales usando Reinforcement Learning\n",
    "- Habilitar un Chatbot para entregar respuestas √∫tiles usando Large Language Models.\n",
    "\n",
    "El laboratorio deber√° ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m√°ximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante m√°s eficientes que los iteradores nativos sobre DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hmHHQ9BuyAG"
   },
   "source": [
    "## **1. Reinforcement Learning (2.0 puntos)**\n",
    "\n",
    "En esta secci√≥n van a usar m√©todos de RL para resolver dos problemas interesantes: `Blackjack` y `LunarLander`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gOcejYb6uzOO",
    "outputId": "cd24480a-0905-43d9-a9bf-e0be2e915605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/958.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ\u001b[0m \u001b[32m931.8/958.1 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m952.3/958.1 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -qqq gymnasium stable_baselines3\n",
    "!pip install -qqq swig\n",
    "!pip install -qqq gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBPet_Mq8dX9"
   },
   "source": [
    "### **1.1 Blackjack (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.recreoviral.com/wp-content/uploads/2016/08/s3.amazonaws.com-Math.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "La idea de esta subsecci√≥n es que puedan implementar m√©todos de RL y as√≠ generar una estrategia para jugar el cl√°sico juego Blackjack y de paso puedan ~~hacerse millonarios~~ aprender a resolver problemas mediante RL.\n",
    "\n",
    "Comencemos primero preparando el ambiente. El siguiente bloque de c√≥digo transforma las observaciones del ambiente a `np.array`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vqeddmvKPfBt"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import MultiDiscrete\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LpZ8bBKk9ZlU"
   },
   "outputs": [],
   "source": [
    "class FlattenObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(FlattenObservation, self).__init__(env)\n",
    "        self.observation_space = MultiDiscrete(np.array([32, 11, 2]))\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.array(observation).flatten()\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "env = FlattenObservation(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJ6J1_-Y9nHO"
   },
   "source": [
    "#### **1.1.1 Descripci√≥n de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripci√≥n sobre el ambiente [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/) y su formulaci√≥n en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5i1Wt1p770x"
   },
   "source": [
    "`escriba su respuesta ac√°`\n",
    "\n",
    "El ambiente Blackjack simula el conocido juego de cartas, formulado como un proceso de decisi√≥n de Markov (MDP). Primero se dar√° una breve descripci√≥n del juego y luego sus componentes como MDP:\n",
    "\n",
    "El objetivo del BlackJack es obtener una mano de cartas cuya suma sea lo m√°s cercana posible a 21 sin pasarse. Sus reglas son:\n",
    "\n",
    "1. **Cartas y valores**:\n",
    "    - Las cartas numeradas del 2 al 10 tienen su valor nominal.\n",
    "    - Las cartas con figuras (J, Q, K) valen 10 puntos cada una.\n",
    "    - El As puede valer 1 u 11 puntos, dependiendo de cu√°l valor beneficie m√°s al jugador.\n",
    "\n",
    "2. **El juego**:\n",
    "    - Cada jugador recibe dos cartas al inicio del juego, al igual que el dealer (la casa). Una de las cartas del dealer se muestra boca arriba y la otra boca abajo.\n",
    "    - Los jugadores pueden optar por \"pedir\" (hit) m√°s cartas para acercarse a 21 o \"plantarse\" (stand) si creen que su mano es lo suficientemente fuerte.\n",
    "    - Si un jugador supera los 21 puntos, pierde autom√°ticamente (esto se llama \"bust\").\n",
    "    - Despu√©s de que todos los jugadores han terminado sus turnos, el dealer revela su carta oculta y debe seguir ciertas reglas: debe pedir cartas hasta alcanzar al menos 17 puntos y plantarse si tiene 17 o m√°s.\n",
    "\n",
    "3. **Ganador**:\n",
    "    - Si el jugador tiene una mano m√°s cercana a 21 que la del dealer sin pasarse, gana.\n",
    "    - Si el dealer supera los 21 puntos, todos los jugadores que no se hayan pasado ganan.\n",
    "    - Si el jugador y el dealer tienen la misma cantidad de puntos, es un empate (push) y se devuelve la apuesta al jugador.\n",
    "\n",
    "Descripci√≥n como MDP:\n",
    "\n",
    "**Estados**\n",
    "El espacio de estados es un tupla que describe:\n",
    "1. **Suma actual del jugador**: Rango [4, 21] (sin superar 21).\n",
    "2. **Valor de la carta visible del dealer**: Rango [1, 10], donde 1 representa un As.\n",
    "3. **As utilizable**: Binario (0 o 1), indicando si el jugador tiene un As que puede contar como 11 sin pasarse de 21.\n",
    "\n",
    "Estado: `(player_sum, dealer_card, usable_ace)`\n",
    "\n",
    "**Acciones**\n",
    "El jugador tiene dos acciones discretas:\n",
    "- `0`: Stick (plantarse, no pedir m√°s cartas).\n",
    "- `1`: Hit (pedir otra carta).\n",
    "\n",
    "Espacio de acciones: `Discrete(2)`\n",
    "\n",
    "**Recompensas**\n",
    "- `+1`: Si el jugador gana (sin blackjack natural).\n",
    "- `-1`: Si el jugador pierde.\n",
    "- `0`: Si hay empate (draw).\n",
    "- `+1.5`: Si el jugador gana con un blackjack natural (opcional seg√∫n el par√°metro `natural`).\n",
    "- Nota: Si el jugador excede 21, pierde inmediatamente (`-1`).\n",
    "\n",
    "**Din√°mica del MDP**\n",
    "- Transici√≥n: Las acciones determinan si el jugador toma otra carta o se planta. Si toma otra carta y excede 21, el episodio termina.\n",
    "- El dealer sigue una estrategia fija: revela su carta oculta y contin√∫a sacando hasta alcanzar un m√≠nimo de 17.\n",
    "- Los episodios terminan cuando el jugador se planta, excede 21, o el dealer determina el resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmcX6bRC9agQ"
   },
   "source": [
    "#### **1.1.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulaci√≥n 5000 veces y reporte el promedio y desviaci√≥n de las recompensas. ¬øC√≥mo calificar√≠a el performance de esta pol√≠tica? ¬øC√≥mo podr√≠a interpretar las recompensas obtenidas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9p2PrLLR9yju",
    "outputId": "60c4bedc-16c5-4b2e-b51e-8498ef7e3dc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de las recompensas: -0.3628\n",
      "Desviaci√≥n est√°ndar de las recompensas: 0.9088\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 5000\n",
    "rewards = []\n",
    "\n",
    "for _ in range(n_episodes):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample() #accion random\n",
    "        observation, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "print(f\"Promedio de las recompensas: {np.mean(rewards):.4f}\")\n",
    "print(f\"Desviaci√≥n est√°ndar de las recompensas: {np.std(rewards):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gg7sE3TLL-0v"
   },
   "source": [
    "Un promedio de recompensa de -0.3628 dice que jugar al azar tiende levemente a la perdida, hace sentido pues si no se tiene en consideraci√≥n la suma actual de las cartas y se pide al azar, pareciera ser m√°s probable pasarse de 21, adem√°s, la desviaci√≥n est√°ndar de 0.9088 dice que hay una alta variabilidad en los resultados, por lo que las recompensas obtenidad no parecieran tener alguna tendencia muy definida, por lo que no es una buena manera de maximizar las recompensas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEO_dY4x_SJu"
   },
   "source": [
    "#### **1.1.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `Blackjack`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xnze0KomMHku"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from gymnasium.spaces import MultiDiscrete\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m9JsFA1wGmnH",
    "outputId": "f22d6a00-ec3f-438d-ffb8-91fcb647197c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07     |\n",
      "|    ep_rew_mean        | -0.26    |\n",
      "| time/                 |          |\n",
      "|    fps                | 376      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.131   |\n",
      "|    explained_variance | 0.509    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.0137   |\n",
      "|    value_loss         | 0.54     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.02     |\n",
      "|    ep_rew_mean        | -0.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0649  |\n",
      "|    explained_variance | 0.0252   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.00424  |\n",
      "|    value_loss         | 1.14     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1         |\n",
      "|    ep_rew_mean        | -0.2      |\n",
      "| time/                 |           |\n",
      "|    fps                | 355       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.114    |\n",
      "|    explained_variance | -2.51e+14 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | 0.0105    |\n",
      "|    value_loss         | 0.818     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01     |\n",
      "|    ep_rew_mean        | -0.26    |\n",
      "| time/                 |          |\n",
      "|    fps                | 353      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0322  |\n",
      "|    explained_variance | -3.7e+14 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.00306 |\n",
      "|    value_loss         | 0.66     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.02     |\n",
      "|    ep_rew_mean        | -0.29    |\n",
      "| time/                 |          |\n",
      "|    fps                | 365      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0481  |\n",
      "|    explained_variance | -0.0731  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.16     |\n",
      "|    value_loss         | 1.34     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.01     |\n",
      "|    ep_rew_mean        | -0.04    |\n",
      "| time/                 |          |\n",
      "|    fps                | 374      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.159   |\n",
      "|    explained_variance | 0.0542   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.00744 |\n",
      "|    value_loss         | 0.757    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.08      |\n",
      "|    ep_rew_mean        | -0.17     |\n",
      "| time/                 |           |\n",
      "|    fps                | 368       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.269    |\n",
      "|    explained_variance | -5.09e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -0.554    |\n",
      "|    value_loss         | 0.852     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04     |\n",
      "|    ep_rew_mean        | -0.07    |\n",
      "| time/                 |          |\n",
      "|    fps                | 374      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0234  |\n",
      "|    explained_variance | 0.594    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.000509 |\n",
      "|    value_loss         | 0.401    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.03     |\n",
      "|    ep_rew_mean        | -0.15    |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0215  |\n",
      "|    explained_variance | 0.818    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -0.00117 |\n",
      "|    value_loss         | 0.216    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04     |\n",
      "|    ep_rew_mean        | -0.28    |\n",
      "| time/                 |          |\n",
      "|    fps                | 383      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.139   |\n",
      "|    explained_variance | 0.594    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.095    |\n",
      "|    value_loss         | 0.396    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.06     |\n",
      "|    ep_rew_mean        | -0.14    |\n",
      "| time/                 |          |\n",
      "|    fps                | 386      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00851 |\n",
      "|    explained_variance | 0.332    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.000337 |\n",
      "|    value_loss         | 0.645    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11     |\n",
      "|    ep_rew_mean        | -0.21    |\n",
      "| time/                 |          |\n",
      "|    fps                | 388      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00406 |\n",
      "|    explained_variance | 0.266    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.000129 |\n",
      "|    value_loss         | 0.918    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04     |\n",
      "|    ep_rew_mean        | -0.09    |\n",
      "| time/                 |          |\n",
      "|    fps                | 383      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.17    |\n",
      "|    explained_variance | 0.65     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -0.0424  |\n",
      "|    value_loss         | 0.381    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09     |\n",
      "|    ep_rew_mean        | -0.25    |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0354  |\n",
      "|    explained_variance | 0.475    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.000968 |\n",
      "|    value_loss         | 0.549    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1        |\n",
      "|    ep_rew_mean        | -0.18    |\n",
      "| time/                 |          |\n",
      "|    fps                | 382      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 19       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.116   |\n",
      "|    explained_variance | 0.315    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.0167   |\n",
      "|    value_loss         | 0.696    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08     |\n",
      "|    ep_rew_mean        | -0.04    |\n",
      "| time/                 |          |\n",
      "|    fps                | 384      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0076  |\n",
      "|    explained_variance | -0.153   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.000873 |\n",
      "|    value_loss         | 1.21     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.06      |\n",
      "|    ep_rew_mean        | -0.17     |\n",
      "| time/                 |           |\n",
      "|    fps                | 387       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00398  |\n",
      "|    explained_variance | 0.595     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -5.44e-05 |\n",
      "|    value_loss         | 0.391     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12     |\n",
      "|    ep_rew_mean        | -0.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 381      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 23       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.223   |\n",
      "|    explained_variance | 0.427    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.2      |\n",
      "|    value_loss         | 0.558    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08     |\n",
      "|    ep_rew_mean        | -0.07    |\n",
      "| time/                 |          |\n",
      "|    fps                | 383      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 24       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0114  |\n",
      "|    explained_variance | -0.21    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.000238 |\n",
      "|    value_loss         | 1.22     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.06      |\n",
      "|    ep_rew_mean        | -0.36     |\n",
      "| time/                 |           |\n",
      "|    fps                | 381       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 26        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0089   |\n",
      "|    explained_variance | 0.902     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -0.000261 |\n",
      "|    value_loss         | 0.109     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.05     |\n",
      "|    ep_rew_mean        | -0.07    |\n",
      "| time/                 |          |\n",
      "|    fps                | 380      |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 27       |\n",
      "|    total_timesteps    | 10500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00471 |\n",
      "|    explained_variance | 0.639    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | 8.17e-06 |\n",
      "|    value_loss         | 0.45     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.02     |\n",
      "|    ep_rew_mean        | -0.34    |\n",
      "| time/                 |          |\n",
      "|    fps                | 372      |\n",
      "|    iterations         | 2200     |\n",
      "|    time_elapsed       | 29       |\n",
      "|    total_timesteps    | 11000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0333  |\n",
      "|    explained_variance | 0.54     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | -0.00151 |\n",
      "|    value_loss         | 0.4      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.06     |\n",
      "|    ep_rew_mean        | -0.06    |\n",
      "| time/                 |          |\n",
      "|    fps                | 359      |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 31       |\n",
      "|    total_timesteps    | 11500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00383 |\n",
      "|    explained_variance | 0.173    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | 1.69e-05 |\n",
      "|    value_loss         | 0.774    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.02     |\n",
      "|    ep_rew_mean        | -0.14    |\n",
      "| time/                 |          |\n",
      "|    fps                | 362      |\n",
      "|    iterations         | 2400     |\n",
      "|    time_elapsed       | 33       |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00159 |\n",
      "|    explained_variance | -0.492   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | 0.00016  |\n",
      "|    value_loss         | 1.5      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.08      |\n",
      "|    ep_rew_mean        | -0.15     |\n",
      "| time/                 |           |\n",
      "|    fps                | 364       |\n",
      "|    iterations         | 2500      |\n",
      "|    time_elapsed       | 34        |\n",
      "|    total_timesteps    | 12500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00128  |\n",
      "|    explained_variance | -0.204    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2499      |\n",
      "|    policy_loss        | -7.77e-05 |\n",
      "|    value_loss         | 1.17      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.09      |\n",
      "|    ep_rew_mean        | -0.06     |\n",
      "| time/                 |           |\n",
      "|    fps                | 366       |\n",
      "|    iterations         | 2600      |\n",
      "|    time_elapsed       | 35        |\n",
      "|    total_timesteps    | 13000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.173    |\n",
      "|    explained_variance | -7.25e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2599      |\n",
      "|    policy_loss        | 0.0229    |\n",
      "|    value_loss         | 1.2       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.12      |\n",
      "|    ep_rew_mean        | -0.32     |\n",
      "| time/                 |           |\n",
      "|    fps                | 368       |\n",
      "|    iterations         | 2700      |\n",
      "|    time_elapsed       | 36        |\n",
      "|    total_timesteps    | 13500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00914  |\n",
      "|    explained_variance | 0.627     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2699      |\n",
      "|    policy_loss        | -0.000868 |\n",
      "|    value_loss         | 0.392     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09     |\n",
      "|    ep_rew_mean        | -0.01    |\n",
      "| time/                 |          |\n",
      "|    fps                | 370      |\n",
      "|    iterations         | 2800     |\n",
      "|    time_elapsed       | 37       |\n",
      "|    total_timesteps    | 14000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00236 |\n",
      "|    explained_variance | 0.249    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | 0.000144 |\n",
      "|    value_loss         | 0.556    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.07      |\n",
      "|    ep_rew_mean        | -0.04     |\n",
      "| time/                 |           |\n",
      "|    fps                | 371       |\n",
      "|    iterations         | 2900      |\n",
      "|    time_elapsed       | 38        |\n",
      "|    total_timesteps    | 14500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00322  |\n",
      "|    explained_variance | 0.236     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2899      |\n",
      "|    policy_loss        | -3.36e-05 |\n",
      "|    value_loss         | 0.576     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.07      |\n",
      "|    ep_rew_mean        | -0.03     |\n",
      "| time/                 |           |\n",
      "|    fps                | 373       |\n",
      "|    iterations         | 3000      |\n",
      "|    time_elapsed       | 40        |\n",
      "|    total_timesteps    | 15000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00136  |\n",
      "|    explained_variance | 0.665     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2999      |\n",
      "|    policy_loss        | -6.28e-05 |\n",
      "|    value_loss         | 0.328     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.05     |\n",
      "|    ep_rew_mean        | -0.26    |\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 3100     |\n",
      "|    time_elapsed       | 41       |\n",
      "|    total_timesteps    | 15500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.104   |\n",
      "|    explained_variance | 0.408    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3099     |\n",
      "|    policy_loss        | -0.0446  |\n",
      "|    value_loss         | 0.491    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.07      |\n",
      "|    ep_rew_mean        | -0.28     |\n",
      "| time/                 |           |\n",
      "|    fps                | 374       |\n",
      "|    iterations         | 3200      |\n",
      "|    time_elapsed       | 42        |\n",
      "|    total_timesteps    | 16000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00367  |\n",
      "|    explained_variance | 0.633     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3199      |\n",
      "|    policy_loss        | -0.000216 |\n",
      "|    value_loss         | 0.368     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.15      |\n",
      "|    ep_rew_mean        | -0.21     |\n",
      "| time/                 |           |\n",
      "|    fps                | 371       |\n",
      "|    iterations         | 3300      |\n",
      "|    time_elapsed       | 44        |\n",
      "|    total_timesteps    | 16500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.27     |\n",
      "|    explained_variance | -1.14e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3299      |\n",
      "|    policy_loss        | -0.406    |\n",
      "|    value_loss         | 0.56      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.1       |\n",
      "|    ep_rew_mean        | -0.21     |\n",
      "| time/                 |           |\n",
      "|    fps                | 372       |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 45        |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000761 |\n",
      "|    explained_variance | 0.347     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | -4.96e-05 |\n",
      "|    value_loss         | 0.527     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.11      |\n",
      "|    ep_rew_mean        | -0.19     |\n",
      "| time/                 |           |\n",
      "|    fps                | 373       |\n",
      "|    iterations         | 3500      |\n",
      "|    time_elapsed       | 46        |\n",
      "|    total_timesteps    | 17500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000361 |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3499      |\n",
      "|    policy_loss        | -2.22e-05 |\n",
      "|    value_loss         | 0.5       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08     |\n",
      "|    ep_rew_mean        | -0.04    |\n",
      "| time/                 |          |\n",
      "|    fps                | 374      |\n",
      "|    iterations         | 3600     |\n",
      "|    time_elapsed       | 48       |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0866  |\n",
      "|    explained_variance | 0.0322   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3599     |\n",
      "|    policy_loss        | 0.0271   |\n",
      "|    value_loss         | 0.803    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1      |\n",
      "|    ep_rew_mean        | 0.01     |\n",
      "| time/                 |          |\n",
      "|    fps                | 376      |\n",
      "|    iterations         | 3700     |\n",
      "|    time_elapsed       | 49       |\n",
      "|    total_timesteps    | 18500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0111  |\n",
      "|    explained_variance | -2.45    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3699     |\n",
      "|    policy_loss        | 0.00105  |\n",
      "|    value_loss         | 0.651    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.06      |\n",
      "|    ep_rew_mean        | -0.06     |\n",
      "| time/                 |           |\n",
      "|    fps                | 375       |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 50        |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000804 |\n",
      "|    explained_variance | 0.653     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | -8.56e-05 |\n",
      "|    value_loss         | 0.365     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.06      |\n",
      "|    ep_rew_mean        | -0.17     |\n",
      "| time/                 |           |\n",
      "|    fps                | 376       |\n",
      "|    iterations         | 3900      |\n",
      "|    time_elapsed       | 51        |\n",
      "|    total_timesteps    | 19500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000355 |\n",
      "|    explained_variance | 0.64      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3899      |\n",
      "|    policy_loss        | -2.93e-05 |\n",
      "|    value_loss         | 0.653     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.08      |\n",
      "|    ep_rew_mean        | -0.16     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 53        |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000605 |\n",
      "|    explained_variance | -1.41     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3999      |\n",
      "|    policy_loss        | -2.38e-05 |\n",
      "|    value_loss         | 2.1       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07     |\n",
      "|    ep_rew_mean        | -0.11    |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 4100     |\n",
      "|    time_elapsed       | 54       |\n",
      "|    total_timesteps    | 20500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0461  |\n",
      "|    explained_variance | 0.724    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4099     |\n",
      "|    policy_loss        | -0.0122  |\n",
      "|    value_loss         | 0.537    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.06     |\n",
      "|    ep_rew_mean        | -0.08    |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 4200     |\n",
      "|    time_elapsed       | 55       |\n",
      "|    total_timesteps    | 21000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0483  |\n",
      "|    explained_variance | -0.38    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4199     |\n",
      "|    policy_loss        | -0.0115  |\n",
      "|    value_loss         | 1.11     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.07      |\n",
      "|    ep_rew_mean        | -0.06     |\n",
      "| time/                 |           |\n",
      "|    fps                | 376       |\n",
      "|    iterations         | 4300      |\n",
      "|    time_elapsed       | 57        |\n",
      "|    total_timesteps    | 21500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000121 |\n",
      "|    explained_variance | 0.103     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4299      |\n",
      "|    policy_loss        | -3.7e-06  |\n",
      "|    value_loss         | 0.503     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.15      |\n",
      "|    ep_rew_mean        | -0.29     |\n",
      "| time/                 |           |\n",
      "|    fps                | 376       |\n",
      "|    iterations         | 4400      |\n",
      "|    time_elapsed       | 58        |\n",
      "|    total_timesteps    | 22000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0309   |\n",
      "|    explained_variance | -1.58e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4399      |\n",
      "|    policy_loss        | -0.0071   |\n",
      "|    value_loss         | 1.37      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.11      |\n",
      "|    ep_rew_mean        | -0.19     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 59        |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000654 |\n",
      "|    explained_variance | 0.771     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4499      |\n",
      "|    policy_loss        | -8.47e-06 |\n",
      "|    value_loss         | 0.503     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07     |\n",
      "|    ep_rew_mean        | -0.02    |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 4600     |\n",
      "|    time_elapsed       | 60       |\n",
      "|    total_timesteps    | 23000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00102 |\n",
      "|    explained_variance | 0.275    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4599     |\n",
      "|    policy_loss        | 8.72e-05 |\n",
      "|    value_loss         | 0.464    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04     |\n",
      "|    ep_rew_mean        | 0.02     |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 4700     |\n",
      "|    time_elapsed       | 61       |\n",
      "|    total_timesteps    | 23500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0196  |\n",
      "|    explained_variance | 0.0767   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4699     |\n",
      "|    policy_loss        | 0.00649  |\n",
      "|    value_loss         | 1.73     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.15     |\n",
      "|    ep_rew_mean        | -0.36    |\n",
      "| time/                 |          |\n",
      "|    fps                | 380      |\n",
      "|    iterations         | 4800     |\n",
      "|    time_elapsed       | 63       |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0154  |\n",
      "|    explained_variance | 0.371    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4799     |\n",
      "|    policy_loss        | 0.000438 |\n",
      "|    value_loss         | 0.658    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.06      |\n",
      "|    ep_rew_mean        | -0.12     |\n",
      "| time/                 |           |\n",
      "|    fps                | 381       |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 64        |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000719 |\n",
      "|    explained_variance | 0.8       |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | -3.58e-06 |\n",
      "|    value_loss         | 0.216     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.03      |\n",
      "|    ep_rew_mean        | -0.19     |\n",
      "| time/                 |           |\n",
      "|    fps                | 382       |\n",
      "|    iterations         | 5000      |\n",
      "|    time_elapsed       | 65        |\n",
      "|    total_timesteps    | 25000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000611 |\n",
      "|    explained_variance | -0.00603  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4999      |\n",
      "|    policy_loss        | 3.23e-05  |\n",
      "|    value_loss         | 1.19      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.05     |\n",
      "|    ep_rew_mean        | -0.15    |\n",
      "| time/                 |          |\n",
      "|    fps                | 382      |\n",
      "|    iterations         | 5100     |\n",
      "|    time_elapsed       | 66       |\n",
      "|    total_timesteps    | 25500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00123 |\n",
      "|    explained_variance | 0.19     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5099     |\n",
      "|    policy_loss        | 2.61e-05 |\n",
      "|    value_loss         | 0.864    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.08      |\n",
      "|    ep_rew_mean        | 0.05      |\n",
      "| time/                 |           |\n",
      "|    fps                | 383       |\n",
      "|    iterations         | 5200      |\n",
      "|    time_elapsed       | 67        |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000482 |\n",
      "|    explained_variance | 0.231     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5199      |\n",
      "|    policy_loss        | 1.34e-05  |\n",
      "|    value_loss         | 0.685     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1      |\n",
      "|    ep_rew_mean        | -0.11    |\n",
      "| time/                 |          |\n",
      "|    fps                | 381      |\n",
      "|    iterations         | 5300     |\n",
      "|    time_elapsed       | 69       |\n",
      "|    total_timesteps    | 26500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00465 |\n",
      "|    explained_variance | 0.411    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5299     |\n",
      "|    policy_loss        | 0.000852 |\n",
      "|    value_loss         | 0.628    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.03     |\n",
      "|    ep_rew_mean        | -0.14    |\n",
      "| time/                 |          |\n",
      "|    fps                | 380      |\n",
      "|    iterations         | 5400     |\n",
      "|    time_elapsed       | 70       |\n",
      "|    total_timesteps    | 27000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00578 |\n",
      "|    explained_variance | -0.0439  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5399     |\n",
      "|    policy_loss        | 0.000112 |\n",
      "|    value_loss         | 0.839    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11     |\n",
      "|    ep_rew_mean        | -0.07    |\n",
      "| time/                 |          |\n",
      "|    fps                | 381      |\n",
      "|    iterations         | 5500     |\n",
      "|    time_elapsed       | 72       |\n",
      "|    total_timesteps    | 27500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00292 |\n",
      "|    explained_variance | -0.229   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5499     |\n",
      "|    policy_loss        | 0.000201 |\n",
      "|    value_loss         | 1.08     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.06      |\n",
      "|    ep_rew_mean        | -0.14     |\n",
      "| time/                 |           |\n",
      "|    fps                | 381       |\n",
      "|    iterations         | 5600      |\n",
      "|    time_elapsed       | 73        |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00689  |\n",
      "|    explained_variance | 0.161     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5599      |\n",
      "|    policy_loss        | -0.000303 |\n",
      "|    value_loss         | 1.04      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.04      |\n",
      "|    ep_rew_mean        | -0.14     |\n",
      "| time/                 |           |\n",
      "|    fps                | 382       |\n",
      "|    iterations         | 5700      |\n",
      "|    time_elapsed       | 74        |\n",
      "|    total_timesteps    | 28500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00069  |\n",
      "|    explained_variance | 0.207     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5699      |\n",
      "|    policy_loss        | -8.32e-06 |\n",
      "|    value_loss         | 0.549     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.05     |\n",
      "|    ep_rew_mean        | -0.06    |\n",
      "| time/                 |          |\n",
      "|    fps                | 382      |\n",
      "|    iterations         | 5800     |\n",
      "|    time_elapsed       | 75       |\n",
      "|    total_timesteps    | 29000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.139   |\n",
      "|    explained_variance | -0.0365  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5799     |\n",
      "|    policy_loss        | 0.0488   |\n",
      "|    value_loss         | 0.645    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.05      |\n",
      "|    ep_rew_mean        | -0.24     |\n",
      "| time/                 |           |\n",
      "|    fps                | 383       |\n",
      "|    iterations         | 5900      |\n",
      "|    time_elapsed       | 76        |\n",
      "|    total_timesteps    | 29500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000293 |\n",
      "|    explained_variance | 0.17      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5899      |\n",
      "|    policy_loss        | -5.46e-06 |\n",
      "|    value_loss         | 0.532     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.05      |\n",
      "|    ep_rew_mean        | -0.07     |\n",
      "| time/                 |           |\n",
      "|    fps                | 384       |\n",
      "|    iterations         | 6000      |\n",
      "|    time_elapsed       | 78        |\n",
      "|    total_timesteps    | 30000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000502 |\n",
      "|    explained_variance | -0.613    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5999      |\n",
      "|    policy_loss        | -5.37e-06 |\n",
      "|    value_loss         | 1.03      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.07      |\n",
      "|    ep_rew_mean        | -0.2      |\n",
      "| time/                 |           |\n",
      "|    fps                | 385       |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 79        |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000674 |\n",
      "|    explained_variance | 0.27      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6099      |\n",
      "|    policy_loss        | 7.59e-05  |\n",
      "|    value_loss         | 1.17      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.08      |\n",
      "|    ep_rew_mean        | -0.21     |\n",
      "| time/                 |           |\n",
      "|    fps                | 385       |\n",
      "|    iterations         | 6200      |\n",
      "|    time_elapsed       | 80        |\n",
      "|    total_timesteps    | 31000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000692 |\n",
      "|    explained_variance | -0.0769   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6199      |\n",
      "|    policy_loss        | 7.61e-05  |\n",
      "|    value_loss         | 1.34      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.04      |\n",
      "|    ep_rew_mean        | -0.35     |\n",
      "| time/                 |           |\n",
      "|    fps                | 384       |\n",
      "|    iterations         | 6300      |\n",
      "|    time_elapsed       | 81        |\n",
      "|    total_timesteps    | 31500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00147  |\n",
      "|    explained_variance | 0.821     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6299      |\n",
      "|    policy_loss        | -5.84e-05 |\n",
      "|    value_loss         | 0.16      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07     |\n",
      "|    ep_rew_mean        | -0.12    |\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 6400     |\n",
      "|    time_elapsed       | 85       |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0108  |\n",
      "|    explained_variance | 0.194    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6399     |\n",
      "|    policy_loss        | 0.00142  |\n",
      "|    value_loss         | 0.77     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.1       |\n",
      "|    ep_rew_mean        | -0.14     |\n",
      "| time/                 |           |\n",
      "|    fps                | 375       |\n",
      "|    iterations         | 6500      |\n",
      "|    time_elapsed       | 86        |\n",
      "|    total_timesteps    | 32500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000943 |\n",
      "|    explained_variance | 0.957     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6499      |\n",
      "|    policy_loss        | -3.32e-05 |\n",
      "|    value_loss         | 0.148     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09     |\n",
      "|    ep_rew_mean        | -0.26    |\n",
      "| time/                 |          |\n",
      "|    fps                | 376      |\n",
      "|    iterations         | 6600     |\n",
      "|    time_elapsed       | 87       |\n",
      "|    total_timesteps    | 33000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0997  |\n",
      "|    explained_variance | 0.128    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6599     |\n",
      "|    policy_loss        | -0.0568  |\n",
      "|    value_loss         | 0.73     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08     |\n",
      "|    ep_rew_mean        | -0.22    |\n",
      "| time/                 |          |\n",
      "|    fps                | 377      |\n",
      "|    iterations         | 6700     |\n",
      "|    time_elapsed       | 88       |\n",
      "|    total_timesteps    | 33500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.123   |\n",
      "|    explained_variance | 0.408    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6699     |\n",
      "|    policy_loss        | 0.0461   |\n",
      "|    value_loss         | 0.738    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.02      |\n",
      "|    ep_rew_mean        | -0.35     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 6800      |\n",
      "|    time_elapsed       | 90        |\n",
      "|    total_timesteps    | 34000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00533  |\n",
      "|    explained_variance | 0.656     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6799      |\n",
      "|    policy_loss        | -0.000415 |\n",
      "|    value_loss         | 0.192     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09     |\n",
      "|    ep_rew_mean        | -0.06    |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 6900     |\n",
      "|    time_elapsed       | 91       |\n",
      "|    total_timesteps    | 34500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00161 |\n",
      "|    explained_variance | 0.393    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6899     |\n",
      "|    policy_loss        | -1.9e-05 |\n",
      "|    value_loss         | 0.598    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.14      |\n",
      "|    ep_rew_mean        | -0.2      |\n",
      "| time/                 |           |\n",
      "|    fps                | 378       |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 92        |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00143  |\n",
      "|    explained_variance | 0.696     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | -0.000109 |\n",
      "|    value_loss         | 0.277     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09     |\n",
      "|    ep_rew_mean        | -0.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 7100     |\n",
      "|    time_elapsed       | 93       |\n",
      "|    total_timesteps    | 35500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0533  |\n",
      "|    explained_variance | 0.413    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7099     |\n",
      "|    policy_loss        | 0.00775  |\n",
      "|    value_loss         | 0.568    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12     |\n",
      "|    ep_rew_mean        | -0.17    |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 7200     |\n",
      "|    time_elapsed       | 94       |\n",
      "|    total_timesteps    | 36000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0149  |\n",
      "|    explained_variance | -0.942   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7199     |\n",
      "|    policy_loss        | 8.73e-05 |\n",
      "|    value_loss         | 1.3      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.11      |\n",
      "|    ep_rew_mean        | -0.08     |\n",
      "| time/                 |           |\n",
      "|    fps                | 378       |\n",
      "|    iterations         | 7300      |\n",
      "|    time_elapsed       | 96        |\n",
      "|    total_timesteps    | 36500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00442  |\n",
      "|    explained_variance | -0.198    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7299      |\n",
      "|    policy_loss        | -0.000327 |\n",
      "|    value_loss         | 1.27      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.06     |\n",
      "|    ep_rew_mean        | -0.05    |\n",
      "| time/                 |          |\n",
      "|    fps                | 377      |\n",
      "|    iterations         | 7400     |\n",
      "|    time_elapsed       | 97       |\n",
      "|    total_timesteps    | 37000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00222 |\n",
      "|    explained_variance | 0.331    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7399     |\n",
      "|    policy_loss        | 0.000191 |\n",
      "|    value_loss         | 0.652    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.1       |\n",
      "|    ep_rew_mean        | -0.02     |\n",
      "| time/                 |           |\n",
      "|    fps                | 375       |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 99        |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0019   |\n",
      "|    explained_variance | 0.299     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7499      |\n",
      "|    policy_loss        | -3.09e-07 |\n",
      "|    value_loss         | 0.673     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11     |\n",
      "|    ep_rew_mean        | -0.04    |\n",
      "| time/                 |          |\n",
      "|    fps                | 373      |\n",
      "|    iterations         | 7600     |\n",
      "|    time_elapsed       | 101      |\n",
      "|    total_timesteps    | 38000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00113 |\n",
      "|    explained_variance | -0.373   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7599     |\n",
      "|    policy_loss        | 3.62e-05 |\n",
      "|    value_loss         | 1.32     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09     |\n",
      "|    ep_rew_mean        | -0.09    |\n",
      "| time/                 |          |\n",
      "|    fps                | 374      |\n",
      "|    iterations         | 7700     |\n",
      "|    time_elapsed       | 102      |\n",
      "|    total_timesteps    | 38500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0261  |\n",
      "|    explained_variance | -0.142   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7699     |\n",
      "|    policy_loss        | 0.0028   |\n",
      "|    value_loss         | 1.1      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.06     |\n",
      "|    ep_rew_mean        | -0.11    |\n",
      "| time/                 |          |\n",
      "|    fps                | 374      |\n",
      "|    iterations         | 7800     |\n",
      "|    time_elapsed       | 104      |\n",
      "|    total_timesteps    | 39000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00117 |\n",
      "|    explained_variance | 0.269    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7799     |\n",
      "|    policy_loss        | 5.89e-05 |\n",
      "|    value_loss         | 0.544    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.07      |\n",
      "|    ep_rew_mean        | -0.23     |\n",
      "| time/                 |           |\n",
      "|    fps                | 375       |\n",
      "|    iterations         | 7900      |\n",
      "|    time_elapsed       | 105       |\n",
      "|    total_timesteps    | 39500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00357  |\n",
      "|    explained_variance | 0.175     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7899      |\n",
      "|    policy_loss        | -0.000157 |\n",
      "|    value_loss         | 0.549     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09     |\n",
      "|    ep_rew_mean        | -0.13    |\n",
      "| time/                 |          |\n",
      "|    fps                | 376      |\n",
      "|    iterations         | 8000     |\n",
      "|    time_elapsed       | 106      |\n",
      "|    total_timesteps    | 40000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.14    |\n",
      "|    explained_variance | 0.694    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7999     |\n",
      "|    policy_loss        | -0.136   |\n",
      "|    value_loss         | 0.311    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.08      |\n",
      "|    ep_rew_mean        | -0.24     |\n",
      "| time/                 |           |\n",
      "|    fps                | 376       |\n",
      "|    iterations         | 8100      |\n",
      "|    time_elapsed       | 107       |\n",
      "|    total_timesteps    | 40500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00534  |\n",
      "|    explained_variance | -1.15e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8099      |\n",
      "|    policy_loss        | -0.000185 |\n",
      "|    value_loss         | 0.118     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1      |\n",
      "|    ep_rew_mean        | -0.17    |\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 8200     |\n",
      "|    time_elapsed       | 109      |\n",
      "|    total_timesteps    | 41000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00958 |\n",
      "|    explained_variance | -0.385   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8199     |\n",
      "|    policy_loss        | -8.6e-05 |\n",
      "|    value_loss         | 0.843    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1      |\n",
      "|    ep_rew_mean        | -0.08    |\n",
      "| time/                 |          |\n",
      "|    fps                | 374      |\n",
      "|    iterations         | 8300     |\n",
      "|    time_elapsed       | 110      |\n",
      "|    total_timesteps    | 41500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.157   |\n",
      "|    explained_variance | 0.366    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8299     |\n",
      "|    policy_loss        | -0.139   |\n",
      "|    value_loss         | 0.611    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09     |\n",
      "|    ep_rew_mean        | -0.09    |\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 8400     |\n",
      "|    time_elapsed       | 111      |\n",
      "|    total_timesteps    | 42000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.016   |\n",
      "|    explained_variance | 0.108    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8399     |\n",
      "|    policy_loss        | 0.00256  |\n",
      "|    value_loss         | 1.25     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.06      |\n",
      "|    ep_rew_mean        | -0.4      |\n",
      "| time/                 |           |\n",
      "|    fps                | 375       |\n",
      "|    iterations         | 8500      |\n",
      "|    time_elapsed       | 113       |\n",
      "|    total_timesteps    | 42500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00836  |\n",
      "|    explained_variance | 0.191     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8499      |\n",
      "|    policy_loss        | -0.000316 |\n",
      "|    value_loss         | 0.9       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04     |\n",
      "|    ep_rew_mean        | -0.08    |\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 8600     |\n",
      "|    time_elapsed       | 114      |\n",
      "|    total_timesteps    | 43000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0973  |\n",
      "|    explained_variance | -0.143   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8599     |\n",
      "|    policy_loss        | -0.031   |\n",
      "|    value_loss         | 1.19     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04     |\n",
      "|    ep_rew_mean        | -0.36    |\n",
      "| time/                 |          |\n",
      "|    fps                | 376      |\n",
      "|    iterations         | 8700     |\n",
      "|    time_elapsed       | 115      |\n",
      "|    total_timesteps    | 43500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00209 |\n",
      "|    explained_variance | 0.0759   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8699     |\n",
      "|    policy_loss        | 7.44e-05 |\n",
      "|    value_loss         | 0.897    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.05      |\n",
      "|    ep_rew_mean        | -0.21     |\n",
      "| time/                 |           |\n",
      "|    fps                | 376       |\n",
      "|    iterations         | 8800      |\n",
      "|    time_elapsed       | 116       |\n",
      "|    total_timesteps    | 44000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00129  |\n",
      "|    explained_variance | 0.861     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8799      |\n",
      "|    policy_loss        | -4.78e-05 |\n",
      "|    value_loss         | 0.238     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.05      |\n",
      "|    ep_rew_mean        | -0.32     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 8900      |\n",
      "|    time_elapsed       | 117       |\n",
      "|    total_timesteps    | 44500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000779 |\n",
      "|    explained_variance | 0.733     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | -3.92e-05 |\n",
      "|    value_loss         | 0.282     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09     |\n",
      "|    ep_rew_mean        | -0.24    |\n",
      "| time/                 |          |\n",
      "|    fps                | 377      |\n",
      "|    iterations         | 9000     |\n",
      "|    time_elapsed       | 119      |\n",
      "|    total_timesteps    | 45000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.137   |\n",
      "|    explained_variance | 0.094    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8999     |\n",
      "|    policy_loss        | 0.0907   |\n",
      "|    value_loss         | 0.964    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.06      |\n",
      "|    ep_rew_mean        | -0.23     |\n",
      "| time/                 |           |\n",
      "|    fps                | 376       |\n",
      "|    iterations         | 9100      |\n",
      "|    time_elapsed       | 120       |\n",
      "|    total_timesteps    | 45500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00111  |\n",
      "|    explained_variance | 0.0485    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9099      |\n",
      "|    policy_loss        | -3.18e-05 |\n",
      "|    value_loss         | 0.618     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.07      |\n",
      "|    ep_rew_mean        | -0.24     |\n",
      "| time/                 |           |\n",
      "|    fps                | 374       |\n",
      "|    iterations         | 9200      |\n",
      "|    time_elapsed       | 122       |\n",
      "|    total_timesteps    | 46000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000648 |\n",
      "|    explained_variance | 0.159     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9199      |\n",
      "|    policy_loss        | -2.3e-05  |\n",
      "|    value_loss         | 0.811     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.11      |\n",
      "|    ep_rew_mean        | 0.01      |\n",
      "| time/                 |           |\n",
      "|    fps                | 373       |\n",
      "|    iterations         | 9300      |\n",
      "|    time_elapsed       | 124       |\n",
      "|    total_timesteps    | 46500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00335  |\n",
      "|    explained_variance | 0.176     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9299      |\n",
      "|    policy_loss        | -0.000239 |\n",
      "|    value_loss         | 0.792     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09     |\n",
      "|    ep_rew_mean        | -0.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 373      |\n",
      "|    iterations         | 9400     |\n",
      "|    time_elapsed       | 125      |\n",
      "|    total_timesteps    | 47000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0166  |\n",
      "|    explained_variance | 0.105    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9399     |\n",
      "|    policy_loss        | 0.00121  |\n",
      "|    value_loss         | 0.828    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.12      |\n",
      "|    ep_rew_mean        | -0.12     |\n",
      "| time/                 |           |\n",
      "|    fps                | 374       |\n",
      "|    iterations         | 9500      |\n",
      "|    time_elapsed       | 126       |\n",
      "|    total_timesteps    | 47500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000701 |\n",
      "|    explained_variance | 0.936     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9499      |\n",
      "|    policy_loss        | -1.91e-05 |\n",
      "|    value_loss         | 0.171     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.14      |\n",
      "|    ep_rew_mean        | -0.1      |\n",
      "| time/                 |           |\n",
      "|    fps                | 374       |\n",
      "|    iterations         | 9600      |\n",
      "|    time_elapsed       | 128       |\n",
      "|    total_timesteps    | 48000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000595 |\n",
      "|    explained_variance | -0.347    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9599      |\n",
      "|    policy_loss        | 2.78e-06  |\n",
      "|    value_loss         | 1.11      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.15     |\n",
      "|    ep_rew_mean        | -0.06    |\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 9700     |\n",
      "|    time_elapsed       | 129      |\n",
      "|    total_timesteps    | 48500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0242  |\n",
      "|    explained_variance | 0.534    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9699     |\n",
      "|    policy_loss        | 0.000727 |\n",
      "|    value_loss         | 0.458    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08     |\n",
      "|    ep_rew_mean        | -0.25    |\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 9800     |\n",
      "|    time_elapsed       | 130      |\n",
      "|    total_timesteps    | 49000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0369  |\n",
      "|    explained_variance | 0.102    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9799     |\n",
      "|    policy_loss        | 0.0105   |\n",
      "|    value_loss         | 1.49     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09     |\n",
      "|    ep_rew_mean        | -0.19    |\n",
      "| time/                 |          |\n",
      "|    fps                | 376      |\n",
      "|    iterations         | 9900     |\n",
      "|    time_elapsed       | 131      |\n",
      "|    total_timesteps    | 49500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00105 |\n",
      "|    explained_variance | 0.0668   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9899     |\n",
      "|    policy_loss        | 4.07e-05 |\n",
      "|    value_loss         | 1.7      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.11      |\n",
      "|    ep_rew_mean        | -0.01     |\n",
      "| time/                 |           |\n",
      "|    fps                | 376       |\n",
      "|    iterations         | 10000     |\n",
      "|    time_elapsed       | 132       |\n",
      "|    total_timesteps    | 50000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00445  |\n",
      "|    explained_variance | 0.588     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9999      |\n",
      "|    policy_loss        | -0.000234 |\n",
      "|    value_loss         | 0.398     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.08      |\n",
      "|    ep_rew_mean        | -0.34     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 10100     |\n",
      "|    time_elapsed       | 133       |\n",
      "|    total_timesteps    | 50500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0774   |\n",
      "|    explained_variance | -1.83e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10099     |\n",
      "|    policy_loss        | -0.0133   |\n",
      "|    value_loss         | 0.489     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07     |\n",
      "|    ep_rew_mean        | -0.11    |\n",
      "| time/                 |          |\n",
      "|    fps                | 377      |\n",
      "|    iterations         | 10200    |\n",
      "|    time_elapsed       | 135      |\n",
      "|    total_timesteps    | 51000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0299  |\n",
      "|    explained_variance | -0.0651  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10199    |\n",
      "|    policy_loss        | -0.00357 |\n",
      "|    value_loss         | 0.918    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.05      |\n",
      "|    ep_rew_mean        | -0.2      |\n",
      "| time/                 |           |\n",
      "|    fps                | 376       |\n",
      "|    iterations         | 10300     |\n",
      "|    time_elapsed       | 136       |\n",
      "|    total_timesteps    | 51500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0057   |\n",
      "|    explained_variance | 0.497     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10299     |\n",
      "|    policy_loss        | -0.000345 |\n",
      "|    value_loss         | 0.488     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07     |\n",
      "|    ep_rew_mean        | -0.04    |\n",
      "| time/                 |          |\n",
      "|    fps                | 376      |\n",
      "|    iterations         | 10400    |\n",
      "|    time_elapsed       | 138      |\n",
      "|    total_timesteps    | 52000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00241 |\n",
      "|    explained_variance | 0.147    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10399    |\n",
      "|    policy_loss        | 0.000338 |\n",
      "|    value_loss         | 1.19     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.15      |\n",
      "|    ep_rew_mean        | -0.19     |\n",
      "| time/                 |           |\n",
      "|    fps                | 376       |\n",
      "|    iterations         | 10500     |\n",
      "|    time_elapsed       | 139       |\n",
      "|    total_timesteps    | 52500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00269  |\n",
      "|    explained_variance | 0.945     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10499     |\n",
      "|    policy_loss        | -9.04e-05 |\n",
      "|    value_loss         | 0.164     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07     |\n",
      "|    ep_rew_mean        | -0.17    |\n",
      "| time/                 |          |\n",
      "|    fps                | 376      |\n",
      "|    iterations         | 10600    |\n",
      "|    time_elapsed       | 140      |\n",
      "|    total_timesteps    | 53000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0155  |\n",
      "|    explained_variance | 0.385    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10599    |\n",
      "|    policy_loss        | 0.00304  |\n",
      "|    value_loss         | 1.27     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.04     |\n",
      "|    ep_rew_mean        | -0.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 377      |\n",
      "|    iterations         | 10700    |\n",
      "|    time_elapsed       | 141      |\n",
      "|    total_timesteps    | 53500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00877 |\n",
      "|    explained_variance | 0.27     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10699    |\n",
      "|    policy_loss        | 0.000605 |\n",
      "|    value_loss         | 0.736    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.13     |\n",
      "|    ep_rew_mean        | -0.32    |\n",
      "| time/                 |          |\n",
      "|    fps                | 377      |\n",
      "|    iterations         | 10800    |\n",
      "|    time_elapsed       | 143      |\n",
      "|    total_timesteps    | 54000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0216  |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10799    |\n",
      "|    policy_loss        | -0.00353 |\n",
      "|    value_loss         | 0.344    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.08      |\n",
      "|    ep_rew_mean        | -0.22     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 10900     |\n",
      "|    time_elapsed       | 144       |\n",
      "|    total_timesteps    | 54500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0141   |\n",
      "|    explained_variance | -2.87e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10899     |\n",
      "|    policy_loss        | -0.00071  |\n",
      "|    value_loss         | 0.245     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.1       |\n",
      "|    ep_rew_mean        | -0.09     |\n",
      "| time/                 |           |\n",
      "|    fps                | 378       |\n",
      "|    iterations         | 11000     |\n",
      "|    time_elapsed       | 145       |\n",
      "|    total_timesteps    | 55000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00242  |\n",
      "|    explained_variance | 0.467     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10999     |\n",
      "|    policy_loss        | -0.000146 |\n",
      "|    value_loss         | 0.475     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.1       |\n",
      "|    ep_rew_mean        | -0.12     |\n",
      "| time/                 |           |\n",
      "|    fps                | 378       |\n",
      "|    iterations         | 11100     |\n",
      "|    time_elapsed       | 146       |\n",
      "|    total_timesteps    | 55500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.032    |\n",
      "|    explained_variance | 0.734     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11099     |\n",
      "|    policy_loss        | -0.000535 |\n",
      "|    value_loss         | 0.144     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.07      |\n",
      "|    ep_rew_mean        | -0.41     |\n",
      "| time/                 |           |\n",
      "|    fps                | 378       |\n",
      "|    iterations         | 11200     |\n",
      "|    time_elapsed       | 147       |\n",
      "|    total_timesteps    | 56000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00446  |\n",
      "|    explained_variance | -0.259    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11199     |\n",
      "|    policy_loss        | -0.000236 |\n",
      "|    value_loss         | 0.841     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.1      |\n",
      "|    ep_rew_mean        | -0.22    |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 11300    |\n",
      "|    time_elapsed       | 149      |\n",
      "|    total_timesteps    | 56500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00255 |\n",
      "|    explained_variance | 0.0782   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11299    |\n",
      "|    policy_loss        | 0.000142 |\n",
      "|    value_loss         | 0.895    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.05      |\n",
      "|    ep_rew_mean        | 0         |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 11400     |\n",
      "|    time_elapsed       | 151       |\n",
      "|    total_timesteps    | 57000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0076   |\n",
      "|    explained_variance | -0.213    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11399     |\n",
      "|    policy_loss        | -0.000346 |\n",
      "|    value_loss         | 1.23      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.04      |\n",
      "|    ep_rew_mean        | -0.26     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 11500     |\n",
      "|    time_elapsed       | 152       |\n",
      "|    total_timesteps    | 57500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00153  |\n",
      "|    explained_variance | -0.531    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11499     |\n",
      "|    policy_loss        | -7.53e-05 |\n",
      "|    value_loss         | 0.353     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.05     |\n",
      "|    ep_rew_mean        | 0        |\n",
      "| time/                 |          |\n",
      "|    fps                | 377      |\n",
      "|    iterations         | 11600    |\n",
      "|    time_elapsed       | 153      |\n",
      "|    total_timesteps    | 58000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00188 |\n",
      "|    explained_variance | 0.29     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11599    |\n",
      "|    policy_loss        | 0.000168 |\n",
      "|    value_loss         | 0.654    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.06      |\n",
      "|    ep_rew_mean        | -0.3      |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 11700     |\n",
      "|    time_elapsed       | 154       |\n",
      "|    total_timesteps    | 58500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0231   |\n",
      "|    explained_variance | -0.0367   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11699     |\n",
      "|    policy_loss        | -0.000995 |\n",
      "|    value_loss         | 1.01      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.11      |\n",
      "|    ep_rew_mean        | -0.33     |\n",
      "| time/                 |           |\n",
      "|    fps                | 378       |\n",
      "|    iterations         | 11800     |\n",
      "|    time_elapsed       | 155       |\n",
      "|    total_timesteps    | 59000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0395   |\n",
      "|    explained_variance | -1.01e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11799     |\n",
      "|    policy_loss        | -0.00212  |\n",
      "|    value_loss         | 0.589     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.24     |\n",
      "|    ep_rew_mean        | -0.04    |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 11900    |\n",
      "|    time_elapsed       | 157      |\n",
      "|    total_timesteps    | 59500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0862  |\n",
      "|    explained_variance | -0.537   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11899    |\n",
      "|    policy_loss        | 0.0049   |\n",
      "|    value_loss         | 1.1      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09     |\n",
      "|    ep_rew_mean        | -0.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 12000    |\n",
      "|    time_elapsed       | 158      |\n",
      "|    total_timesteps    | 60000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0421  |\n",
      "|    explained_variance | 0.401    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11999    |\n",
      "|    policy_loss        | -0.00315 |\n",
      "|    value_loss         | 0.221    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07     |\n",
      "|    ep_rew_mean        | -0.09    |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 12100    |\n",
      "|    time_elapsed       | 159      |\n",
      "|    total_timesteps    | 60500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.188   |\n",
      "|    explained_variance | -0.0665  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12099    |\n",
      "|    policy_loss        | 0.196    |\n",
      "|    value_loss         | 1.15     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11     |\n",
      "|    ep_rew_mean        | -0.25    |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 12200    |\n",
      "|    time_elapsed       | 160      |\n",
      "|    total_timesteps    | 61000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.232   |\n",
      "|    explained_variance | -0.203   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12199    |\n",
      "|    policy_loss        | -0.0935  |\n",
      "|    value_loss         | 0.255    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.14     |\n",
      "|    ep_rew_mean        | -0.18    |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 12300    |\n",
      "|    time_elapsed       | 162      |\n",
      "|    total_timesteps    | 61500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.213   |\n",
      "|    explained_variance | 0.314    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12299    |\n",
      "|    policy_loss        | 0.0647   |\n",
      "|    value_loss         | 1.86     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08     |\n",
      "|    ep_rew_mean        | -0.13    |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 12400    |\n",
      "|    time_elapsed       | 163      |\n",
      "|    total_timesteps    | 62000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0199  |\n",
      "|    explained_variance | -0.793   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12399    |\n",
      "|    policy_loss        | 0.00296  |\n",
      "|    value_loss         | 0.488    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.05     |\n",
      "|    ep_rew_mean        | -0.14    |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 12500    |\n",
      "|    time_elapsed       | 165      |\n",
      "|    total_timesteps    | 62500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00778 |\n",
      "|    explained_variance | 0.469    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12499    |\n",
      "|    policy_loss        | 0.00107  |\n",
      "|    value_loss         | 0.512    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.09      |\n",
      "|    ep_rew_mean        | -0.29     |\n",
      "| time/                 |           |\n",
      "|    fps                | 378       |\n",
      "|    iterations         | 12600     |\n",
      "|    time_elapsed       | 166       |\n",
      "|    total_timesteps    | 63000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000156 |\n",
      "|    explained_variance | 0.223     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12599     |\n",
      "|    policy_loss        | 3.91e-06  |\n",
      "|    value_loss         | 0.75      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.09      |\n",
      "|    ep_rew_mean        | 0.01      |\n",
      "| time/                 |           |\n",
      "|    fps                | 379       |\n",
      "|    iterations         | 12700     |\n",
      "|    time_elapsed       | 167       |\n",
      "|    total_timesteps    | 63500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00136  |\n",
      "|    explained_variance | 0.111     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12699     |\n",
      "|    policy_loss        | -4.78e-05 |\n",
      "|    value_loss         | 0.588     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09     |\n",
      "|    ep_rew_mean        | -0.13    |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 12800    |\n",
      "|    time_elapsed       | 168      |\n",
      "|    total_timesteps    | 64000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00419 |\n",
      "|    explained_variance | -0.261   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12799    |\n",
      "|    policy_loss        | 0.000734 |\n",
      "|    value_loss         | 1.23     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.15     |\n",
      "|    ep_rew_mean        | -0.22    |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 12900    |\n",
      "|    time_elapsed       | 169      |\n",
      "|    total_timesteps    | 64500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.124   |\n",
      "|    explained_variance | 0.522    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12899    |\n",
      "|    policy_loss        | 0.151    |\n",
      "|    value_loss         | 0.52     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07     |\n",
      "|    ep_rew_mean        | -0.25    |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 13000    |\n",
      "|    time_elapsed       | 171      |\n",
      "|    total_timesteps    | 65000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.053   |\n",
      "|    explained_variance | 0.809    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12999    |\n",
      "|    policy_loss        | -0.00181 |\n",
      "|    value_loss         | 0.288    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12     |\n",
      "|    ep_rew_mean        | 0.01     |\n",
      "| time/                 |          |\n",
      "|    fps                | 380      |\n",
      "|    iterations         | 13100    |\n",
      "|    time_elapsed       | 172      |\n",
      "|    total_timesteps    | 65500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0155  |\n",
      "|    explained_variance | -0.308   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13099    |\n",
      "|    policy_loss        | 0.00504  |\n",
      "|    value_loss         | 1.4      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11     |\n",
      "|    ep_rew_mean        | -0.19    |\n",
      "| time/                 |          |\n",
      "|    fps                | 380      |\n",
      "|    iterations         | 13200    |\n",
      "|    time_elapsed       | 173      |\n",
      "|    total_timesteps    | 66000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0756  |\n",
      "|    explained_variance | 0.57     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13199    |\n",
      "|    policy_loss        | 0.0235   |\n",
      "|    value_loss         | 0.385    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.08      |\n",
      "|    ep_rew_mean        | -0.05     |\n",
      "| time/                 |           |\n",
      "|    fps                | 379       |\n",
      "|    iterations         | 13300     |\n",
      "|    time_elapsed       | 175       |\n",
      "|    total_timesteps    | 66500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00014  |\n",
      "|    explained_variance | 0.368     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13299     |\n",
      "|    policy_loss        | -1.32e-06 |\n",
      "|    value_loss         | 0.608     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.17     |\n",
      "|    ep_rew_mean        | -0.06    |\n",
      "| time/                 |          |\n",
      "|    fps                | 377      |\n",
      "|    iterations         | 13400    |\n",
      "|    time_elapsed       | 177      |\n",
      "|    total_timesteps    | 67000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00836 |\n",
      "|    explained_variance | -0.427   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13399    |\n",
      "|    policy_loss        | -0.00143 |\n",
      "|    value_loss         | 1.14     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.13     |\n",
      "|    ep_rew_mean        | -0.13    |\n",
      "| time/                 |          |\n",
      "|    fps                | 377      |\n",
      "|    iterations         | 13500    |\n",
      "|    time_elapsed       | 178      |\n",
      "|    total_timesteps    | 67500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0606  |\n",
      "|    explained_variance | 0.239    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13499    |\n",
      "|    policy_loss        | -0.00326 |\n",
      "|    value_loss         | 0.765    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12     |\n",
      "|    ep_rew_mean        | -0.17    |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 13600    |\n",
      "|    time_elapsed       | 179      |\n",
      "|    total_timesteps    | 68000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0626  |\n",
      "|    explained_variance | -0.25    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13599    |\n",
      "|    policy_loss        | 0.0315   |\n",
      "|    value_loss         | 1.79     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12     |\n",
      "|    ep_rew_mean        | -0.31    |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 13700    |\n",
      "|    time_elapsed       | 181      |\n",
      "|    total_timesteps    | 68500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00379 |\n",
      "|    explained_variance | 0.161    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13699    |\n",
      "|    policy_loss        | 0.000547 |\n",
      "|    value_loss         | 0.992    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.07      |\n",
      "|    ep_rew_mean        | -0.17     |\n",
      "| time/                 |           |\n",
      "|    fps                | 378       |\n",
      "|    iterations         | 13800     |\n",
      "|    time_elapsed       | 182       |\n",
      "|    total_timesteps    | 69000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.65e-05 |\n",
      "|    explained_variance | 0.0533    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13799     |\n",
      "|    policy_loss        | 4.97e-07  |\n",
      "|    value_loss         | 0.812     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.14     |\n",
      "|    ep_rew_mean        | -0.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 13900    |\n",
      "|    time_elapsed       | 183      |\n",
      "|    total_timesteps    | 69500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.14    |\n",
      "|    explained_variance | -4.85    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13899    |\n",
      "|    policy_loss        | 0.0661   |\n",
      "|    value_loss         | 0.684    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.13      |\n",
      "|    ep_rew_mean        | -0.32     |\n",
      "| time/                 |           |\n",
      "|    fps                | 379       |\n",
      "|    iterations         | 14000     |\n",
      "|    time_elapsed       | 184       |\n",
      "|    total_timesteps    | 70000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00138  |\n",
      "|    explained_variance | -0.0136   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13999     |\n",
      "|    policy_loss        | -4.01e-05 |\n",
      "|    value_loss         | 0.701     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.02      |\n",
      "|    ep_rew_mean        | -0.04     |\n",
      "| time/                 |           |\n",
      "|    fps                | 379       |\n",
      "|    iterations         | 14100     |\n",
      "|    time_elapsed       | 185       |\n",
      "|    total_timesteps    | 70500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00628  |\n",
      "|    explained_variance | 0.41      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14099     |\n",
      "|    policy_loss        | -0.000461 |\n",
      "|    value_loss         | 0.582     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.12     |\n",
      "|    ep_rew_mean        | -0.15    |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 14200    |\n",
      "|    time_elapsed       | 186      |\n",
      "|    total_timesteps    | 71000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0017  |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14199    |\n",
      "|    policy_loss        | 0.000314 |\n",
      "|    value_loss         | 0.647    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.14     |\n",
      "|    ep_rew_mean        | -0.12    |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 14300    |\n",
      "|    time_elapsed       | 188      |\n",
      "|    total_timesteps    | 71500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00712 |\n",
      "|    explained_variance | 0.369    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14299    |\n",
      "|    policy_loss        | 6.11e-05 |\n",
      "|    value_loss         | 0.656    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.05     |\n",
      "|    ep_rew_mean        | -0.11    |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 14400    |\n",
      "|    time_elapsed       | 189      |\n",
      "|    total_timesteps    | 72000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00091 |\n",
      "|    explained_variance | 0.834    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14399    |\n",
      "|    policy_loss        | 1.71e-05 |\n",
      "|    value_loss         | 0.167    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.11     |\n",
      "|    ep_rew_mean        | -0.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 14500    |\n",
      "|    time_elapsed       | 191      |\n",
      "|    total_timesteps    | 72500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0425  |\n",
      "|    explained_variance | 0.292    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14499    |\n",
      "|    policy_loss        | -0.00167 |\n",
      "|    value_loss         | 0.869    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.09      |\n",
      "|    ep_rew_mean        | -0.33     |\n",
      "| time/                 |           |\n",
      "|    fps                | 379       |\n",
      "|    iterations         | 14600     |\n",
      "|    time_elapsed       | 192       |\n",
      "|    total_timesteps    | 73000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0023   |\n",
      "|    explained_variance | 0.831     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14599     |\n",
      "|    policy_loss        | -8.71e-05 |\n",
      "|    value_loss         | 0.264     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.14     |\n",
      "|    ep_rew_mean        | -0.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 14700    |\n",
      "|    time_elapsed       | 193      |\n",
      "|    total_timesteps    | 73500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00931 |\n",
      "|    explained_variance | -0.115   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14699    |\n",
      "|    policy_loss        | 0.00165  |\n",
      "|    value_loss         | 0.815    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.08     |\n",
      "|    ep_rew_mean        | -0.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 14800    |\n",
      "|    time_elapsed       | 194      |\n",
      "|    total_timesteps    | 74000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00282 |\n",
      "|    explained_variance | 0.469    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14799    |\n",
      "|    policy_loss        | 0.000128 |\n",
      "|    value_loss         | 0.589    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.09     |\n",
      "|    ep_rew_mean        | -0.14    |\n",
      "| time/                 |          |\n",
      "|    fps                | 380      |\n",
      "|    iterations         | 14900    |\n",
      "|    time_elapsed       | 196      |\n",
      "|    total_timesteps    | 74500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00462 |\n",
      "|    explained_variance | -0.927   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14899    |\n",
      "|    policy_loss        | 0.00109  |\n",
      "|    value_loss         | 1.43     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.09      |\n",
      "|    ep_rew_mean        | -0.12     |\n",
      "| time/                 |           |\n",
      "|    fps                | 380       |\n",
      "|    iterations         | 15000     |\n",
      "|    time_elapsed       | 197       |\n",
      "|    total_timesteps    | 75000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0108   |\n",
      "|    explained_variance | -0.496    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14999     |\n",
      "|    policy_loss        | -5.85e-05 |\n",
      "|    value_loss         | 1.11      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.1       |\n",
      "|    ep_rew_mean        | -0.22     |\n",
      "| time/                 |           |\n",
      "|    fps                | 380       |\n",
      "|    iterations         | 15100     |\n",
      "|    time_elapsed       | 198       |\n",
      "|    total_timesteps    | 75500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00115  |\n",
      "|    explained_variance | 0.19      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15099     |\n",
      "|    policy_loss        | -0.000103 |\n",
      "|    value_loss         | 0.853     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.21     |\n",
      "|    ep_rew_mean        | 0.01     |\n",
      "| time/                 |          |\n",
      "|    fps                | 381      |\n",
      "|    iterations         | 15200    |\n",
      "|    time_elapsed       | 199      |\n",
      "|    total_timesteps    | 76000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0197  |\n",
      "|    explained_variance | -0.114   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15199    |\n",
      "|    policy_loss        | 0.00361  |\n",
      "|    value_loss         | 0.703    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.1       |\n",
      "|    ep_rew_mean        | -0.16     |\n",
      "| time/                 |           |\n",
      "|    fps                | 381       |\n",
      "|    iterations         | 15300     |\n",
      "|    time_elapsed       | 200       |\n",
      "|    total_timesteps    | 76500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000191 |\n",
      "|    explained_variance | -0.92     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15299     |\n",
      "|    policy_loss        | 2.39e-05  |\n",
      "|    value_loss         | 1.82      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.07      |\n",
      "|    ep_rew_mean        | -0.3      |\n",
      "| time/                 |           |\n",
      "|    fps                | 381       |\n",
      "|    iterations         | 15400     |\n",
      "|    time_elapsed       | 202       |\n",
      "|    total_timesteps    | 77000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000199 |\n",
      "|    explained_variance | 0.671     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15399     |\n",
      "|    policy_loss        | -8.86e-06 |\n",
      "|    value_loss         | 0.326     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.13     |\n",
      "|    ep_rew_mean        | -0.16    |\n",
      "| time/                 |          |\n",
      "|    fps                | 380      |\n",
      "|    iterations         | 15500    |\n",
      "|    time_elapsed       | 203      |\n",
      "|    total_timesteps    | 77500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0529  |\n",
      "|    explained_variance | -0.0416  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15499    |\n",
      "|    policy_loss        | -0.00281 |\n",
      "|    value_loss         | 0.673    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.13      |\n",
      "|    ep_rew_mean        | -0.23     |\n",
      "| time/                 |           |\n",
      "|    fps                | 378       |\n",
      "|    iterations         | 15600     |\n",
      "|    time_elapsed       | 205       |\n",
      "|    total_timesteps    | 78000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -8.49e-05 |\n",
      "|    explained_variance | 0.00335   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15599     |\n",
      "|    policy_loss        | -4.56e-06 |\n",
      "|    value_loss         | 0.663     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.06      |\n",
      "|    ep_rew_mean        | -0.21     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 15700     |\n",
      "|    time_elapsed       | 208       |\n",
      "|    total_timesteps    | 78500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000284 |\n",
      "|    explained_variance | 0.669     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15699     |\n",
      "|    policy_loss        | -1.27e-05 |\n",
      "|    value_loss         | 0.579     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.05      |\n",
      "|    ep_rew_mean        | -0.25     |\n",
      "| time/                 |           |\n",
      "|    fps                | 375       |\n",
      "|    iterations         | 15800     |\n",
      "|    time_elapsed       | 210       |\n",
      "|    total_timesteps    | 79000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000757 |\n",
      "|    explained_variance | -0.138    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15799     |\n",
      "|    policy_loss        | 1.53e-06  |\n",
      "|    value_loss         | 0.645     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.12      |\n",
      "|    ep_rew_mean        | -0.2      |\n",
      "| time/                 |           |\n",
      "|    fps                | 375       |\n",
      "|    iterations         | 15900     |\n",
      "|    time_elapsed       | 211       |\n",
      "|    total_timesteps    | 79500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000185 |\n",
      "|    explained_variance | 0.767     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15899     |\n",
      "|    policy_loss        | -8.73e-06 |\n",
      "|    value_loss         | 0.424     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.07     |\n",
      "|    ep_rew_mean        | -0.08    |\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 16000    |\n",
      "|    time_elapsed       | 212      |\n",
      "|    total_timesteps    | 80000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0837  |\n",
      "|    explained_variance | 0.346    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15999    |\n",
      "|    policy_loss        | -0.00682 |\n",
      "|    value_loss         | 0.176    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.34     |\n",
      "|    ep_rew_mean        | -0.18    |\n",
      "| time/                 |          |\n",
      "|    fps                | 376      |\n",
      "|    iterations         | 16100    |\n",
      "|    time_elapsed       | 214      |\n",
      "|    total_timesteps    | 80500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.237   |\n",
      "|    explained_variance | 0.746    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16099    |\n",
      "|    policy_loss        | -0.175   |\n",
      "|    value_loss         | 0.268    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.33     |\n",
      "|    ep_rew_mean        | -0.23    |\n",
      "| time/                 |          |\n",
      "|    fps                | 376      |\n",
      "|    iterations         | 16200    |\n",
      "|    time_elapsed       | 215      |\n",
      "|    total_timesteps    | 81000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.16    |\n",
      "|    explained_variance | 0.0502   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16199    |\n",
      "|    policy_loss        | -0.391   |\n",
      "|    value_loss         | 1.23     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.45     |\n",
      "|    ep_rew_mean        | -0.12    |\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 16300    |\n",
      "|    time_elapsed       | 216      |\n",
      "|    total_timesteps    | 81500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0541  |\n",
      "|    explained_variance | 0.691    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16299    |\n",
      "|    policy_loss        | 0.00529  |\n",
      "|    value_loss         | 0.239    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.6      |\n",
      "|    ep_rew_mean        | -0.17    |\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 16400    |\n",
      "|    time_elapsed       | 218      |\n",
      "|    total_timesteps    | 82000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.155   |\n",
      "|    explained_variance | 0.908    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16399    |\n",
      "|    policy_loss        | -0.12    |\n",
      "|    value_loss         | 0.349    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.59     |\n",
      "|    ep_rew_mean        | 0.01     |\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 16500    |\n",
      "|    time_elapsed       | 219      |\n",
      "|    total_timesteps    | 82500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0357  |\n",
      "|    explained_variance | -0.37    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16499    |\n",
      "|    policy_loss        | -0.00577 |\n",
      "|    value_loss         | 0.351    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.55      |\n",
      "|    ep_rew_mean        | -0.19     |\n",
      "| time/                 |           |\n",
      "|    fps                | 376       |\n",
      "|    iterations         | 16600     |\n",
      "|    time_elapsed       | 220       |\n",
      "|    total_timesteps    | 83000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0115   |\n",
      "|    explained_variance | -0.0576   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16599     |\n",
      "|    policy_loss        | -0.000661 |\n",
      "|    value_loss         | 0.649     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.58     |\n",
      "|    ep_rew_mean        | -0.02    |\n",
      "| time/                 |          |\n",
      "|    fps                | 376      |\n",
      "|    iterations         | 16700    |\n",
      "|    time_elapsed       | 221      |\n",
      "|    total_timesteps    | 83500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0666  |\n",
      "|    explained_variance | -0.0641  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16699    |\n",
      "|    policy_loss        | 0.0188   |\n",
      "|    value_loss         | 0.912    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.69      |\n",
      "|    ep_rew_mean        | -0.06     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 16800     |\n",
      "|    time_elapsed       | 222       |\n",
      "|    total_timesteps    | 84000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00252  |\n",
      "|    explained_variance | 0.8       |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16799     |\n",
      "|    policy_loss        | -0.000125 |\n",
      "|    value_loss         | 0.14      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.44      |\n",
      "|    ep_rew_mean        | -0.1      |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 16900     |\n",
      "|    time_elapsed       | 223       |\n",
      "|    total_timesteps    | 84500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00163  |\n",
      "|    explained_variance | -0.0818   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16899     |\n",
      "|    policy_loss        | -0.000115 |\n",
      "|    value_loss         | 0.714     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.47      |\n",
      "|    ep_rew_mean        | -0.07     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 17000     |\n",
      "|    time_elapsed       | 225       |\n",
      "|    total_timesteps    | 85000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -5.03e-05 |\n",
      "|    explained_variance | 0.396     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16999     |\n",
      "|    policy_loss        | -2.44e-06 |\n",
      "|    value_loss         | 0.564     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.61      |\n",
      "|    ep_rew_mean        | -0.09     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 17100     |\n",
      "|    time_elapsed       | 226       |\n",
      "|    total_timesteps    | 85500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000209 |\n",
      "|    explained_variance | 0.394     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17099     |\n",
      "|    policy_loss        | -2.03e-07 |\n",
      "|    value_loss         | 0.596     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.7      |\n",
      "|    ep_rew_mean        | -0.13    |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 17200    |\n",
      "|    time_elapsed       | 227      |\n",
      "|    total_timesteps    | 86000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0964  |\n",
      "|    explained_variance | -0.23    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17199    |\n",
      "|    policy_loss        | 0.0325   |\n",
      "|    value_loss         | 1.16     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.52     |\n",
      "|    ep_rew_mean        | -0.12    |\n",
      "| time/                 |          |\n",
      "|    fps                | 377      |\n",
      "|    iterations         | 17300    |\n",
      "|    time_elapsed       | 228      |\n",
      "|    total_timesteps    | 86500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0939  |\n",
      "|    explained_variance | 0.651    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17299    |\n",
      "|    policy_loss        | -0.0126  |\n",
      "|    value_loss         | 0.256    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.53      |\n",
      "|    ep_rew_mean        | -0.04     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 17400     |\n",
      "|    time_elapsed       | 230       |\n",
      "|    total_timesteps    | 87000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000261 |\n",
      "|    explained_variance | 0.848     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17399     |\n",
      "|    policy_loss        | -1.07e-05 |\n",
      "|    value_loss         | 0.174     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.63      |\n",
      "|    ep_rew_mean        | -0.09     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 17500     |\n",
      "|    time_elapsed       | 231       |\n",
      "|    total_timesteps    | 87500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00911  |\n",
      "|    explained_variance | 0.207     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17499     |\n",
      "|    policy_loss        | -0.000562 |\n",
      "|    value_loss         | 0.658     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.43     |\n",
      "|    ep_rew_mean        | -0.03    |\n",
      "| time/                 |          |\n",
      "|    fps                | 377      |\n",
      "|    iterations         | 17600    |\n",
      "|    time_elapsed       | 232      |\n",
      "|    total_timesteps    | 88000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.108   |\n",
      "|    explained_variance | 0.59     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17599    |\n",
      "|    policy_loss        | -0.175   |\n",
      "|    value_loss         | 0.0827   |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.54      |\n",
      "|    ep_rew_mean        | -0.12     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 17700     |\n",
      "|    time_elapsed       | 234       |\n",
      "|    total_timesteps    | 88500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00402  |\n",
      "|    explained_variance | 0.487     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17699     |\n",
      "|    policy_loss        | -0.000106 |\n",
      "|    value_loss         | 0.293     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.47     |\n",
      "|    ep_rew_mean        | -0.21    |\n",
      "| time/                 |          |\n",
      "|    fps                | 378      |\n",
      "|    iterations         | 17800    |\n",
      "|    time_elapsed       | 235      |\n",
      "|    total_timesteps    | 89000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0946  |\n",
      "|    explained_variance | 0.919    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17799    |\n",
      "|    policy_loss        | -0.015   |\n",
      "|    value_loss         | 0.117    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.65      |\n",
      "|    ep_rew_mean        | 0.01      |\n",
      "| time/                 |           |\n",
      "|    fps                | 378       |\n",
      "|    iterations         | 17900     |\n",
      "|    time_elapsed       | 236       |\n",
      "|    total_timesteps    | 89500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.51e-05 |\n",
      "|    explained_variance | 0.0103    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17899     |\n",
      "|    policy_loss        | -1.48e-07 |\n",
      "|    value_loss         | 1.01      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.58      |\n",
      "|    ep_rew_mean        | -0.09     |\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 18000     |\n",
      "|    time_elapsed       | 238       |\n",
      "|    total_timesteps    | 90000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000155 |\n",
      "|    explained_variance | 0.176     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17999     |\n",
      "|    policy_loss        | -1.88e-05 |\n",
      "|    value_loss         | 0.703     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.59      |\n",
      "|    ep_rew_mean        | 0.02      |\n",
      "| time/                 |           |\n",
      "|    fps                | 375       |\n",
      "|    iterations         | 18100     |\n",
      "|    time_elapsed       | 240       |\n",
      "|    total_timesteps    | 90500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.57e-05 |\n",
      "|    explained_variance | -3.3e+03  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18099     |\n",
      "|    policy_loss        | -5.4e-06  |\n",
      "|    value_loss         | 0.535     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.51     |\n",
      "|    ep_rew_mean        | -0.25    |\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 18200    |\n",
      "|    time_elapsed       | 242      |\n",
      "|    total_timesteps    | 91000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.229   |\n",
      "|    explained_variance | -130     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18199    |\n",
      "|    policy_loss        | -0.0444  |\n",
      "|    value_loss         | 0.173    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.49      |\n",
      "|    ep_rew_mean        | 0.02      |\n",
      "| time/                 |           |\n",
      "|    fps                | 373       |\n",
      "|    iterations         | 18300     |\n",
      "|    time_elapsed       | 244       |\n",
      "|    total_timesteps    | 91500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000124 |\n",
      "|    explained_variance | 0.459     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18299     |\n",
      "|    policy_loss        | 4.19e-06  |\n",
      "|    value_loss         | 0.937     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.49     |\n",
      "|    ep_rew_mean        | -0.06    |\n",
      "| time/                 |          |\n",
      "|    fps                | 372      |\n",
      "|    iterations         | 18400    |\n",
      "|    time_elapsed       | 246      |\n",
      "|    total_timesteps    | 92000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.106   |\n",
      "|    explained_variance | -0.491   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18399    |\n",
      "|    policy_loss        | 0.319    |\n",
      "|    value_loss         | 1.65     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.56      |\n",
      "|    ep_rew_mean        | -0.23     |\n",
      "| time/                 |           |\n",
      "|    fps                | 372       |\n",
      "|    iterations         | 18500     |\n",
      "|    time_elapsed       | 248       |\n",
      "|    total_timesteps    | 92500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00101  |\n",
      "|    explained_variance | 0.853     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18499     |\n",
      "|    policy_loss        | -6.37e-05 |\n",
      "|    value_loss         | 0.18      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.56     |\n",
      "|    ep_rew_mean        | -0.02    |\n",
      "| time/                 |          |\n",
      "|    fps                | 373      |\n",
      "|    iterations         | 18600    |\n",
      "|    time_elapsed       | 249      |\n",
      "|    total_timesteps    | 93000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0106  |\n",
      "|    explained_variance | 0.46     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18599    |\n",
      "|    policy_loss        | 0.00114  |\n",
      "|    value_loss         | 0.467    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.58      |\n",
      "|    ep_rew_mean        | -0.16     |\n",
      "| time/                 |           |\n",
      "|    fps                | 373       |\n",
      "|    iterations         | 18700     |\n",
      "|    time_elapsed       | 250       |\n",
      "|    total_timesteps    | 93500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000498 |\n",
      "|    explained_variance | 0.433     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18699     |\n",
      "|    policy_loss        | -9.6e-06  |\n",
      "|    value_loss         | 0.637     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.35     |\n",
      "|    ep_rew_mean        | -0.29    |\n",
      "| time/                 |          |\n",
      "|    fps                | 373      |\n",
      "|    iterations         | 18800    |\n",
      "|    time_elapsed       | 251      |\n",
      "|    total_timesteps    | 94000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00594 |\n",
      "|    explained_variance | 0.167    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18799    |\n",
      "|    policy_loss        | 0.0012   |\n",
      "|    value_loss         | 0.804    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.5       |\n",
      "|    ep_rew_mean        | -0.01     |\n",
      "| time/                 |           |\n",
      "|    fps                | 373       |\n",
      "|    iterations         | 18900     |\n",
      "|    time_elapsed       | 252       |\n",
      "|    total_timesteps    | 94500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000341 |\n",
      "|    explained_variance | 0.305     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18899     |\n",
      "|    policy_loss        | 3.64e-05  |\n",
      "|    value_loss         | 0.732     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.51     |\n",
      "|    ep_rew_mean        | -0.23    |\n",
      "| time/                 |          |\n",
      "|    fps                | 373      |\n",
      "|    iterations         | 19000    |\n",
      "|    time_elapsed       | 254      |\n",
      "|    total_timesteps    | 95000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.132   |\n",
      "|    explained_variance | 0.295    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18999    |\n",
      "|    policy_loss        | 0.266    |\n",
      "|    value_loss         | 0.561    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.58      |\n",
      "|    ep_rew_mean        | -0.14     |\n",
      "| time/                 |           |\n",
      "|    fps                | 374       |\n",
      "|    iterations         | 19100     |\n",
      "|    time_elapsed       | 255       |\n",
      "|    total_timesteps    | 95500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000157 |\n",
      "|    explained_variance | -0.0784   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19099     |\n",
      "|    policy_loss        | -3.85e-06 |\n",
      "|    value_loss         | 0.6       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.46     |\n",
      "|    ep_rew_mean        | -0.08    |\n",
      "| time/                 |          |\n",
      "|    fps                | 374      |\n",
      "|    iterations         | 19200    |\n",
      "|    time_elapsed       | 256      |\n",
      "|    total_timesteps    | 96000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00135 |\n",
      "|    explained_variance | 0.261    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19199    |\n",
      "|    policy_loss        | 0.000116 |\n",
      "|    value_loss         | 0.198    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.6      |\n",
      "|    ep_rew_mean        | -0.05    |\n",
      "| time/                 |          |\n",
      "|    fps                | 373      |\n",
      "|    iterations         | 19300    |\n",
      "|    time_elapsed       | 258      |\n",
      "|    total_timesteps    | 96500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.261   |\n",
      "|    explained_variance | 0.556    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19299    |\n",
      "|    policy_loss        | 0.0584   |\n",
      "|    value_loss         | 0.316    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.65     |\n",
      "|    ep_rew_mean        | 0        |\n",
      "| time/                 |          |\n",
      "|    fps                | 373      |\n",
      "|    iterations         | 19400    |\n",
      "|    time_elapsed       | 259      |\n",
      "|    total_timesteps    | 97000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0195  |\n",
      "|    explained_variance | 0.0236   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19399    |\n",
      "|    policy_loss        | -0.00213 |\n",
      "|    value_loss         | 0.841    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.67     |\n",
      "|    ep_rew_mean        | -0.11    |\n",
      "| time/                 |          |\n",
      "|    fps                | 373      |\n",
      "|    iterations         | 19500    |\n",
      "|    time_elapsed       | 260      |\n",
      "|    total_timesteps    | 97500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0321  |\n",
      "|    explained_variance | 0.686    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19499    |\n",
      "|    policy_loss        | -0.0025  |\n",
      "|    value_loss         | 0.379    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.44      |\n",
      "|    ep_rew_mean        | -0.34     |\n",
      "| time/                 |           |\n",
      "|    fps                | 374       |\n",
      "|    iterations         | 19600     |\n",
      "|    time_elapsed       | 261       |\n",
      "|    total_timesteps    | 98000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0035   |\n",
      "|    explained_variance | 0.861     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19599     |\n",
      "|    policy_loss        | -0.000233 |\n",
      "|    value_loss         | 0.368     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.39     |\n",
      "|    ep_rew_mean        | -0.02    |\n",
      "| time/                 |          |\n",
      "|    fps                | 374      |\n",
      "|    iterations         | 19700    |\n",
      "|    time_elapsed       | 263      |\n",
      "|    total_timesteps    | 98500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0226  |\n",
      "|    explained_variance | 0.576    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19699    |\n",
      "|    policy_loss        | -0.00446 |\n",
      "|    value_loss         | 0.446    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.49      |\n",
      "|    ep_rew_mean        | -0.05     |\n",
      "| time/                 |           |\n",
      "|    fps                | 374       |\n",
      "|    iterations         | 19800     |\n",
      "|    time_elapsed       | 264       |\n",
      "|    total_timesteps    | 99000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000527 |\n",
      "|    explained_variance | -0.143    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19799     |\n",
      "|    policy_loss        | 7.99e-05  |\n",
      "|    value_loss         | 0.988     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 1.35     |\n",
      "|    ep_rew_mean        | -0.11    |\n",
      "| time/                 |          |\n",
      "|    fps                | 375      |\n",
      "|    iterations         | 19900    |\n",
      "|    time_elapsed       | 265      |\n",
      "|    total_timesteps    | 99500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00252 |\n",
      "|    explained_variance | 0.181    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19899    |\n",
      "|    policy_loss        | 0.000124 |\n",
      "|    value_loss         | 0.337    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.43      |\n",
      "|    ep_rew_mean        | 0.03      |\n",
      "| time/                 |           |\n",
      "|    fps                | 375       |\n",
      "|    iterations         | 20000     |\n",
      "|    time_elapsed       | 266       |\n",
      "|    total_timesteps    | 100000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00219  |\n",
      "|    explained_variance | 0.791     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19999     |\n",
      "|    policy_loss        | -0.000154 |\n",
      "|    value_loss         | 0.388     |\n",
      "-------------------------------------\n",
      "Promedio de recompensa despu√©s del entrenamiento: -0.1000\n",
      "Desviaci√≥n est√°ndar de recompensa: 0.9434\n"
     ]
    }
   ],
   "source": [
    "class FlattenObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(FlattenObservation, self).__init__(env)\n",
    "        self.observation_space = MultiDiscrete(np.array([32, 11, 2]))\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.array(observation).flatten()\n",
    "\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "env = FlattenObservation(env)\n",
    "\n",
    "vec_env = make_vec_env(lambda: env, n_envs=1)\n",
    "\n",
    "model = A2C(\"MlpPolicy\", vec_env, verbose=1) #se escoge A2C\n",
    "\n",
    "model.learn(total_timesteps=100000)\n",
    "\n",
    "model.save(\"a2c_blackjack_model\")\n",
    "\n",
    "episodes = 100\n",
    "rewards = []\n",
    "for _ in range(episodes):\n",
    "    obs = vec_env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, _ = vec_env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "print(f\"Promedio de recompensa despu√©s del entrenamiento: {np.mean(rewards):.4f}\")\n",
    "print(f\"Desviaci√≥n est√°ndar de recompensa: {np.std(rewards):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-bpdb8wZID1"
   },
   "source": [
    "#### **1.1.4 Evaluaci√≥n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.1.2 pero utilizando el modelo entrenado. ¬øC√≥mo es el performance de su agente? ¬øEs mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-d7d8GFf7F6",
    "outputId": "7ebbf515-5713-45d5-9403-f9c6045eeda6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de las recompensas: -0.1092\n",
      "Desviaci√≥n est√°ndar de las recompensas: 0.9483\n"
     ]
    }
   ],
   "source": [
    "episodes = 5000\n",
    "rewards = []\n",
    "\n",
    "for _ in range(episodes):\n",
    "    obs = vec_env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, _= vec_env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "print(f\"Promedio de las recompensas: {np.mean(rewards):.4f}\")\n",
    "print(f\"Desviaci√≥n est√°ndar de las recompensas: {np.std(rewards):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Es9c6d-8MTby"
   },
   "source": [
    "Como era de esperar, el promedio de recompensas aument√≥, pero lo que no era tan predecible es que la desviaci√≥n est√°ndar no disminuyera, por lo que se tiene que el modelo no alcanza una estrategia consistente en sus resultados. De este tenemos que hay mejores formas de jugar que al azar, pero no est√° claro si haya una forma de consistentemente ganar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO-EsAaPAYEm"
   },
   "source": [
    "#### **1.1.5 Estudio de acciones (0.2 puntos)**\n",
    "\n",
    "Genere una funci√≥n que reciba un estado y retorne la accion del agente. Luego, use esta funci√≥n para entregar la acci√≥n escogida frente a los siguientes escenarios:\n",
    "\n",
    "- Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene tiene un as\n",
    "- Suma de cartas del agente es 19, dealer muestra un 3, agente tiene tiene un as\n",
    "\n",
    "¬øSon coherentes sus acciones con las reglas del juego?\n",
    "\n",
    "Hint: ¬øA que clase de python pertenecen los estados? Pruebe a usar el m√©todo `.reset` para saberlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fh8XlGyzwtRp",
    "outputId": "ef953f5e-5d8f-4104-cacd-82281d68e908"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acci√≥n para el escenario 1: 1 (0: Stick, 1: Hit)\n",
      "Acci√≥n para el escenario 2: 0 (0: Stick, 1: Hit)\n"
     ]
    }
   ],
   "source": [
    "def get_action(agent, state):\n",
    "    \"\"\"\n",
    "    Funci√≥n para obtener la acci√≥n del agente dado un estado espec√≠fico.\n",
    "\n",
    "    Args:\n",
    "        agent: Modelo entrenado (e.g., A2C).\n",
    "        state: Estado del ambiente (tuple con la suma del jugador, carta del dealer y si tiene un as usable).\n",
    "\n",
    "    Returns:\n",
    "        Acci√≥n (0: Stick, 1: Hit).\n",
    "    \"\"\"\n",
    "    state = np.array(state).reshape(1, -1)\n",
    "    action, _ = agent.predict(state, deterministic=True)\n",
    "    return action[0]\n",
    "\n",
    "scenario_1 = (6, 7, 0)   # suma del agente: 6, dealer muestra: 7, no tiene as\n",
    "scenario_2 = (19, 3, 1)  # suma del agente: 19, dealer muestra: 3, tiene un as usable\n",
    "\n",
    "action_1 = get_action(model, scenario_1)\n",
    "action_2 = get_action(model, scenario_2)\n",
    "\n",
    "print(f\"Acci√≥n para el escenario 1: {action_1} (0: Stick, 1: Hit)\")\n",
    "print(f\"Acci√≥n para el escenario 2: {action_2} (0: Stick, 1: Hit)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EN1SoMCdMkP8"
   },
   "source": [
    "Parecen ser razonables, veamos cada acci√≥n:\n",
    "\n",
    "1) Estado: Agente tiene una suma baja (6), mientras que el dealer muestra un 7.\n",
    "En este caso lo esperado esq el agente deber√≠a pedir (hit, 1) para intentar mejorar su suma, ya que pararse (stick, 0) con 6 ser√≠a casi seguro una p√©rdida.\n",
    "\n",
    "2) Estado: Agente tiene 19 con un as usable, y el dealer muestra un 3.\n",
    "Se espera que el agente se quedara (stick, 0), ya que 19 es un puntaje fuerte y arriesgarse con un hit podr√≠a llevarlo a pasarse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEqCTqqroh03"
   },
   "source": [
    "### **1.2 LunarLander**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://i.redd.it/097t6tk29zf51.jpg\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la secci√≥n 2.1, en esta secci√≥n usted se encargar√° de implementar una gente de RL que pueda resolver el ambiente `LunarLander`.\n",
    "\n",
    "Comencemos preparando el ambiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nvQUyuZ_FtZ4"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\", continuous = True) # notar el par√°metro continuous = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBU4lGX3wpN6"
   },
   "source": [
    "Noten que se especifica el par√°metro `continuous = True`. ¬øQue implicancias tiene esto sobre el ambiente?\n",
    "\n",
    "Adem√°s, se le facilita la funci√≥n `export_gif` para el ejercicio 2.2.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "bRiWpSo9yfr9"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "def export_gif(model, n = 5):\n",
    "  '''\n",
    "  funci√≥n que exporta a gif el comportamiento del agente en n episodios\n",
    "  '''\n",
    "  images = []\n",
    "  for episode in range(n):\n",
    "    obs = model.env.reset()\n",
    "    img = model.env.render()\n",
    "    done = False\n",
    "    while not done:\n",
    "      images.append(img)\n",
    "      action, _ = model.predict(obs)\n",
    "      obs, reward, done, info = model.env.step(action)\n",
    "      img = model.env.render(mode=\"rgb_array\")\n",
    "\n",
    "  imageio.mimsave(\"agent_performance.gif\", [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sk5VJVppXh3N"
   },
   "source": [
    "#### **1.2.1 Descripci√≥n de MDP (0.2 puntos)**\n",
    "\n",
    "Entregue una breve descripci√≥n sobre el ambiente [LunarLander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) y su formulaci√≥n en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas. ¬øComo se distinguen las acciones de este ambiente en comparaci√≥n a `Blackjack`?\n",
    "\n",
    "Nota: recuerde que se especific√≥ el par√°metro `continuous = True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yb-u9LUE8O9a"
   },
   "source": [
    "`escriba su respuesta ac√°`\n",
    "\n",
    "El ambiente **LunarLander** modela el problema de optimizaci√≥n de trayectorias de un cohete para aterrizar en una plataforma. Est√° formulado como un MDP:\n",
    "\n",
    "- **Estados**:  \n",
    "  El espacio de observaci√≥n es continuo, representado por un vector de 8 dimensiones:  \n",
    "  1. Coordenadas `x` e `y` del cohete.  \n",
    "  2. Velocidades lineales en `x` e `y`.  \n",
    "  3. √Ångulo y velocidad angular.  \n",
    "  4. Dos booleanos indicando si las patas del cohete est√°n en contacto con el suelo.  \n",
    "\n",
    "- **Acciones**:  \n",
    "  Con el par√°metro `continuous=True`, el espacio de acciones es continuo y consiste en un vector de dos dimensiones:  \n",
    "  1. Intensidad del motor principal (valor entre -1 y 1).  \n",
    "  2. Intensidad de los propulsores laterales (valor entre -1 y 1).  \n",
    "\n",
    "  Las acciones determinan el uso de los motores:  \n",
    "  - Motor principal solo funciona con al menos el 50% de potencia.  \n",
    "  - Propulsores laterales no se activan entre valores de -0.5 y 0.5.  \n",
    "\n",
    "- **Recompensas**:  \n",
    "  Recompensas se otorgan seg√∫n:  \n",
    "  - Cercan√≠a del cohete a la plataforma.  \n",
    "  - Reducci√≥n de velocidad (lineal y angular).  \n",
    "  - Estabilidad en el √°ngulo.  \n",
    "  - Contacto de las patas con el suelo (+10 por cada pata).  \n",
    "  - Penalizaciones por el uso de motores (0.03 por cada frame para propulsores laterales y 0.3 para el motor principal).  \n",
    "  - Recompensas grandes (+100 por aterrizaje exitoso, -100 por accidente).  \n",
    "\n",
    "- **T√©rmino**:  \n",
    "  El episodio termina si:  \n",
    "  - El cohete se estrella.  \n",
    "  - Sale del √°rea visible.  \n",
    "  - Aterriza con √©xito y se estabiliza.  \n",
    "\n",
    "\n",
    "Notemos que a diferencia de BlackJack, en este caso se tiene que el espacio de estados es diferente ya que en el primer caso, el BlackJack tiene una combinaci√≥n finita de cartas y estados (i.e espacio discreto) mientras que LunarLander cuenta con un espacio continuo de 8 dimensiones. Por otro lado, el espacio de acciones es diferente en cada caso, en BlackJack el espacio de acciones es discreto y cuenta con dos posibilidades, pararse o pedir, LunarLander tiene un espacio de acciones continuo bidimensional para regular la potencia de los motores. Por √∫ltimo, las recompensas son distintas ya que en el BlackJack las recompensas se entregan √∫nicamente al final del episodio y en LunarLander se entregan acumulativamente, d√°ndose en cada paso del episodio seg√∫n el desempe√±o."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YChodtNQwzG2"
   },
   "source": [
    "#### **1.2.2 Generando un Baseline (0.2 puntos)**\n",
    "\n",
    "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulaci√≥n 10 veces y reporte el promedio y desviaci√≥n de las recompensas. ¬øC√≥mo calificar√≠a el performance de esta pol√≠tica?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5bwc3A0GX7a8",
    "outputId": "d0ffd971-2be7-4725-9eb2-43a3ebbbc92b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensas: -240.96\n",
      "Desviaci√≥n est√°ndar de recompensas: 99.86\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10\n",
    "rewards = []\n",
    "\n",
    "# con acciones aleatorias\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # acci√≥n al azar\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "print(f\"Promedio de recompensas: {np.mean(rewards):.2f}\")\n",
    "print(f\"Desviaci√≥n est√°ndar de recompensas: {np.std(rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyySYhUjNrwn"
   },
   "source": [
    "Esta estrategia es muy mala, la recompensa promedio negativa indica que en general se llega a acciodentes o que se hizo muchas cosas negativas para lo buscado, el cohete en la mayor√≠a de casos no aterriza y en general la recompensa final es negativa, ya que pese a que la desviaci√≥n est√°ndar es alta, no es lo suficiente como para que esta estrategia se acerque consistentemente a resultados de recompensa positiva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQrZVQflX_5f"
   },
   "source": [
    "#### **1.2.3 Entrenamiento de modelo (0.2 puntos)**\n",
    "\n",
    "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `LunarLander` **usando 10000 timesteps de entrenamiento**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_6Ia9uoF7Hs",
    "outputId": "89238d00-28dc-4b33-f975-b5e4f709b4bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 115      |\n",
      "|    ep_rew_mean        | -219     |\n",
      "| time/                 |          |\n",
      "|    fps                | 219      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.85    |\n",
      "|    explained_variance | -0.00148 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 26.8     |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 80.4     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 145      |\n",
      "|    ep_rew_mean        | -342     |\n",
      "| time/                 |          |\n",
      "|    fps                | 297      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.85    |\n",
      "|    explained_variance | 0.00867  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -20.2    |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 65.5     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 143      |\n",
      "|    ep_rew_mean        | -384     |\n",
      "| time/                 |          |\n",
      "|    fps                | 339      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.83    |\n",
      "|    explained_variance | 0.00136  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -21.9    |\n",
      "|    std                | 0.996    |\n",
      "|    value_loss         | 115      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 144      |\n",
      "|    ep_rew_mean        | -383     |\n",
      "| time/                 |          |\n",
      "|    fps                | 351      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.83    |\n",
      "|    explained_variance | 0.000435 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -27.2    |\n",
      "|    std                | 0.998    |\n",
      "|    value_loss         | 107      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 146      |\n",
      "|    ep_rew_mean        | -410     |\n",
      "| time/                 |          |\n",
      "|    fps                | 350      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.82    |\n",
      "|    explained_variance | -0.0016  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -9.97    |\n",
      "|    std                | 0.993    |\n",
      "|    value_loss         | 15.3     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 144      |\n",
      "|    ep_rew_mean        | -415     |\n",
      "| time/                 |          |\n",
      "|    fps                | 360      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.82    |\n",
      "|    explained_variance | -0.0192  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -1.51    |\n",
      "|    std                | 0.991    |\n",
      "|    value_loss         | 1.87     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 144       |\n",
      "|    ep_rew_mean        | -419      |\n",
      "| time/                 |           |\n",
      "|    fps                | 373       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.82     |\n",
      "|    explained_variance | -0.000148 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 6.15      |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 5.2       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 143      |\n",
      "|    ep_rew_mean        | -397     |\n",
      "| time/                 |          |\n",
      "|    fps                | 382      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.82    |\n",
      "|    explained_variance | -0.00194 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -109     |\n",
      "|    std                | 0.99     |\n",
      "|    value_loss         | 3.28e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 143      |\n",
      "|    ep_rew_mean        | -411     |\n",
      "| time/                 |          |\n",
      "|    fps                | 390      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.83    |\n",
      "|    explained_variance | -1.9e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -73.1    |\n",
      "|    std                | 0.993    |\n",
      "|    value_loss         | 498      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 139      |\n",
      "|    ep_rew_mean        | -401     |\n",
      "| time/                 |          |\n",
      "|    fps                | 396      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.8     |\n",
      "|    explained_variance | -0.00243 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -78.8    |\n",
      "|    std                | 0.981    |\n",
      "|    value_loss         | 1.73e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 134      |\n",
      "|    ep_rew_mean        | -391     |\n",
      "| time/                 |          |\n",
      "|    fps                | 401      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.81    |\n",
      "|    explained_variance | 0.00116  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 9.63     |\n",
      "|    std                | 0.984    |\n",
      "|    value_loss         | 16.4     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 132      |\n",
      "|    ep_rew_mean        | -402     |\n",
      "| time/                 |          |\n",
      "|    fps                | 406      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.81    |\n",
      "|    explained_variance | 0.000318 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -9.03    |\n",
      "|    std                | 0.988    |\n",
      "|    value_loss         | 20.1     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 129       |\n",
      "|    ep_rew_mean        | -396      |\n",
      "| time/                 |           |\n",
      "|    fps                | 410       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.8      |\n",
      "|    explained_variance | -0.000118 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 5.19      |\n",
      "|    std                | 0.982     |\n",
      "|    value_loss         | 2.84      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 127       |\n",
      "|    ep_rew_mean        | -417      |\n",
      "| time/                 |           |\n",
      "|    fps                | 413       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.8      |\n",
      "|    explained_variance | -2.26e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -55.2     |\n",
      "|    std                | 0.983     |\n",
      "|    value_loss         | 458       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 123      |\n",
      "|    ep_rew_mean        | -460     |\n",
      "| time/                 |          |\n",
      "|    fps                | 413      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.8     |\n",
      "|    explained_variance | 0.000726 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -7.77    |\n",
      "|    std                | 0.981    |\n",
      "|    value_loss         | 13.4     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 120       |\n",
      "|    ep_rew_mean        | -475      |\n",
      "| time/                 |           |\n",
      "|    fps                | 408       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.82     |\n",
      "|    explained_variance | -2.98e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -18.2     |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 130       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 119       |\n",
      "|    ep_rew_mean        | -477      |\n",
      "| time/                 |           |\n",
      "|    fps                | 409       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.8      |\n",
      "|    explained_variance | -2.05e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 14.6      |\n",
      "|    std                | 0.983     |\n",
      "|    value_loss         | 38.1      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 119      |\n",
      "|    ep_rew_mean        | -472     |\n",
      "| time/                 |          |\n",
      "|    fps                | 411      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.8     |\n",
      "|    explained_variance | 1.19e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 30.2     |\n",
      "|    std                | 0.979    |\n",
      "|    value_loss         | 228      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 118      |\n",
      "|    ep_rew_mean        | -463     |\n",
      "| time/                 |          |\n",
      "|    fps                | 414      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 22       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.78    |\n",
      "|    explained_variance | -0.00017 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -1.02    |\n",
      "|    std                | 0.973    |\n",
      "|    value_loss         | 0.359    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 117      |\n",
      "|    ep_rew_mean        | -466     |\n",
      "| time/                 |          |\n",
      "|    fps                | 416      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 24       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.81    |\n",
      "|    explained_variance | 0.000282 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 3.07     |\n",
      "|    std                | 0.984    |\n",
      "|    value_loss         | 1.99     |\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v3\", continuous=True)\n",
    "\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Guardar el modelo entrenado\n",
    "model.save(\"a2c_lunar_lander\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3z-oIUSrlAsY"
   },
   "source": [
    "#### **1.2.4 Evaluaci√≥n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita el ejercicio 1.2.2 pero utilizando el modelo entrenado. ¬øC√≥mo es el performance de su agente? ¬øEs mejor o peor que el escenario baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ophyU3KrWrwl",
    "outputId": "65e694f8-b4aa-4d64-e19d-9630901da2cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensas: -448.96739943018366\n",
      "Desviaci√≥n est√°ndar de recompensas: 123.7272590595172\n"
     ]
    }
   ],
   "source": [
    "model = A2C.load(\"a2c_lunar_lander\")\n",
    "\n",
    "rewards = []\n",
    "for episode in range(10):\n",
    "    obs = env.reset()[0]\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "    rewards.append(episode_reward)\n",
    "\n",
    "print(f\"Promedio de recompensas: {np.mean(rewards)}\")\n",
    "print(f\"Desviaci√≥n est√°ndar de recompensas: {np.std(rewards)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXTbkSgiN9z2"
   },
   "source": [
    "Dada la naturaleza del problema, lo esperable es que mejoraran los resultados ya que se puede pensar que existen estrategias ganadoras, y en consecuencia, que el modelo sea capaz de encontrar alguna, esto quiz√°s se pueda alcanzar con un alto n√∫mero de iteraciones pero tambi√©n aumenta mucho el tiempo de c√≥mputo, por lo que al quedarnos con un modelo de 10000 iteraciones, se obtiene resultados similares a escoger las acciones al azar, de hecho, con este modelo la desviaci√≥n est√°ndar aumenta, lo que hace que sea peor que el baseline, ya que la estrategia obtenida no muestra resultados consistentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6Xw4YHT3P5d"
   },
   "source": [
    "#### **1.2.5 Optimizaci√≥n de modelo (0.2 puntos)**\n",
    "\n",
    "Repita los ejercicios 1.2.3 y 1.2.4 hasta obtener un nivel de recompensas promedio mayor a 50. Para esto, puede cambiar manualmente par√°metros como:\n",
    "- `total_timesteps`\n",
    "- `learning_rate`\n",
    "- `batch_size`\n",
    "\n",
    "Una vez optimizado el modelo, use la funci√≥n `export_gif` para estudiar el comportamiento de su agente en la resoluci√≥n del ambiente y comente sobre sus resultados.\n",
    "\n",
    "Adjunte el gif generado en su entrega (mejor a√∫n si adem√°s adjuntan el gif en el markdown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TBi5hNcnbyrH",
    "outputId": "abb88ce6-03d3-4466-a3cc-5a487224565e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 88.8     |\n",
      "|    ep_rew_mean        | -397     |\n",
      "| time/                 |          |\n",
      "|    fps                | 358      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.01    |\n",
      "|    explained_variance | 0.074    |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.911    |\n",
      "|    value_loss         | 5.07     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 124      |\n",
      "|    ep_rew_mean        | -496     |\n",
      "| time/                 |          |\n",
      "|    fps                | 314      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00851 |\n",
      "|    explained_variance | 0.00626  |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0.0259  |\n",
      "|    value_loss         | 773      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 120      |\n",
      "|    ep_rew_mean        | -534     |\n",
      "| time/                 |          |\n",
      "|    fps                | 329      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.974   |\n",
      "|    explained_variance | -0.00178 |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0.326   |\n",
      "|    value_loss         | 1.19     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 127      |\n",
      "|    ep_rew_mean        | -550     |\n",
      "| time/                 |          |\n",
      "|    fps                | 338      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.517   |\n",
      "|    explained_variance | -17.4    |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -17.3    |\n",
      "|    value_loss         | 5.64e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 165      |\n",
      "|    ep_rew_mean        | -510     |\n",
      "| time/                 |          |\n",
      "|    fps                | 336      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.484   |\n",
      "|    explained_variance | -0.00135 |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.62     |\n",
      "|    value_loss         | 25.3     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 161      |\n",
      "|    ep_rew_mean        | -471     |\n",
      "| time/                 |          |\n",
      "|    fps                | 341      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.652   |\n",
      "|    explained_variance | -0.01    |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.365   |\n",
      "|    value_loss         | 0.432    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 168      |\n",
      "|    ep_rew_mean        | -444     |\n",
      "| time/                 |          |\n",
      "|    fps                | 339      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.495   |\n",
      "|    explained_variance | -0.0247  |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 3.44     |\n",
      "|    value_loss         | 54       |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 190      |\n",
      "|    ep_rew_mean        | -413     |\n",
      "| time/                 |          |\n",
      "|    fps                | 341      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00239 |\n",
      "|    explained_variance | -0.0243  |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.0215  |\n",
      "|    value_loss         | 1.19e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 183      |\n",
      "|    ep_rew_mean        | -388     |\n",
      "| time/                 |          |\n",
      "|    fps                | 346      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.94    |\n",
      "|    explained_variance | -0.187   |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -0.549   |\n",
      "|    value_loss         | 0.948    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 196      |\n",
      "|    ep_rew_mean        | -384     |\n",
      "| time/                 |          |\n",
      "|    fps                | 336      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.798   |\n",
      "|    explained_variance | 0.393    |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 1.14     |\n",
      "|    value_loss         | 3.9      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 206      |\n",
      "|    ep_rew_mean        | -384     |\n",
      "| time/                 |          |\n",
      "|    fps                | 327      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0142  |\n",
      "|    explained_variance | -0.00476 |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.0143   |\n",
      "|    value_loss         | 127      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 203      |\n",
      "|    ep_rew_mean        | -388     |\n",
      "| time/                 |          |\n",
      "|    fps                | 331      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0385  |\n",
      "|    explained_variance | -0.167   |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.204   |\n",
      "|    value_loss         | 904      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 197      |\n",
      "|    ep_rew_mean        | -394     |\n",
      "| time/                 |          |\n",
      "|    fps                | 335      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 19       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.225   |\n",
      "|    explained_variance | -0.709   |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -1.28    |\n",
      "|    value_loss         | 40.6     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 204      |\n",
      "|    ep_rew_mean        | -382     |\n",
      "| time/                 |          |\n",
      "|    fps                | 336      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.505   |\n",
      "|    explained_variance | 0.0178   |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -0.664   |\n",
      "|    value_loss         | 4.82     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 201      |\n",
      "|    ep_rew_mean        | -374     |\n",
      "| time/                 |          |\n",
      "|    fps                | 335      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 22       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.361   |\n",
      "|    explained_variance | 0.828    |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.553    |\n",
      "|    value_loss         | 3.88     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 215      |\n",
      "|    ep_rew_mean        | -361     |\n",
      "| time/                 |          |\n",
      "|    fps                | 335      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 23       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.326   |\n",
      "|    explained_variance | -0.284   |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.81     |\n",
      "|    value_loss         | 15.5     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 218      |\n",
      "|    ep_rew_mean        | -357     |\n",
      "| time/                 |          |\n",
      "|    fps                | 336      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 25       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.396   |\n",
      "|    explained_variance | -1.27    |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -2.01    |\n",
      "|    value_loss         | 9.86     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 217      |\n",
      "|    ep_rew_mean        | -335     |\n",
      "| time/                 |          |\n",
      "|    fps                | 337      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 26       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.403   |\n",
      "|    explained_variance | 0.615    |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -0.542   |\n",
      "|    value_loss         | 4.82     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 215      |\n",
      "|    ep_rew_mean        | -315     |\n",
      "| time/                 |          |\n",
      "|    fps                | 333      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 28       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0149  |\n",
      "|    explained_variance | -0.522   |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -0.0138  |\n",
      "|    value_loss         | 100      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 215      |\n",
      "|    ep_rew_mean        | -315     |\n",
      "| time/                 |          |\n",
      "|    fps                | 330      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 30       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.407   |\n",
      "|    explained_variance | 0.867    |\n",
      "|    learning_rate      | 0.005    |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -1.53    |\n",
      "|    value_loss         | 1.85     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x7b0ad8ba0ac0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "env = make_vec_env(\"LunarLander-v3\", n_envs=1)\n",
    "\n",
    "model1 = A2C(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=0.005,\n",
    "    verbose=1,\n",
    ")\n",
    "model1.learn(total_timesteps=10000)  # aumentar timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sspxKWszORfF",
    "outputId": "7cf66911-e158-403f-a040-a51ef60e0b29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio de recompensas optimizado: 54.099287352489775\n",
      "Desviaci√≥n est√°ndar optimizado: 106.20646700784887\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "for episode in range(10):\n",
    "    obs = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = model1.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward[0]\n",
    "        done = done[0]\n",
    "    rewards.append(episode_reward)\n",
    "\n",
    "print(f\"Promedio de recompensas optimizado: {np.mean(rewards)}\")\n",
    "print(f\"Desviaci√≥n est√°ndar optimizado: {np.std(rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Accm6jAIcirX"
   },
   "outputs": [],
   "source": [
    "export_gif(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPUY-Ktgf2BO"
   },
   "source": [
    "## **2. Large Language Models (4.0 puntos)**\n",
    "\n",
    "En esta secci√≥n se enfocar√°n en habilitar un Chatbot que nos permita responder preguntas √∫tiles a trav√©s de LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQ4fPRRihGLe"
   },
   "source": [
    "### **2.0 Configuraci√≥n Inicial**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/uqAs9atZH58AAAAd/config-config-issue.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Como siempre, cargamos todas nuestras API KEY al entorno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ud2Xm_k-hFJn"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
    "\n",
    "if \"TAVILY_API_KEY\" not in os.environ:\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIbdy9arK40P",
    "outputId": "0b045509-84b1-48b9-b810-01434974b8e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), temperature=0.0, max_retries=2, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000002025B731F40>, default_metadata=())"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rj9JvQUsgZZJ"
   },
   "source": [
    "### **2.1 Retrieval Augmented Generation (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://y.yarn.co/218aaa02-c47e-4ec9-b1c9-07792a06a88f_text.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsecci√≥n es que habiliten un chatbot que pueda responder preguntas usando informaci√≥n contenida en documentos PDF a trav√©s de **Retrieval Augmented Generation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrxOQroVnaZ5"
   },
   "source": [
    "#### **2.1.1 Reunir Documentos (0 puntos)**\n",
    "\n",
    "Reuna documentos PDF sobre los que hacer preguntas siguiendo las siguientes instrucciones:\n",
    "  - 2 documentos .pdf como m√≠nimo.\n",
    "  - 50 p√°ginas de contenido como m√≠nimo entre todos los documentos.\n",
    "  - Ideas para documentos: Documentos relacionados a temas acad√©micos, laborales o de ocio. Aprovechen este ejercicio para construir algo √∫til y/o relevante para ustedes!\n",
    "  - Deben ocupar documentos reales, no pueden utilizar los mismos de la clase.\n",
    "  - Deben registrar sus documentos en la siguiente [planilla](https://docs.google.com/spreadsheets/d/1Hy1w_dOiG2UCHJ8muyxhdKPZEPrrL7BNHm6E90imIIM/edit?usp=sharing). **NO PUEDEN USAR LOS MISMOS DOCUMENTOS QUE OTRO GRUPO**\n",
    "  - **Recuerden adjuntar los documentos en su entrega**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5D1tIRCi4oJJ",
    "outputId": "39f6d4fc-63cb-4b9b-d48f-48d60df25ee5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzq2TjWCnu15"
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "doc_paths = ['AttentionIsAllYouNeed.pdf', 'BERT.pdf', 'ChainOfThought.pdf'] # rellenar con los path a sus documentos\n",
    "\n",
    "assert len(doc_paths) >= 2, \"Deben adjuntar un m√≠nimo de 2 documentos\"\n",
    "\n",
    "total_paginas = sum(len(PyPDF2.PdfReader(open(doc, \"rb\")).pages) for doc in doc_paths)\n",
    "assert total_paginas >= 50, f\"P√°ginas insuficientes: {total_paginas}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yymp25NBK40U",
    "outputId": "52733d58-ba66-4e74-df8b-f577bce42c51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani‚àó\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer‚àó\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar‚àó\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit‚àó\\nGoogle Research\\nusz@google.com\\nLlion Jones‚àó\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez‚àó ‚Ä†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\n≈Åukasz Kaiser‚àó\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin‚àó ‚Ä°\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n‚àóEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n‚Ä†Work performed while at Google Brain.\\n‚Ä°Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 1}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht‚àí1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 2}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 3}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by ‚àödk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n‚àödk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1‚àödk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1‚àödk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q ¬∑ k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 4}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ‚àà Rdmodel√ódk , WK\\ni ‚àà Rdmodel√ódk , WV\\ni ‚àà Rdmodel√ódv\\nand WO ‚àà Rhdv√ódmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n‚Ä¢ In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n‚Ä¢ The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n‚Ä¢ Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to ‚àí‚àû) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by ‚àödmodel.\\n5')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "docs = []\n",
    "for path in doc_paths:\n",
    "    loader = PyPDFLoader(path)\n",
    "    docs += loader.load()\n",
    "\n",
    "docs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r811-P71nizA"
   },
   "source": [
    "#### **2.1.2 Vectorizar Documentos (0.2 puntos)**\n",
    "\n",
    "Vectorice los documentos y almacene sus representaciones de manera acorde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-yXAdCSn4JM",
    "outputId": "5a502aa7-353f-4b77-f8fe-bbe79568648b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani‚àó\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer‚àó\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar‚àó\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit‚àó\\nGoogle Research\\nusz@google.com\\nLlion Jones‚àó\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez‚àó ‚Ä†\\nUniversity of Toronto\\naidan@cs.toronto.edu'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 0}, page_content='University of Toronto\\naidan@cs.toronto.edu\\n≈Åukasz Kaiser‚àó\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin‚àó ‚Ä°\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 0}, page_content='based solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 0}, page_content='our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n‚àóEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 0}, page_content='the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "splits[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EEXGEMD4K40W",
    "outputId": "aee57976-ed37-410c-d5c2-cdef9ff01934"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x202b6690140>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embedding)\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAUkP5zrnyBK"
   },
   "source": [
    "#### **2.1.3 Habilitar RAG (0.3 puntos)**\n",
    "\n",
    "Habilite la soluci√≥n RAG a trav√©s de una *chain* y gu√°rdela en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPIySdDFn99l"
   },
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3},\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "retriever_chain = retriever | format_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycg5S5i_n-kL"
   },
   "source": [
    "#### **2.1.4 Verificaci√≥n de respuestas (0.5 puntos)**\n",
    "\n",
    "Genere un listado de 3 tuplas (\"pregunta\", \"respuesta correcta\") y analice la respuesta de su soluci√≥n para cada una. ¬øSu soluci√≥n RAG entrega las respuestas que esperaba?\n",
    "\n",
    "Ejemplo de tupla:\n",
    "- Pregunta: ¬øQui√©n es el presidente de Chile?\n",
    "- Respuesta correcta: El presidente de Chile es Gabriel Boric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r8LLuBxQK40Y"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# noten como ahora existe el par√°metro de context!\n",
    "rag_template = '''\n",
    "Eres un asistente experto en investigaci√≥n sobre LLMs.\n",
    "Tu √∫nico rol es contestar preguntas del usuario a partir de informaci√≥n relevante que te sea proporcionada.\n",
    "Responde siempre de la forma m√°s completa posible y usando toda la informaci√≥n entregada.\n",
    "Responde s√≥lo lo que te pregunten a partir de la informaci√≥n relevante, NUNCA inventes una respuesta.\n",
    "\n",
    "Informaci√≥n relevante: {context}\n",
    "Pregunta: {question}\n",
    "Respuesta:\n",
    "'''\n",
    "\n",
    "rag_prompt = PromptTemplate.from_template(rag_template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever_chain,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    } # Los par√°metros de la plantilla\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_UiEn1hoZYR",
    "outputId": "4006d104-ecaa-4ecf-e0cd-477024a3bddd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta esperada:  Un Transformer es un tipo de arquitectura de red neuronal que usa un encoder-decoder para modelar relaciones en datos secuenciales.\n",
      "Respuesta obtenita:  Un Transformer es un modelo de transducci√≥n que utiliza √∫nicamente auto-atenci√≥n para calcular representaciones de su entrada y salida, sin usar RNNs alineados con la secuencia o convoluciones.  Su arquitectura se compone de un codificador y un decodificador, ambos formados por una pila de N=6 capas id√©nticas. Cada capa tiene dos subcapas: un mecanismo de auto-atenci√≥n multi-cabeza y una capa de conexi√≥n completamente conectada posicional.  El modelo utiliza codificaciones posicionales tanto en el codificador como en el decodificador, y un dropout de Pdrop = 0.1 en el modelo base. Durante el entrenamiento, se emple√≥ suavizado de etiquetas con un valor de œµls = 0.1.\n",
      "\n",
      "\n",
      "Respuesta esperada:  Se puede descomponer el problema en partes m√°s peque√±as y resolver cada parte por separado.\n",
      "Respuesta obtenita:  Seg√∫n la informaci√≥n proporcionada, la t√©cnica Chain of Thought (pensamiento en cadena) se puede usar para resolver problemas complejos con un LLM.  Esta t√©cnica permite descomponer problemas de m√∫ltiples pasos en subproblemas m√°s peque√±os y manejables,  imitando un proceso de pensamiento paso a paso para llegar a la respuesta.  Adem√°s,  los modelos de lenguaje pueden generar cadenas de pensamiento si se proporcionan demostraciones de razonamiento de pensamiento en cadena en los ejemplos para la indicaci√≥n de pocos disparos (few-shot prompting).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question_answer_list = [\n",
    "    (\n",
    "        \"Qu√© es un Transformer?\",\n",
    "        \"Un Transformer es un tipo de arquitectura de red neuronal que usa un encoder-decoder para modelar relaciones en datos secuenciales.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Seg√∫n Chain of Thought, ¬øqu√© t√©cnica se puede usar para resolver un problema complejo con un LLM?\",\n",
    "        \"Se puede descomponer el problema en partes m√°s peque√±as y resolver cada parte por separado.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "for question_answer in question_answer_list:\n",
    "    res = rag_chain.invoke(question_answer[0])\n",
    "    print(\"Respuesta esperada: \", question_answer[1])\n",
    "    print(\"Respuesta obtenita: \", res)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzhKlq6DK40a"
   },
   "source": [
    "**RESPUESTA**: La soluci√≥n de RAG entrega respuestas correctas, y bastante parecidas a las esperadas. Al ser preguntas no simples, era dif√≠cil que la respuesta fuera exactamente igual, pero en general, la respuesta entregada por RAG es correcta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8d5zTMHoUgF"
   },
   "source": [
    "#### **2.1.5 Sensibilidad de Hiperpar√°metros (0.5 puntos)**\n",
    "\n",
    "Extienda el an√°lisis del punto 2.1.4 analizando c√≥mo cambian las respuestas entregadas cambiando los siguientes hiperpar√°metros:\n",
    "- `Tama√±o del chunk`. (*¬øC√≥mo repercute que los chunks sean mas grandes o chicos?*)\n",
    "- `La cantidad de chunks recuperados`. (*¬øQu√© pasa si se devuelven muchos/pocos chunks?*)\n",
    "- `El tipo de b√∫squeda`. (*¬øC√≥mo afecta el tipo de b√∫squeda a las respuestas de mi RAG?*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDh_QgeXLGHc"
   },
   "outputs": [],
   "source": [
    "def try_other_config(\n",
    "    chunk_size: int = 500,\n",
    "    ammount_of_chunks: int = 3,\n",
    "    search_type: str = 'similarity'\n",
    "):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=50)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\") # inicializamos los embeddings\n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=embedding) # vectorizacion y almacenamiento\n",
    "\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=search_type, # m√©todo de b√∫squeda\n",
    "        search_kwargs={\"k\": ammount_of_chunks}, # n¬∞ documentos a recuperar\n",
    "    )\n",
    "\n",
    "    retriever_chain = retriever | format_docs\n",
    "\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": retriever_chain,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | rag_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    for question_answer in question_answer_list:\n",
    "        res = rag_chain.invoke(question_answer[0])\n",
    "        print(\"Respuesta esperada: \", question_answer[1])\n",
    "        print(\"Respuesta obtenita: \", res)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOBE4zrBK40b"
   },
   "source": [
    "\n",
    "**PROBANDO CAMBIAR TAMA√ëO DE CHUNK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0bwpPm07K40c",
    "outputId": "3cd64f73-ab5b-4bab-c49e-217753786cfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta esperada:  Un Transformer es un tipo de arquitectura de red neuronal que usa un encoder-decoder para modelar relaciones en datos secuenciales.\n",
      "Respuesta obtenita:  Basado en la informaci√≥n proporcionada, un Transformer es un modelo que puede ser usado para tareas de traducci√≥n.  Tambi√©n se menciona que existe una versi√≥n del Transformer referida como \"Transformer\".\n",
      "\n",
      "\n",
      "Respuesta esperada:  Se puede descomponer el problema en partes m√°s peque√±as y resolver cada parte por separado.\n",
      "Respuesta obtenita:  Seg√∫n la informaci√≥n proporcionada, la t√©cnica \"chain of thought\" es particularmente adecuada para resolver problemas complejos con un LLM.  El ejemplo dado muestra que un modelo que produce un \"chain of thought\" puede resolver un problema.  Sin embargo, la informaci√≥n no describe la t√©cnica en s√≠ misma, solo indica su idoneidad para este prop√≥sito.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_other_config(chunk_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqByizLRK40c"
   },
   "source": [
    "**RESPUESTA**: Al achicar el tama√±o de los chunks se obtienen respuestas especificas pero distintas a lo que buscabamos. La respuesta es incompleta y difiere de lo que se esperaba. Esto se debe a que al achicar el tama√±o de los chunks, se obtiene menos informaci√≥n y contexto para responder la pregunta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_SoU4TQK40d",
    "outputId": "d1b16df3-1c93-48b4-8056-b5b21b35066d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta esperada:  Un Transformer es un tipo de arquitectura de red neuronal que usa un encoder-decoder para modelar relaciones en datos secuenciales.\n",
      "Respuesta obtenita:  Un Transformer es una nueva arquitectura de red simple basada √∫nicamente en mecanismos de atenci√≥n, que prescinde por completo de la recurrencia y las convoluciones.  Est√° compuesta por un codificador y un decodificador, ambos con capas apiladas de auto-atenci√≥n y capas totalmente conectadas puntuales.  El codificador tiene una pila de N=6 capas id√©nticas, cada una con dos subcapas: un mecanismo de auto-atenci√≥n multi-cabeza y una red de avance totalmente conectada posicional simple.  Se utiliza una conexi√≥n residual alrededor de cada una de las dos subcapas, seguida de una normalizaci√≥n de capa.  Se aplica abandono (dropout) a la salida de cada subcapa, antes de que se agregue a la entrada de la subcapa y se normalice.  Tambi√©n se aplica abandono a las sumas de las incrustaciones y las codificaciones posicionales tanto en el codificador como en el decodificador.  Durante el entrenamiento, se emple√≥ un suavizado de etiquetas.\n",
      "\n",
      "\n",
      "Respuesta esperada:  Se puede descomponer el problema en partes m√°s peque√±as y resolver cada parte por separado.\n",
      "Respuesta obtenita:  Seg√∫n la informaci√≥n proporcionada, la t√©cnica para resolver un problema complejo con un LLM es el *Chain-of-Thought prompting*.  Este m√©todo consiste en proporcionar ejemplos de razonamiento paso a paso (cadenas de pensamiento) en los ejemplos para el prompting de pocos disparos.  Esto permite que los modelos de lenguaje generen sus propias cadenas de pensamiento para resolver el problema.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_other_config(chunk_size=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUsAwGlFK40d"
   },
   "source": [
    "**RESPUESTA**: Al aumentar el tama√±o de los chunks se obtiene informaci√≥n muchisimo m√°s completa. Al tener tanta informaci√≥n, y dejar que el LLM la procese, se obtiene una respuesta mucho m√°s completa y correcta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anUc1p2cK40e"
   },
   "source": [
    "**PROBANDO CAMBIAR CANTIDAD DE CHUNKS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ld6Ok-JNK40e",
    "outputId": "7fa1c77e-6d8b-40ce-814c-6ce50ca38ec9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta esperada:  Un Transformer es un tipo de arquitectura de red neuronal que usa un encoder-decoder para modelar relaciones en datos secuenciales.\n",
      "Respuesta obtenita:  Basado en la informaci√≥n proporcionada, un Transformer es un modelo de arquitectura que utiliza capas apiladas de auto-atenci√≥n y capas totalmente conectadas puntuales, tanto para el codificador como para el decodificador.  El codificador est√° compuesto por una pila de N=6 capas id√©nticas, cada una con dos subcapas: una de mecanismo de auto-atenci√≥n multi-cabeza y otra, una capa totalmente conectada simple y posicional.  La figura 1 muestra la arquitectura completa del modelo, con el codificador en la mitad izquierda y el decodificador en la mitad derecha.\n",
      "\n",
      "\n",
      "Respuesta esperada:  Se puede descomponer el problema en partes m√°s peque√±as y resolver cada parte por separado.\n",
      "Respuesta obtenita:  Seg√∫n la informaci√≥n proporcionada, Chain of Thought es una t√©cnica que se puede usar para resolver problemas complejos con un LLM.  La informaci√≥n indica que es robusta a diferentes √≥rdenes de ejemplos y a un n√∫mero variable de ellos.  Adem√°s, aunque es particularmente adecuada para problemas matem√°ticos, su naturaleza basada en el lenguaje la hace aplicable a una amplia gama de problemas de razonamiento de sentido com√∫n que involucran el razonamiento sobre interacciones f√≠sicas y humanas bajo la presunci√≥n de conocimiento general.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_other_config(ammount_of_chunks=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxedNNPwK40e"
   },
   "source": [
    "**RESPUESTA**: Al disminuir la cantidad de chunks recuperados, se obtiene una respuesta m√°s general y menos precisa, ya que el modelo obtieen menos contexto de diferentes partes del documento. Sin embargo, la respuesta en este caso es bastante buena, probablemente debido a que el tama√±o de los chunks es lo suficientemente grande para que el modelo pueda responder correctamente, y que el chunk que se obtiene es realmente suficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHBwHTyjK40f",
    "outputId": "4e7f670f-70bd-4fb0-ce53-51c0ccf7df38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta esperada:  Un Transformer es un tipo de arquitectura de red neuronal que usa un encoder-decoder para modelar relaciones en datos secuenciales.\n",
      "Respuesta obtenita:  El Transformer es una nueva y simple arquitectura de red propuesta como modelo de transducci√≥n de secuencias.  A diferencia de los modelos dominantes basados en redes neuronales recurrentes o convolucionales complejas, el Transformer se basa completamente en la auto-atenci√≥n para calcular las representaciones de su entrada y salida, sin usar RNNs o convoluciones alineados con la secuencia.  Es el primer modelo de transducci√≥n que hace esto.  Su arquitectura utiliza capas apiladas de auto-atenci√≥n y capas totalmente conectadas puntuales, tanto para el codificador como para el decodificador.  El codificador est√° compuesto por una pila de N=6 capas id√©nticas, cada una con dos subcapas: un mecanismo de auto-atenci√≥n multi-cabeza y una capa de conexi√≥n totalmente conectada posicional.  El modelo tambi√©n utiliza codificaciones posicionales tanto en el codificador como en el decodificador, y un suavizado de etiquetas durante el entrenamiento.\n",
      "\n",
      "\n",
      "Respuesta esperada:  Se puede descomponer el problema en partes m√°s peque√±as y resolver cada parte por separado.\n",
      "Respuesta obtenita:  Seg√∫n la informaci√≥n proporcionada, la t√©cnica Chain-of-Thought (pensamiento en cadena) se puede usar para resolver problemas complejos con un LLM.  Esta t√©cnica implica descomponer el problema en pasos intermedios y resolver cada uno antes de dar la respuesta final, imitando un proceso de pensamiento paso a paso.  Adem√°s, es robusta a diferentes √≥rdenes de ejemplos y a un n√∫mero variable de ellos.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_other_config(ammount_of_chunks=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiBvyNg0K40f"
   },
   "source": [
    "**RESPUESTA**: Al aumentar la cantidad de chunks recuperados, se obtiene una respuesta m√°s completa y precisa, ya que el modelo obtiene m√°s contexto de diferentes partes del documento. Esto se cumple en este caso, entregando una respuesta bastante completa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8YsRDLxK40g"
   },
   "source": [
    "**PROBANDO CAMBIAR TIPO DE BUSQUEDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_qHrXwaK40h",
    "outputId": "48bfe10a-c364-474f-e112-89b7171c5922"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta esperada:  Un Transformer es un tipo de arquitectura de red neuronal que usa un encoder-decoder para modelar relaciones en datos secuenciales.\n",
      "Respuesta obtenita:  Un Transformer es un modelo de arquitectura que utiliza capas apiladas de auto-atenci√≥n y capas totalmente conectadas punto a punto, tanto para el codificador como para el decodificador.  El codificador est√° compuesto por una pila de N=6 capas id√©nticas, cada una con dos subcapas: un mecanismo de auto-atenci√≥n multi-cabeza y una capa de conexi√≥n totalmente conectada posicional.  Existen diferentes tama√±os de Transformers,  por ejemplo, uno con (L=6, H=1024, A=16) y 100M par√°metros, y otro m√°s grande (L=64, H=512, A=2) con 235M par√°metros.  Se ha demostrado que aumentar el tama√±o del modelo lleva a mejoras continuas en tareas a gran escala como la traducci√≥n autom√°tica y el modelado del lenguaje.\n",
      "\n",
      "\n",
      "Respuesta esperada:  Se puede descomponer el problema en partes m√°s peque√±as y resolver cada parte por separado.\n",
      "Respuesta obtenita:  Seg√∫n la informaci√≥n proporcionada, la t√©cnica Chain of Thought (CoT) se puede usar para resolver problemas complejos con un LLM.  La informaci√≥n indica que CoT es robusta a diferentes √≥rdenes de ejemplos y a un n√∫mero variable de ellos.  Adem√°s, aunque es particularmente adecuada para problemas matem√°ticos, su naturaleza basada en el lenguaje la hace aplicable a una amplia gama de problemas de razonamiento de sentido com√∫n.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try_other_config(search_type='mmr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V45wtV4JK40h"
   },
   "source": [
    "**RESPUESTA**: Al cambiar similarity por MMR (Maximal Marginal Relevance), se obtiene una respuesta m√°s completa gracias a la diversidad de los chunks recuperados. Esto dado a que MMR busca chunks que sean diferentes entre s√≠, lo que permite que el modelo tenga m√°s contexto para responder la pregunta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENJiPPM0giX8"
   },
   "source": [
    "### **2.2 Agentes (1.0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/rcqnN2aJCSEAAAAd/secret-agent-man.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Similar a la secci√≥n anterior, en esta secci√≥n se busca habilitar **Agentes** para obtener informaci√≥n a trav√©s de tools y as√≠ responder la pregunta del usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V47l7Mjfrk0N"
   },
   "source": [
    "#### **2.2.1 Tool de Tavily (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas al motor de b√∫squeda **Tavily**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6SLKwcWr0AG"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SonB1A-9rtRq"
   },
   "source": [
    "#### **2.2.2 Tool de Wikipedia (0.2 puntos)**\n",
    "\n",
    "Generar una *tool* que pueda hacer consultas a **Wikipedia**.\n",
    "\n",
    "*Hint: Le puede ser de ayuda el siguiente [link](https://python.langchain.com/v0.1/docs/modules/tools/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-_2NwU1K40k",
    "outputId": "52552943-2392-4c32-e9b4-ad45957a7a6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lucas\\python-envs\\nb-env\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehJJpoqsr26-"
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper = WikipediaAPIWrapper(lang='es', top_k_results=1, doc_content_chars_max=100)\n",
    "wiki_tool = WikipediaQueryRun(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvUIMdX6r0ne"
   },
   "source": [
    "#### **2.2.3 Crear Agente (0.3 puntos)**\n",
    "\n",
    "Crear un agente que pueda responder preguntas preguntas usando las *tools* antes generadas. Aseg√∫rese que su agente responda en espa√±ol. Por √∫ltimo, guarde el agente en una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VO9cE0TQK40l",
    "outputId": "aa9908f8-2fb3-449a-c09d-0dede14f699d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\langsmith\\client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "{tools}\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [{tool_names}]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {input}\n",
      "Thought:{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "react_prompt = hub.pull(\"hwchase17/react\") # template de ReAct\n",
    "print(react_prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pD1_n0wrsDI5",
    "outputId": "d793a1ba-d15b-49b6-c8e5-3da6517ed783"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentExecutor(verbose=True, agent=RunnableAgent(runnable=RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_log_to_str(x['intermediate_steps']))\n",
       "})\n",
       "| PromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={}, partial_variables={'tools': 'tavily_search_results_json - A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.\\nwikipedia - A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'tool_names': 'tavily_search_results_json, wikipedia'}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'react', 'lc_hub_commit_hash': 'd15fe3c426f1c4b3f37c9198853e4a86e20c425ca7f4752ec0c9b0e97ca7ea4d'}, template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}')\n",
       "| RunnableBinding(bound=ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), temperature=0.0, max_retries=2, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000002025B731F40>, default_metadata=()), kwargs={'stop': ['\\nObservation']}, config={}, config_factories=[])\n",
       "| ReActSingleInputOutputParser(), input_keys_arg=[], return_keys_arg=[], stream_runnable=True), tools=[TavilySearchResults(max_results=1, api_wrapper=TavilySearchAPIWrapper(tavily_api_key=SecretStr('**********'))), WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'c:\\\\Users\\\\lucas\\\\python-envs\\\\nb-env\\\\Lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='es', load_all_available_meta=False, doc_content_chars_max=100))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "\n",
    "tools = [tavily_tool, wiki_tool]\n",
    "\n",
    "agent = create_react_agent(llm, tools, react_prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6EW2E_MbK40m",
    "outputId": "fa58a55b-7b3e-4c3d-8fd9-d8ee0b4fec08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To answer this question, I need to find a list of professors from the Faculty of Physics and Mathematics (FCFM) at the University of Chile.  A Wikipedia search might be a good starting point, but it might not be completely up-to-date.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: \"Faculty of Physics and Mathematics, University of Chile\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Universidad de California en Berkeley\n",
      "Summary: La Universidad de California en Berkeley (en id\u001b[0m\u001b[32;1m\u001b[1;3mThought: That was the wrong Wikipedia page. I need to try a different search term.  Let's try searching directly for the faculty's website.  If that doesn't work, I'll try a different approach.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: \"Facultad de Ciencias F√≠sicas y Matem√°ticas, Universidad de Chile\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Facultad de Ciencias F√≠sicas y Matem√°ticas de la Universidad de Chile\n",
      "Summary: La Facultad de \u001b[0m\u001b[32;1m\u001b[1;3mThought: The Wikipedia page for the Facultad de Ciencias F√≠sicas y Matem√°ticas (FCFM) of the University of Chile should list faculty members, or at least provide a link to a page with that information.  Let's see if the page provides a list of professors or a link to a faculty directory.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: \"Facultad de Ciencias F√≠sicas y Matem√°ticas, Universidad de Chile\"\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Facultad de Ciencias F√≠sicas y Matem√°ticas de la Universidad de Chile\n",
      "Summary: La Facultad de \u001b[0m\u001b[32;1m\u001b[1;3mThought: The Wikipedia page doesn't seem to have a readily available list of current professors.  I'll try searching directly for the FCFM's website using a search engine.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"Facultad de Ciencias F√≠sicas y Matem√°ticas Universidad de Chile profesores\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://ingenieria.uchile.cl/sobre-la-fcfm', 'content': 'La Facultad de Ciencias F√≠sicas y Matem√°ticas se orienta a la ense√±anza superior y al desarrollo de las ciencias b√°sicas, ingenier√≠a, aplicaciones tecnol√≥gicas e innovaci√≥n. ... Profesores/as Em√©ritos/as FCFM-U. de Chile. Francisco Brieva Rodr√≠guez ... Internacional; Sustentabilidad; Biblioteca; Facultad de Ciencias F√≠sicas y'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: The search result provides a link to the FCFM website, but doesn't list professors directly.  I need to navigate to that website to find a faculty directory.  Since I can't directly browse websites, I'll have to rely on the information I can gather from the search results and potentially further searches.  The search result mentions \"Profesores/as Em√©ritos/as FCFM-U. de Chile. Francisco Brieva Rodr√≠guez\", so I have at least one name.\n",
      "\n",
      "Thought: I now know the final answer.  I can't provide a comprehensive list, but I have at least one name.\n",
      "\n",
      "Final Answer: At least one professor at the FCFM of the University of Chile is Francisco Brieva Rodr√≠guez.  A more complete list would require accessing and navigating the FCFM website, which is beyond my current capabilities.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "At least one professor at the FCFM of the University of Chile is Francisco Brieva Rodr√≠guez.  A more complete list would require accessing and navigating the FCFM website, which is beyond my current capabilities.\n"
     ]
    }
   ],
   "source": [
    "def call_agent(input: str):\n",
    "    response = agent_executor.invoke({\"input\": input})\n",
    "    print(response[\"output\"])\n",
    "\n",
    "call_agent(\"Dame el nombre de algunos profesores de la FCFM de la Universidad de Chile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKV0JxK3r-XG"
   },
   "source": [
    "#### **2.2.4 Verificaci√≥n de respuestas (0.3 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente y aseg√∫rese que el agente est√© ocupando correctamente las tools disponibles. ¬øEn qu√© casos el agente deber√≠a ocupar la tool de Tavily? ¬øEn qu√© casos deber√≠a ocupar la tool de Wikipedia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55H0LYWXK40o",
    "outputId": "522e3f52-8080-45b4-8f75-59f33914e471"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find information about Barack Obama.  Wikipedia is a good resource for biographical information.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]‚Äã ( en ingl√©s estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Qui√©n es Obama?\n",
      "Thought: I need to find information about Barack Obama.  Wikipedia is a good resource for biographical information.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]‚Äã ( en ingl√©s estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Qui√©n es Obama?\n",
      "Thought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]‚Äã ( en ingl√©s estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Qui√©n es Obama?\n",
      "Thought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]‚Äã ( en ingl√©s estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Qui√©n es Obama?\n",
      "Thought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]‚Äã ( en ingl√©s estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Qui√©n es Obama?\n",
      "Thought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]‚Äã ( en ingl√©s estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Qui√©n es Obama?\n",
      "Thought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]‚Äã ( en ingl√©s estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Qui√©n es Obama?\n",
      "Thought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]‚Äã ( en ingl√©s estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Qui√©n es Obama?\n",
      "Thought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]‚Äã ( en ingl√©s estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]‚Äã ( en ingl√©s estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to get a full summary of Barack Obama's life from Wikipedia to answer the question properly.  The previous attempts only showed a snippet.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]‚Äã ( en ingl√©s estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mThought: The previous attempts only returned a truncated summary. I need to access the full Wikipedia page to get a complete answer.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]‚Äã ( en ingl√©s estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mThought: The repeated attempts to access the Wikipedia page for Barack Obama only yielded truncated summaries.  I need a different approach to get a complete answer.  I will try again, specifying that I need the full text.  If that fails, I will resort to a different strategy.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama  (Requesting full page content)\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Drake (m√∫sico)\n",
      "Summary: Aubrey Drake Graham (Toronto, 24 de octubre de 1986) conocido simpleme\u001b[0m\u001b[32;1m\u001b[1;3mThought: The Wikipedia calls are failing to return the full page content.  I will try a different approach.  I will use a search engine to find a reliable source with a biography of Barack Obama.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"Barack Obama biography\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.whitehouse.gov/about-the-white-house/presidents/barack-obama/', 'content': 'Mobile Menu Overlay\\nThe White House\\n1600 Pennsylvania Ave NW\\nWashington, DC 20500\\nBarack Obama\\nThe 44th President of the United States\\nNavigate this Section\\nThe biography for President Obama and past presidents is courtesy of the White House Historical Association.\\n In the last year of his second term, Obama spoke at two events that clearly moved him‚Äîthe 50th anniversary of the civil rights march from Selma to Montgomery, and the dedication of the National Museum of African American History and Culture. His story is the American story ‚Äî values from the heartland, a middle-class upbringing in a strong family, hard work and education as the means of getting ahead, and the conviction that a life so blessed should be lived in service to others.\\n Following graduation in 1983, Obama worked in New York City, then became a community organizer on the South Side of Chicago, coordinating with churches to improve housing conditions and set up job-training programs in a community hit hard by steel mill closures. ‚ÄúAnd that‚Äôs why we celebrate,‚Äù he told those attending the museum opening in Washington, ‚Äúmindful that our work is not yet done.‚Äù\\nLearn more about Barack Obama‚Äôs spouse, Michelle Obama.\\n'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now have a concise biography of Barack Obama from a reliable source.\n",
      "\n",
      "Final Answer: Barack Obama was the 44th President of the United States.  He had a middle-class upbringing and worked as a community organizer in Chicago before entering politics.  His life has been dedicated to public service.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Barack Obama was the 44th President of the United States.  He had a middle-class upbringing and worked as a community organizer in Chicago before entering politics.  His life has been dedicated to public service.\n"
     ]
    }
   ],
   "source": [
    "call_agent(\"Qui√©n es Obama?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56bVbo06K40o",
    "outputId": "c1e4f995-c989-4a04-829b-d2ab9732e8e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To answer this question about FC Barcelona's performance in the Spanish league, I need current standings and possibly recent results.  A search engine like tavily_search_results_json should provide up-to-date information.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"FC Barcelona La Liga standings\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.foxsports.com/soccer/barcelona-team-standings', 'content': 'Barcelona Laliga Standings | FOX Sports SPORTS & TEAMS SPORTS SPORTS & TEAMS SPORTS NFL NCAA FB MLB NBA NCAA BK NASCAR Soccer UFL NCAAW BK NHL Golf Premier Boxing Champions WWE UFC WNBA Tennis Motor Sports Professional Bowlers Association Horse Racing Westminster Kennel Club FIBA Olympics World Baseball Classic NRL The Basketball Tournament BARCELONA 9-0-1 ¬∑ 1ST IN LALIGA ¬∑ CHAMPIONS LEAGUE 9-0-1 ¬∑ 1ST IN LALIGA ¬∑ CHAMPIONS LEAGUE Next Match vs Real Madrid (A) ¬∑ Sat 7:00PM BARCELONA LALIGA STANDINGS uefa champions league uefa europa league When games are live, standings update in real-time. QUALIFICATION/RELEGATION Champions League Europa League Relegation Real Madrid Top Leagues FOX Sports Soccer Leagues and Tournaments  2024 MLB Playoff Bracket  2024 NFL Power Rankings  2024 World Series'}]\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: C√≥mo le est√° yendo al FC Barcelona en la liga espa√±ola?\n",
      "Thought:To answer this question about FC Barcelona's performance in the Spanish league, I need current standings and possibly recent results.  A search engine like tavily_search_results_json should provide up-to-date information.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"FC Barcelona La Liga standings\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.foxsports.com/soccer/barcelona-team-standings', 'content': 'Barcelona Laliga Standings | FOX Sports SPORTS & TEAMS SPORTS SPORTS & TEAMS SPORTS NFL NCAA FB MLB NBA NCAA BK NASCAR Soccer UFL NCAAW BK NHL Golf Premier Boxing Champions WWE UFC WNBA Tennis Motor Sports Professional Bowlers Association Horse Racing Westminster Kennel Club FIBA Olympics World Baseball Classic NRL The Basketball Tournament BARCELONA 9-0-1 ¬∑ 1ST IN LALIGA ¬∑ CHAMPIONS LEAGUE 9-0-1 ¬∑ 1ST IN LALIGA ¬∑ CHAMPIONS LEAGUE Next Match vs Real Madrid (A) ¬∑ Sat 7:00PM BARCELONA LALIGA STANDINGS uefa champions league uefa europa league When games are live, standings update in real-time. QUALIFICATION/RELEGATION Champions League Europa League Relegation Real Madrid Top Leagues FOX Sports Soccer Leagues and Tournaments  2024 MLB Playoff Bracket  2024 NFL Power Rankings  2024 World Series'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought:The observation shows that FC Barcelona is currently 1st in La Liga with a record of 9 wins, 0 losses, and 1 draw.\n",
      "\n",
      "Thought:I now know the final answer.\n",
      "\n",
      "Final Answer: Al FC Barcelona le est√° yendo muy bien en la liga espa√±ola.  Actualmente est√°n en primer lugar con un r√©cord de 9 victorias, 0 derrotas y 1 empate.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Al FC Barcelona le est√° yendo muy bien en la liga espa√±ola.  Actualmente est√°n en primer lugar con un r√©cord de 9 victorias, 0 derrotas y 1 empate.\n"
     ]
    }
   ],
   "source": [
    "call_agent(\"C√≥mo le est√° yendo al FC Barcelona en la liga espa√±ola?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNF3CkQjK40p"
   },
   "source": [
    "En general deber√≠a usar Wikipedia para informaci√≥n especifica que suele estar contenida en wikipedia, mientras que en cualquier otro caso m√°s general deber√≠a usar Tavily. Que tipo de informaci√≥n deber√≠a buscar en wikipedia? Cuando se le pregunta el nombre de una persona, una entidad economica, etc. Es posible que incluso en esos casos se use la de Tavily, pero ese ser√≠a el criterio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZbDTYiogquv"
   },
   "source": [
    "### **2.3 Multi Agente (1.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/r7QMJLxU4BoAAAAd/this-is-getting-out-of-hand-star-wars.gif\"\n",
    "\" width=\"450\">\n",
    "</p>\n",
    "\n",
    "El objetivo de esta subsecci√≥n es encapsular las funcionalidades creadas en una soluci√≥n multiagente con un **supervisor**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-iUfH0WvI6m"
   },
   "source": [
    "#### **2.3.1 Generando Tools (0.5 puntos)**\n",
    "\n",
    "Transforme la soluci√≥n RAG de la secci√≥n 2.1 y el agente de la secci√≥n 2.2 a *tools* (una tool por cada uno)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lV4VrNiEK40r"
   },
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def call_rag(question: str) -> str:\n",
    "    \"\"\"Call the RAG model to answer a question\"\"\"\n",
    "    return rag_chain.invoke(question)\n",
    "\n",
    "@tool\n",
    "def call_react(input: str) -> str:\n",
    "    \"\"\"Call the ReAct model to answer a question\"\"\"\n",
    "    return agent_executor.invoke({\"input\": input})[\"output\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQYNjT_0vPCg"
   },
   "source": [
    "#### **2.3.2 Agente Supervisor (0.5 puntos)**\n",
    "\n",
    "Habilite un agente que tenga acceso a las tools del punto anterior y pueda responder preguntas relacionadas. Almacene este agente en una variable llamada supervisor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pw1cfTtvv1AZ"
   },
   "outputs": [],
   "source": [
    "supervisor_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Eres un agente enrrutador de preguntas.\n",
    "    Tu rol es decidir que acci√≥n tomar para poder contestar la pregunta de la mejor manera posible:\n",
    "    - 'llm': Cuando tengas que usar un sistema RAG para extraer informaci√≥n sobre LLMs. Esto incluye informaci√≥n de BERT, Transformers, CoT, etc.\n",
    "    - 'general': Cuando la pregunta sea relacionada a cosas que puedan estar en internet o personas o entidades que conoce wikipedia.\n",
    "    - 'otro': Todo aquella pregunta que no est√© contenida en las categor√≠as anteriores.\n",
    "    - 'fin': Si crees que la pregunta ya puede ser contestada con la informaci√≥n actual.\n",
    "\n",
    "    No respondas con m√°s de una palabra y no incluyas.\n",
    "\n",
    "    Pregunta:\n",
    "    {question}\n",
    "\n",
    "\n",
    "    Informaci√≥n actual:\n",
    "    {context}\n",
    "\n",
    "\n",
    "    Categor√≠a:\"\"\"\n",
    ")\n",
    "\n",
    "supervisor_chain = (\n",
    "    supervisor_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-c8m_C_K40s"
   },
   "outputs": [],
   "source": [
    "answer_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Eres un agente experto en usar informaci√≥n dada para responder una pregunta.\n",
    "    Tu rol es usar la informaci√≥n entregada para responder la pregunta de la mejor manera posible.\n",
    "    No seas tan exigente con la informaci√≥n necesaria, si crees que puedes responder con la informaci√≥n entregada, hazlo.\n",
    "\n",
    "    Informaci√≥n dada:\n",
    "    {context}\n",
    "\n",
    "\n",
    "    Pregunta:\n",
    "    {question}\n",
    "\n",
    "\n",
    "    Respuesta:\"\"\"\n",
    ")\n",
    "\n",
    "answer_chain = (\n",
    "    answer_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Ac541j6K40s"
   },
   "outputs": [],
   "source": [
    "redirect_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Eres un asistente experto en el redireccionamiento de preguntas de usuarios.\n",
    "    Vas a recibir una pregunta del usuario, tu √∫nico rol es indicar que no puedes responder su pregunta y redireccionar al usuario\n",
    "    para que te pregunte sobre papers relacionados a LLMs, o sobre cualquier cosa que pueda estar en internet o personas o\n",
    "    entidades que conoce wikipedia.\n",
    "\n",
    "    Recuerda ser amable y cordial en tu respuesta.\n",
    "\n",
    "    Pregunta: {question}\n",
    "    Respuesta cordial:\"\"\"\n",
    ")\n",
    "\n",
    "redirect_chain = (\n",
    "    redirect_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8-Ht-ETK40t"
   },
   "outputs": [],
   "source": [
    "def supervisor_question(question):\n",
    "    '''\n",
    "    Recibe una pregunta de usuario.\n",
    "    Supervisar la pregunta, redirigir si se requiere m√°s informaci√≥n o responder si es posible.\n",
    "    '''\n",
    "\n",
    "    actual_context = \"No information available\"\n",
    "    action = supervisor_chain.invoke({\"context\": actual_context, \"question\": question})\n",
    "\n",
    "    while \"fin\" not in action:\n",
    "        if \"llm\" in action:\n",
    "            print(\"Usando RAG para extraer informaci√≥n sobre LLMs\")\n",
    "            extra_info = call_rag(question)\n",
    "        elif \"general\" in action:\n",
    "            print(\"Usando ReAct para buscar informaci√≥n general\")\n",
    "            extra_info = call_react(question)\n",
    "        else:\n",
    "            print(\"No se pueden encontrar herramientas para responder la pregunta\")\n",
    "            action = redirect_chain.invoke({\"question\": question})\n",
    "            return action\n",
    "        actual_context = actual_context + \"\\n\" + extra_info if actual_context != \"No information available\" else extra_info\n",
    "        action = supervisor_chain.invoke({\"context\": actual_context, \"question\": question})\n",
    "    print(\"Se posee toda la informaci√≥n necesaria para responder la pregunta\")\n",
    "    return answer_chain.invoke({\"context\": actual_context, \"question\": question})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea3zWlvyvY7K"
   },
   "source": [
    "#### **2.3.3 Verificaci√≥n de respuestas (0.25 puntos)**\n",
    "\n",
    "Pruebe el funcionamiento de su agente repitiendo las preguntas realizadas en las secciones 2.1.4 y 2.2.4 y comente sus resultados. ¬øC√≥mo var√≠an las respuestas bajo este enfoque?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_1t0zkgv1qW",
    "outputId": "afd53061-2acd-46f6-acfc-0319cd2cc0da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando RAG para extraer informaci√≥n sobre LLMs\n",
      "Se posee toda la informaci√≥n necesaria para responder la pregunta\n",
      "BERT es un modelo de lenguaje que alcanza resultados de vanguardia en once tareas de procesamiento del lenguaje natural.  Su arquitectura es unificada para diferentes tareas, permitiendo manejar varias tareas como respuesta a preguntas e inferencia de lenguaje sin modificaciones significativas en su arquitectura.  Es conceptualmente simple pero emp√≠ricamente poderoso, mostrando mejoras significativas en benchmarks como GLUE, MultiNLI y SQuAD v1.1.  Se inicializa con par√°metros pre-entrenados y luego se ajusta finamente para tareas espec√≠ficas usando datos etiquetados.  Puede representar tanto oraciones individuales como pares de oraciones (como pregunta-respuesta) en una secuencia de tokens.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(supervisor_question(\"Qu√© es BERT?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A00z2Ty3K40v",
    "outputId": "f280e67f-d675-4517-eced-0e551f9a3805"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando RAG para extraer informaci√≥n sobre LLMs\n",
      "Usando RAG para extraer informaci√≥n sobre LLMs\n",
      "Se posee toda la informaci√≥n necesaria para responder la pregunta\n",
      "Seg√∫n la informaci√≥n proporcionada, la t√©cnica Chain of Thought (pensamiento en cadena) descompone problemas complejos en subproblemas m√°s peque√±os y manejables, imitando un proceso de pensamiento paso a paso para llegar a la respuesta.  Esto se logra, en parte,  proporcionando ejemplos de razonamiento de pensamiento en cadena en la indicaci√≥n de pocos disparos (few-shot prompting) para que el modelo de lenguaje pueda generar sus propias cadenas de pensamiento.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(supervisor_question(\"Seg√∫n Chain of Thought, ¬øqu√© t√©cnica se puede usar para resolver un problema complejo con un LLM?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_RI7qkCK40w",
    "outputId": "23898490-4131-4d85-ba19-45596cf8af77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando ReAct para buscar informaci√≥n general\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find information about Barack Obama.  Wikipedia is a good resource for biographical information.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]‚Äã ( en ingl√©s estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Qui√©n es Obama?\n",
      "Thought: I need to find information about Barack Obama.  Wikipedia is a good resource for biographical information.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]‚Äã ( en ingl√©s estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Qui√©n es Obama?\n",
      "Thought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]‚Äã ( en ingl√©s estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Qui√©n es Obama?\n",
      "Thought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]‚Äã ( en ingl√©s estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Qui√©n es Obama?\n",
      "Thought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]‚Äã ( en ingl√©s estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Qui√©n es Obama?\n",
      "Thought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]‚Äã ( en ingl√©s estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Qui√©n es Obama?\n",
      "Thought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]‚Äã ( en ingl√©s estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Qui√©n es Obama?\n",
      "Thought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]‚Äã ( en ingl√©s estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: Qui√©n es Obama?\n",
      "Thought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]‚Äã ( en ingl√©s estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to find information about Barack Obama. Wikipedia is a good resource for biographical information.\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]‚Äã ( en ingl√©s estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mThought: I need to get a full summary of Barack Obama's life from Wikipedia to answer the question properly.  The previous attempts only showed a truncated summary.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]‚Äã ( en ingl√©s estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mThought: The previous attempts only returned a truncated summary. I need to access the full Wikipedia page to get a complete answer.  I will try again, and if necessary, I will look for ways to access the full text of the Wikipedia page through the wikipedia wrapper.\n",
      "\n",
      "Action: wikipedia\n",
      "Action Input: Barack Obama\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Barack Obama\n",
      "Summary: Barack Hussein Obama II[1]‚Äã ( en ingl√©s estadounidense; Honolulu, 4 de a\u001b[0m\u001b[32;1m\u001b[1;3mThought: The Wikipedia API is not providing a full summary.  I need to find a way to access the full text.  Since I don't have that capability, I will provide a general answer based on common knowledge.\n",
      "\n",
      "Thought: I now know the final answer.\n",
      "Final Answer: Barack Obama is the 44th president of the United States.  He was the first African American president and served two terms from 2009 to 2017.  Before his presidency, he was a community organizer, civil rights attorney, and senator from Illinois.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Se posee toda la informaci√≥n necesaria para responder la pregunta\n",
      "Obama es el 44¬∫ presidente de los Estados Unidos. Fue el primer presidente afroamericano y sirvi√≥ dos mandatos, de 2009 a 2017. Antes de su presidencia, fue organizador comunitario, abogado de derechos civiles y senador por Illinois.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(supervisor_question(\"Qui√©n es Obama?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nPpLdGKDK40x",
    "outputId": "2adc089d-8167-478e-f361-673272fad799"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando ReAct para buscar informaci√≥n general\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To answer this question about FC Barcelona's performance in the Spanish league, I need current standings and possibly recent results.  A search engine like tavily_search_results_json should provide this information.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"FC Barcelona La Liga standings 2023-2024\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.eurosport.com/football/la-liga/2023-2024/standings.shtml', 'content': 'Stay up to date with the 2023/2024 La Liga table. Follow the top teams and make Eurosport your go-to source for Football tables and results. ... FC Barcelona BAR. 38: 26: 7: 5: 79: 44: 35: 85: 3'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought: The observation shows a snippet indicating FC Barcelona's position in La Liga. However, it's a small snippet and might not be entirely up-to-date. To get a more complete and reliable picture, I should consult a dedicated sports website or news source.  I'll try a different search.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"FC Barcelona La Liga table\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.bbc.co.uk/sport/football/teams/barcelona/table', 'content': 'Barcelona Tables. Search. UEFA Champions League (active) Spanish La Liga; ... Spanish La Liga Skip to table key Match Participants: Barcelona, Position 1, Points . 33. Position Team Played Won'}]\u001b[0m\u001b[32;1m\u001b[1;3mThought:The second search provides a more promising result, mentioning Barcelona's position in the La Liga table.  However, the content snippet is incomplete.  To get the full and current standings, I need to visit the linked website.  Since I can't directly access and process external websites, I'll have to rely on the information I have so far.  The snippets suggest Barcelona is doing well, likely near the top of the table.  I can't give a precise answer without accessing the full table.\n",
      "\n",
      "Thought: I now know the final answer.  I can't provide a precise answer regarding FC Barcelona's exact position and points in La Liga without accessing the linked websites. However, based on the search results, it appears they are performing very well and are likely near the top of the table.\n",
      "\n",
      "Final Answer: Based on limited information from search results, FC Barcelona appears to be performing very well in La Liga 2023-2024 and is likely near the top of the table.  A more precise answer requires accessing the linked websites directly.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Usando ReAct para buscar informaci√≥n general\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To answer this question about FC Barcelona's performance in the Spanish league, I need current standings and possibly recent results.  A search engine like tavily_search_results_json should provide up-to-date information.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"FC Barcelona La Liga standings\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.foxsports.com/soccer/barcelona-team-standings', 'content': 'Barcelona Laliga Standings | FOX Sports SPORTS & TEAMS SPORTS SPORTS & TEAMS SPORTS NFL NCAA FB MLB NBA NCAA BK NASCAR Soccer UFL NCAAW BK NHL Golf Premier Boxing Champions WWE UFC WNBA Tennis Motor Sports Professional Bowlers Association Horse Racing Westminster Kennel Club FIBA Olympics World Baseball Classic NRL The Basketball Tournament BARCELONA 9-0-1 ¬∑ 1ST IN LALIGA ¬∑ CHAMPIONS LEAGUE 9-0-1 ¬∑ 1ST IN LALIGA ¬∑ CHAMPIONS LEAGUE Next Match vs Real Madrid (A) ¬∑ Sat 7:00PM BARCELONA LALIGA STANDINGS uefa champions league uefa europa league When games are live, standings update in real-time. QUALIFICATION/RELEGATION Champions League Europa League Relegation Real Madrid Top Leagues FOX Sports Soccer Leagues and Tournaments  2024 MLB Playoff Bracket  2024 NFL Power Rankings  2024 World Series'}]\u001b[0m\u001b[32;1m\u001b[1;3mQuestion: C√≥mo le est√° yendo al FC Barcelona en la liga espa√±ola?\n",
      "Thought:To answer this question about FC Barcelona's performance in the Spanish league, I need current standings and possibly recent results.  A search engine like tavily_search_results_json should provide up-to-date information.\n",
      "\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"FC Barcelona La Liga standings\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.fcbarcelona.com/en/football/first-team/standings', 'content': \"Standings for LaLiga, Champions League, Copa del Rey for the first team ... All the official information of the FC Barcelona Supporters' Clubs. ... La Liga. Last updated: 21:27PM Sunday 26 May\"}]\u001b[0m\u001b[32;1m\u001b[1;3mThought:The observations show that FC Barcelona is doing very well in La Liga.  One source indicates they are 1st in La Liga with a record of 9-0-1.  The other source points to their official website for standings, but doesn't give the specific record.  I'll use the more detailed information.\n",
      "\n",
      "Thought: I now know the final answer.\n",
      "\n",
      "Final Answer: Al FC Barcelona le est√° yendo muy bien en la liga espa√±ola.  Actualmente est√°n en primer lugar con un r√©cord de 9 victorias, 0 derrotas y 1 empate.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Se posee toda la informaci√≥n necesaria para responder la pregunta\n",
      "Al FC Barcelona le est√° yendo muy bien en La Liga 2023-2024.  Actualmente ocupan el primer lugar con un r√©cord de 9 victorias, 0 derrotas y 1 empate.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(supervisor_question(\"C√≥mo le est√° yendo al FC Barcelona en la liga espa√±ola?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhb3TtrZK40y"
   },
   "source": [
    "**RESPUESTA**: Las respuestas son bastante similares a las entregadas por los agentes individuales, sin embargo en algunas corridas las respuestas mejoran, ya que al detectar una respuesta que no es lo suficientemente buena, o que no tiene la informaci√≥n que se busca, vuelve a buscar ya sea en esa fuente o en otra. Esto permite que las respuestas sean correctas en la mayor√≠a de los casos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qb8bdAmYvgwn"
   },
   "source": [
    "#### **2.3.4 An√°lisis (0.25 puntos)**\n",
    "\n",
    "¬øQu√© diferencias tiene este enfoque con la soluci√≥n *Router* vista en clases? Nombre al menos una ventaja y desventaja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAUlJxqoLK5r"
   },
   "source": [
    "**RESPUESTA**: La diferencia principal es que al correr es c√≥mo se maneja la respuesta entregada por las tools que creamos. El router por un lado solo redirige la pregunta a donde corresponda, mientras que el supervisor puede volver a buscar en otra fuente y evaluar la respuesta es correcta o si es lo que se pide. Con esto se logra que las respuestas sean m√°s precisas y correctas. Sin embargo, esto tambi√©n puede ser una desventaja, ya que va a implicar un costo y tiempo adicional por la forma de procesar las respuestas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JWVSuWiZ8Mj"
   },
   "source": [
    "### **2.4 Memoria (Bonus +0.5 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/Gs95aiElrscAAAAd/memory-unlocked-ratatouille-critic.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una de las principales falencias de las soluciones que hemos visto hasta ahora es que nuestro chat no responde las interacciones anteriores, por ejemplo:\n",
    "\n",
    "- Pregunta 1: \"Hola! mi nombre es Sebasti√°n\"\n",
    "  - Respuesta esperada: \"Hola Sebasti√°n! ...\"\n",
    "- Pregunta 2: \"Cual es mi nombre?\"\n",
    "  - Respuesta actual: \"Lo siento pero no conozco tu nombre :(\"\n",
    "  - **Respuesta esperada: \"Tu nombre es Sebasti√°n\"**\n",
    "\n",
    "Para solucionar esto, se les solicita agregar un componente de **memoria** a la soluci√≥n entregada en el punto 2.3.\n",
    "\n",
    "**Nota: El Bonus es v√°lido <u>s√≥lo para la secci√≥n 2 de Large Language Models.</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6Y7tIPJLPfB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFc3jBT5g0kT"
   },
   "source": [
    "### **2.5 Despliegue (0 puntos)**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media1.tenor.com/m/IytHqOp52EsAAAAd/you-get-a-deploy-deploy.gif\"\n",
    "\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "Una vez tengan los puntos anteriores finalizados, toca la etapa de dar a conocer lo que hicimos! Para eso, vamos a desplegar nuestro modelo a trav√©s de `gradio`, una librer√≠a especializada en el levantamiento r√°pido de demos basadas en ML.\n",
    "\n",
    "Primero instalamos la librer√≠a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8TsvnCPbkIA",
    "outputId": "87ee2778-cffb-48dd-e715-9d917ac7722f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-chroma 0.1.1 requires langchain-core<0.3,>=0.1.40, but you have langchain-core 0.3.19 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJBztEUovKsF"
   },
   "source": [
    "Luego s√≥lo deben ejecutar el siguiente c√≥digo e interactuar con la interfaz a trav√©s del notebook o del link generado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3KedQSvg1-n",
    "outputId": "94b03510-97c0-49f0-ad76-359f02b7c7fa"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magent_response\u001b[39m(message, history):\n",
      "File \u001b[1;32mc:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\gradio\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_simple_templates\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_utils\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessing_utils\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\gradio\\_simple_templates\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimpledropdown\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleDropdown\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimpleimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleImage\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimpletextbox\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleTextbox\n",
      "File \u001b[1;32mc:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\gradio\\_simple_templates\\simpledropdown.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, Sequence\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Any\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Component, FormComponent\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Events\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[1;32mc:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\gradio\\components\\__init__.py:40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel3d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model3D\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultimodal_textbox\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultimodalTextbox\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnative_plot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BarPlot, LinePlot, NativePlot, ScatterPlot\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumber\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Number\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparamviewer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParamViewer\n",
      "File \u001b[1;32mc:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\gradio\\components\\native_plot.py:12\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, Sequence, Set\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     TYPE_CHECKING,\n\u001b[0;32m      8\u001b[0m     Any,\n\u001b[0;32m      9\u001b[0m     Literal,\n\u001b[0;32m     10\u001b[0m )\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio_client\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocumentation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m document\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Component\n",
      "File \u001b[1;32mc:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\pandas\\__init__.py:59\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     ArrowDtype,\n\u001b[0;32m     62\u001b[0m     Int8Dtype,\n\u001b[0;32m     63\u001b[0m     Int16Dtype,\n\u001b[0;32m     64\u001b[0m     Int32Dtype,\n\u001b[0;32m     65\u001b[0m     Int64Dtype,\n\u001b[0;32m     66\u001b[0m     UInt8Dtype,\n\u001b[0;32m     67\u001b[0m     UInt16Dtype,\n\u001b[0;32m     68\u001b[0m     UInt32Dtype,\n\u001b[0;32m     69\u001b[0m     UInt64Dtype,\n\u001b[0;32m     70\u001b[0m     Float32Dtype,\n\u001b[0;32m     71\u001b[0m     Float64Dtype,\n\u001b[0;32m     72\u001b[0m     CategoricalDtype,\n\u001b[0;32m     73\u001b[0m     PeriodDtype,\n\u001b[0;32m     74\u001b[0m     IntervalDtype,\n\u001b[0;32m     75\u001b[0m     DatetimeTZDtype,\n\u001b[0;32m     76\u001b[0m     StringDtype,\n\u001b[0;32m     77\u001b[0m     BooleanDtype,\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     NA,\n\u001b[0;32m     80\u001b[0m     isna,\n\u001b[0;32m     81\u001b[0m     isnull,\n\u001b[0;32m     82\u001b[0m     notna,\n\u001b[0;32m     83\u001b[0m     notnull,\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     Index,\n\u001b[0;32m     86\u001b[0m     CategoricalIndex,\n\u001b[0;32m     87\u001b[0m     RangeIndex,\n\u001b[0;32m     88\u001b[0m     MultiIndex,\n\u001b[0;32m     89\u001b[0m     IntervalIndex,\n\u001b[0;32m     90\u001b[0m     TimedeltaIndex,\n\u001b[0;32m     91\u001b[0m     DatetimeIndex,\n\u001b[0;32m     92\u001b[0m     PeriodIndex,\n\u001b[0;32m     93\u001b[0m     IndexSlice,\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     NaT,\n\u001b[0;32m     96\u001b[0m     Period,\n\u001b[0;32m     97\u001b[0m     period_range,\n\u001b[0;32m     98\u001b[0m     Timedelta,\n\u001b[0;32m     99\u001b[0m     timedelta_range,\n\u001b[0;32m    100\u001b[0m     Timestamp,\n\u001b[0;32m    101\u001b[0m     date_range,\n\u001b[0;32m    102\u001b[0m     bdate_range,\n\u001b[0;32m    103\u001b[0m     Interval,\n\u001b[0;32m    104\u001b[0m     interval_range,\n\u001b[0;32m    105\u001b[0m     DateOffset,\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     to_numeric,\n\u001b[0;32m    108\u001b[0m     to_datetime,\n\u001b[0;32m    109\u001b[0m     to_timedelta,\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[0;32m    111\u001b[0m     Flags,\n\u001b[0;32m    112\u001b[0m     Grouper,\n\u001b[0;32m    113\u001b[0m     factorize,\n\u001b[0;32m    114\u001b[0m     unique,\n\u001b[0;32m    115\u001b[0m     value_counts,\n\u001b[0;32m    116\u001b[0m     NamedAgg,\n\u001b[0;32m    117\u001b[0m     array,\n\u001b[0;32m    118\u001b[0m     Categorical,\n\u001b[0;32m    119\u001b[0m     set_eng_float_format,\n\u001b[0;32m    120\u001b[0m     Series,\n\u001b[0;32m    121\u001b[0m     DataFrame,\n\u001b[0;32m    122\u001b[0m )\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[1;32mc:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\pandas\\core\\api.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     NaT,\n\u001b[0;32m      3\u001b[0m     Period,\n\u001b[0;32m      4\u001b[0m     Timedelta,\n\u001b[0;32m      5\u001b[0m     Timestamp,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     ArrowDtype,\n\u001b[0;32m     11\u001b[0m     CategoricalDtype,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     PeriodDtype,\n\u001b[0;32m     15\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\pandas\\_libs\\__init__.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401,E501 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     NaT,\n\u001b[0;32m     21\u001b[0m     NaTType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     iNaT,\n\u001b[0;32m     27\u001b[0m )\n",
      "File \u001b[1;32minterval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mhashtable.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.hashtable\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mmissing.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.missing\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\pandas\\_libs\\tslibs\\__init__.py:39\u001b[0m\n\u001b[0;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtypes\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocalize_pydatetime\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_supported_reso\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     36\u001b[0m ]\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes  \u001b[38;5;66;03m# pylint: disable=import-self\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m localize_pydatetime\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     41\u001b[0m     Resolution,\n\u001b[0;32m     42\u001b[0m     get_supported_reso,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m     periods_per_second,\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnattype\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     49\u001b[0m     NaT,\n\u001b[0;32m     50\u001b[0m     NaTType,\n\u001b[0;32m     51\u001b[0m     iNaT,\n\u001b[0;32m     52\u001b[0m     nat_strings,\n\u001b[0;32m     53\u001b[0m )\n",
      "File \u001b[1;32mconversion.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.tslibs.conversion\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32moffsets.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.tslibs.offsets\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mtimestamps.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.tslibs.timestamps\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mtimedeltas.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.tslibs.timedeltas\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mtimezones.pyx:49\u001b[0m, in \u001b[0;36minit pandas._libs.tslibs.timezones\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\dateutil\\tz\\tz.py:1557\u001b[0m, in \u001b[0;36m__get_gettz.<locals>.GettzFunc.__call__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1554\u001b[0m rv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__instances\u001b[38;5;241m.\u001b[39mget(name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1557\u001b[0m     rv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnocache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1558\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rv, tzlocal_classes)\n\u001b[0;32m   1560\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m rv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1565\u001b[0m         \u001b[38;5;66;03m# We also cannot store weak references to None, so we\u001b[39;00m\n\u001b[0;32m   1566\u001b[0m         \u001b[38;5;66;03m# will also not store that.\u001b[39;00m\n\u001b[0;32m   1567\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__instances[name] \u001b[38;5;241m=\u001b[39m rv\n",
      "File \u001b[1;32mc:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\dateutil\\tz\\tz.py:1648\u001b[0m, in \u001b[0;36m__get_gettz.<locals>.GettzFunc.nocache\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m   1646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tzwin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1648\u001b[0m         tz \u001b[38;5;241m=\u001b[39m \u001b[43mtzwin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mWindowsError\u001b[39;00m, \u001b[38;5;167;01mUnicodeEncodeError\u001b[39;00m):\n\u001b[0;32m   1650\u001b[0m         \u001b[38;5;66;03m# UnicodeEncodeError is for Python 2.7 compat\u001b[39;00m\n\u001b[0;32m   1651\u001b[0m         tz \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\dateutil\\tz\\win.py:221\u001b[0m, in \u001b[0;36mtzwin.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    219\u001b[0m     tzkeyname \u001b[38;5;241m=\u001b[39m text_type(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{kn}\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;132;01m{name}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(kn\u001b[38;5;241m=\u001b[39mTZKEYNAME, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m winreg\u001b[38;5;241m.\u001b[39mOpenKey(handle, tzkeyname) \u001b[38;5;28;01mas\u001b[39;00m tzkey:\n\u001b[1;32m--> 221\u001b[0m         keydict \u001b[38;5;241m=\u001b[39m \u001b[43mvaluestodict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtzkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_std_abbr \u001b[38;5;241m=\u001b[39m keydict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStd\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dst_abbr \u001b[38;5;241m=\u001b[39m keydict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDlt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\lucas\\python-envs\\nb-env\\Lib\\site-packages\\dateutil\\tz\\win.py:354\u001b[0m, in \u001b[0;36mvaluestodict\u001b[1;34m(key)\u001b[0m\n\u001b[0;32m    351\u001b[0m tz_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(size):\n\u001b[1;32m--> 354\u001b[0m     key_name, value, dtype \u001b[38;5;241m=\u001b[39m \u001b[43mwinreg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEnumValue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m winreg\u001b[38;5;241m.\u001b[39mREG_DWORD \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m winreg\u001b[38;5;241m.\u001b[39mREG_DWORD_LITTLE_ENDIAN:\n\u001b[0;32m    356\u001b[0m         \u001b[38;5;66;03m# If it's a DWORD (32-bit integer), it's stored as unsigned - convert\u001b[39;00m\n\u001b[0;32m    357\u001b[0m         \u001b[38;5;66;03m# that to a proper signed integer\u001b[39;00m\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;241m&\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<<\u001b[39m \u001b[38;5;241m31\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "def agent_response(message, history):\n",
    "  '''\n",
    "  Funci√≥n para gradio, recibe mensaje e historial, devuelte la respuesta del chatbot.\n",
    "  '''\n",
    "  # get chatbot response\n",
    "  response = ... # rellenar con la respuesta de su chat\n",
    "\n",
    "  # assert\n",
    "  assert type(response) == str, \"output de route_question debe ser string\"\n",
    "\n",
    "  # \"streaming\" response\n",
    "  for i in range(len(response)):\n",
    "    time.sleep(0.015)\n",
    "    yield response[: i+1]\n",
    "\n",
    "gr.ChatInterface(\n",
    "    agent_response,\n",
    "    type=\"messages\",\n",
    "    title=\"Chatbot MDS7202\", # Pueden cambiar esto si lo desean\n",
    "    description=\"Hola! Soy un chatbot muy √∫til :)\", # tambi√©n la descripci√≥n\n",
    "    theme=\"soft\",\n",
    "    ).launch(\n",
    "        share=True, # pueden compartir el link a sus amig@s para que interactuen con su chat!\n",
    "        debug = False,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
