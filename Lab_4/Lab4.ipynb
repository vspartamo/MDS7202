{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"e37cb69cb73a49c2ad07cf670e073cb7","deepnote_cell_height":156.390625,"deepnote_cell_type":"markdown","id":"XUZ1dFPHzAHl"},"source":["<h1><center>Laboratorio 4: Spark y EDA</center></h1>\n","\n","<center><strong>MDS7202: Laboratorio de Programación Científica para Ciencia de Datos - Primavera 2024</strong></center>"]},{"cell_type":"markdown","metadata":{"id":"PkEUN6c8S-E_"},"source":["### Cuerpo Docente:\n","\n","- Profesores: Ignacio Meza, Sebastián Tinoco\n","- Auxiliar: Eduardo Moya\n","- Ayudantes: Nicolás Ojeda, Melanie Peña, Valentina Rojas"]},{"cell_type":"markdown","metadata":{"cell_id":"8ebcb0f2f70c43319279fdd28c13fe89","deepnote_cell_height":171.796875,"deepnote_cell_type":"markdown","id":"tXflExjqzAHr"},"source":["### Equipo: SUPER IMPORTANTE - notebooks sin nombre no serán revisados\n","\n","- Nombre de alumno 1:\n","- Nombre de alumno 2:\n"]},{"cell_type":"markdown","metadata":{"cell_id":"290822720f3e4484b09e762655bcdb76","deepnote_cell_height":62,"deepnote_cell_type":"markdown","id":"AD-V0bbZzAHr"},"source":["### **Link de repositorio de GitHub:** [Repositorio](https://github.com/...../)"]},{"cell_type":"markdown","metadata":{"cell_id":"60255b81ff0349ad9b18f598a8d71386","deepnote_cell_height":216,"deepnote_cell_type":"markdown","id":"hnYD2hBMAwXf","tags":[]},"source":["## Reglas:\n","\n","- **Grupos de 2 personas**\n","- Fecha de entrega: 6 días de plazo con descuento de 1 punto por día. Entregas Jueves a las 23:59.\n","- Instrucciones del lab el viernes a las 16:15 en formato online. Asistencia no es obligatoria.\n","- <u>Prohibidas las copias</u>. Cualquier intento de copia será debidamente penalizado con el reglamento de la escuela.\n","- Tienen que subir el laboratorio a u-cursos y a su repositorio de github. Labs que no estén en u-cursos no serán revisados. Recuerden que el repositorio también tiene nota.\n","- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente serán respondidos por este medio.\n","- Pueden usar cualquier material del curso que estimen conveniente."]},{"cell_type":"markdown","metadata":{"cell_id":"5bf6f5f66dcd4da9a6926774cec108ab","deepnote_cell_height":114.390625,"deepnote_cell_type":"markdown","id":"xzz695obAwXg","tags":[]},"source":["### Temas a tratar\n","\n","- Introducción al manejo y análisis de grandes volúmenes de datos por medio de la libreria `pyspark`."]},{"cell_type":"markdown","metadata":{"cell_id":"50ec30f08f2548a29bc979ed1741f5a0","deepnote_cell_height":243.390625,"deepnote_cell_type":"markdown","id":"6uBLPj1PzAHs"},"source":["### Objetivos principales del laboratorio\n","\n","- Entender, aplicar y aprovechar las ventajas que nos ofrece la libreria `pyspark` para manejar datos tabulares de gran volúmen.\n","- Crear gráficos para el desarrollo de Análisis de Datos Exploratorios (EDA)."]},{"cell_type":"markdown","metadata":{"id":"f7hHEyTgm12s"},"source":["### Datos del Lab\n","\n","- Base de datos: https://gitlab.com/imezadelajara/datos_clase_7_mds7202/-/raw/main/datos_lab_spark.parquet\n","- Objeto serializado: https://gitlab.com/imezadelajara/datos_clase_7_mds7202/-/raw/main/object.pkl"]},{"cell_type":"markdown","metadata":{"id":"6CrDdk5NRAKe"},"source":["## 1. Preguntas Teóricas [12 puntos]\n","(2 por pregunta)"]},{"cell_type":"markdown","metadata":{"id":"EmDMGUTxLp7M"},"source":["<center>\n","<img src=\"https://img.buzzfeed.com/buzzfeed-static/static/2018-08/1/17/enhanced/buzzfeed-prod-web-05/anigif_enhanced-9173-1533160033-1.gif\" width=350 />\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pGZZcxMWRdIa"},"source":["Responda en  máximo 5 líneas las siguientes preguntas:\n","1. ¿Qué es Apache Spark y cuáles son sus principales ventajas sobre Pandas?\n","2. ¿Qué es un RDD en Spark? Describe una de sus principales características. ¿Qué tienen que ver con los dataframes?.\n","3. Diferencia entre transformaciones y acciones en Spark. Proporciona ejemplos de cada una. ¿Qué ocurre internamente cuando se ejecuta una acción?\n","4. Explica la importancia del particionamiento en Spark y cómo afecta el rendimiento del procesamiento de datos.\n","5. ¿Cuáles son las funciones de Spark Driver y Spark Executor?\n","6. ¿Qué es el Catalyst Optimizer en Apache Spark y cuál es su función principal en la optimización de consultas SQL?\n"]},{"cell_type":"markdown","metadata":{"id":"1elJgE8JRn2O"},"source":["**Respuestas**\n","\n","1. Apache Spark es un sistema de procesamiento de datos a gran escala open source. Dada su trabajo de forma distribuida, la principal ventaja de Spark frente a Pandas es su velocidad y capacidad de trabajar con grandes volúmenes de datos.\n","2. Un RDD (Resilient Distributed Dataset) es una colección de objetos distribuidos e inmutables que sufren operaciones en paralelo. Sus principales características son las dependencias (que le permiten reconstruir el RDD original, así siendo solido ante fallos), las particiones (que permiten paralelizar y hacer más eficiente el trabajo) y el computo de funciones (toma una partición específica y devuelve un iterador de los elementos procesados).\n"]},{"cell_type":"markdown","metadata":{},"source":["3. - Las transformaciones son operaciones que generan un nuevo dataframe a partir de uno ya existente sin modificar el original (pues, en Spark estos son inmutables). Ejemplos de transformaciones son la función **map**, **filter** y **groupby**\n","- Las acciones son operaciones que desencadenan la ejecución de todas las transformaciones que hasta entonces se encontraban organizadas y almacenadas internamente. Es por esto que ejecutar una acción conlleva una cantidad de tiempo mayor a una transformación. Ejemplos de acciones son **count**, **take** y **show**.\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["info preg 3:\n","\n","- **Transformaciones**: Estas operaciones generan un nuevo DataFrame a partir de uno existente sin modificar el conjunto de datos original, aprovechando la característica de inmutabilidad de los DataFrames de Spark. Permiten realizar manipulaciones como filtrar, mapear y agrupar los datos.\n","\n","- **Acciones**: Son las operaciones que desencadenan la ejecución de todas las transformaciones acumuladas. Las transformaciones se organizan y almacenan internamente hasta que se invoca una acción, momento en el cual Spark lleva a cabo las operaciones necesarias.\n","\n","Es crucial destacar que Spark opera bajo el principio de `lazy evaluation` , lo que significa que las operaciones se planifican y almacenan sin ejecutarse inmediatamente. Esta evaluación perezosa permite a Spark optimizar el plan de ejecución de las queries, encadenando transformaciones y manteniendo un linaje de los datos para proporcionar tolerancia a fallos."]},{"cell_type":"markdown","metadata":{},"source":["info preg 4:\n","El particionamiento de datos es clave para optimizar el rendimiento, ya que promueve el paralelismo. Al distribuir los datos en segmentos o particiones, se facilita que los ejecutores (executors) de Spark procesen datos que se encuentran físicamente más próximos a ellos, reduciendo así el uso del ancho de banda necesario para la transferencia de datos. De este modo, a cada núcleo de procesamiento de un ejecutor se le asigna una partición específica sobre la cual trabajar, maximizando la eficiencia del procesamiento y minimizando el tiempo de ejecución.\n","\n","El particionamiento de datos es fundamental en el funcionamiento de Spark, ya que es una herramienta para lograr el paralelismo, que consiste en ejecutar distintas operaciones de manera simultanea (en paralelo). Al distribuir los datos en particiones se facilita que los executors de Spark procesen datos que se encuentren almacenados más cerca de ellos (a nivel de hardware), lo que reduce el ancho de banda necesario para su transporte."]},{"cell_type":"markdown","metadata":{},"source":["preg5:\n","Spark Driver coordina el proceso de ejecución, gestiona el flujo de datos y el control de tareas, mientras que Spark Executors realizan el procesamiento de datos y ejecutan las tareas asignadas por el Driver, almacenando resultados intermedios en la memoria o en disco."]},{"cell_type":"markdown","metadata":{},"source":["info preg 6:\n","Spark SQL es una de las funcionalidades de alto nivel más destacadas de Spark, brindando a los desarrolladores la capacidad de emplear ANSI SQL:2003. Esto significa que puedes realizar consultas utilizando la sintaxis del SQL clásico. Para facilitar esta integración, Spark utiliza el optimizador Catalyst, que sigue varios pasos para procesar y optimizar las consultas:\n","\n","- **Análisis**: Crea un Árbol de Sintaxis Abstracta (AST) para examinar la estructura de las tablas y las operaciones solicitadas en la consulta, como nombres de columnas, tipos de datos y funciones.\n","\n","- **Optimización Lógica**: Genera múltiples planes de ejecución posibles, optimizando la consulta desde un punto de vista lógico para mejorar la eficiencia.\n","\n","- **Planificación Física**: A partir del mejor plan lógico, selecciona los operadores físicos específicos que se utilizarán para ejecutar la consulta en el motor de Spark.\n","\n","- **Generación de Código**: Produce código Java optimizado para ser ejecutado en cada nodo del clúster, garantizando una ejecución eficiente y distribuida.\n","\n","Esta secuencia de pasos permite que Spark SQL ofrezca un rendimiento excepcional para la ejecución de consultas SQL, aprovechando la infraestructura distribuida de Spark y optimizando las consultas para acelerar el procesamiento de datos."]},{"cell_type":"markdown","metadata":{},"source":["Catalyst es un optimizador enfocado en procesar las consultas SQL hechas en Spark, lo cuál lo realiza mediante 4 etapas de optimización. La primera de ellas es el **Análisis**, que consiste en la creación de un Árbol de Sintaxis Abstracta (AST) que examina la estructura de las tablar y las operaciones solicitadas en la consulta. Segundo, esta la **Optimización lógica** que consiste en generar planes de ejecución posibles, en donde el enfoque está en optimizar la consulta desde un punto de vista lógico para mejorar la eficiencia. Tercero, está la **Planificación Física** que consiste en basarse en el plan lógico para lego seleccionar los operadores físicos específicos que se utilizarán para ejecutar la consulta en el motor de Spark. Cuarto y último, está la **Generación de Código** que produce el código de Java optimizado para ser ejecutado en cada nodo del clúster, que tiene enfoque en garantizar una ejecución eficiente y distribuida.. "]},{"cell_type":"markdown","metadata":{"cell_id":"00002-bf13ea5a-d8bf-4cee-879e-ba1c7035e657","deepnote_cell_type":"markdown","id":"b020ce37"},"source":["## Parte Práctica\n","\n"]},{"cell_type":"markdown","metadata":{"id":"k0DaDvtgEYTV"},"source":["<center>\n","<img src=\"https://pbs.twimg.com/ad_img/1285681293590749189/kDckYy6Z?format=png&name=900x900\" width=350 />"]},{"cell_type":"markdown","metadata":{"id":"uW1dg_5_WR8S"},"source":["Juan Carlos Bodoque, el famoso periodista y empresario, decidió diversificar su portafolio de negocios y crear su propia plataforma de e-commerce. Después de varios años de investigar y analizar el mercado financiero, finalmente logró fundar Bodoque E-Shop con el objetivo de ofrecer a sus clientes una experiencia personalizada y confiable en sus transacciones.\n","\n","Sin embargo, con la llegada de los aliens al planeta Tierra, aparecen nuevos desafíos para el negocio. Por ello, Bodoque decide invertir en un equipo de expertos en tecnología y comercio interplanetario, para que Bodoque Shop implemente las últimas innovaciones en servicio al cliente para garantizar la satisfacción y fidelización de sus nuevos clientes.\n","\n","El primer objetivo de Bodoque E-Shop será la hacer un análisis exploratorio para entender mejor el comportamiento de los usuarios en la plataforma. Para ello Bodoque les hace entrega de un extenso dataset en el que se registran las actividades que han realizado sus clientes durante los últimos meses. A continuación se presenta un diccionario de variables que levanto el equipo de consultores interplanetarios de Bodoque:\n","\n","1. `Transaction ID`: A unique identifier for each transaction.\n","2. `Customer ID`: A unique identifier for each customer.\n","3. `Transaction Amount`: The total amount of money exchanged in the transaction in USD.\n","4. `Transaction Date`: The date and time when the transaction took place.\n","5. `Payment Method`: The method used to complete the transaction (e.g., credit card, PayPal, etc.).\n","6. `Product Category`: The category of the product involved in the transaction.\n","7. `Quantity`: The number of products involved in the transaction.\n","8. `Customer Age`: The age of the customer making the transaction.\n","9. `Customer Location`: The geographical location of the customer.\n","10. `Device Used`: The type of device used to make the transaction (e.g., mobile, desktop).\n","11. `IP Address`: The IP address of the device used for the transaction.\n","Shipping Address: The address where the product was shipped.\n","12. `Billing Address`: The address associated with the payment method.\n","13. `Is An Alien`: A binary indicator of whether customer is an alien.\n","14. `Account Age Days`: The age of the customer's account in days at the time of the transaction.\n","15. `Transaction Hour`: The hour of the day when the transaction occurred.\n"]},{"cell_type":"markdown","metadata":{"cell_id":"1769820f70244385ab5ac51f7509b6de","deepnote_cell_height":61.133331298828125,"deepnote_cell_type":"markdown","id":"MhISwri4zAHy"},"source":["### Importamos librerias utiles y cargamos los datos😸"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xHoq7VBlJoS3"},"outputs":[],"source":["!pip install pyspark\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["import os\n","os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jdk-XX\""]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["# Libreria Core del lab.\n","import pyspark\n","from pyspark import SparkConf, SparkContext\n","from pyspark.sql import SparkSession\n","import pandas as pd\n","from pyspark.sql.types import StringType, IntegerType, FloatType"]},{"cell_type":"code","execution_count":58,"metadata":{"id":"M6MKzLmPSHzY"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: plotly in c:\\users\\nicol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (5.24.0)\n","Collecting plotly\n","  Downloading plotly-5.24.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\nicol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from plotly) (8.2.3)\n","Requirement already satisfied: packaging in c:\\users\\nicol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from plotly) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\nicol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging->plotly) (3.0.9)\n","Downloading plotly-5.24.1-py3-none-any.whl (19.1 MB)\n","   ---------------------------------------- 19.1/19.1 MB 9.2 MB/s eta 0:00:00\n","Installing collected packages: plotly\n","  Attempting uninstall: plotly\n","    Found existing installation: plotly 5.24.0\n","    Uninstalling plotly-5.24.0:\n","      Successfully uninstalled plotly-5.24.0\n","Successfully installed plotly-5.24.1\n","Requirement already satisfied: missingno in c:\\users\\nicol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.5.2)\n","Requirement already satisfied: numpy in c:\\users\\nicol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from missingno) (1.24.4)\n","Requirement already satisfied: matplotlib in c:\\users\\nicol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from missingno) (3.7.1)\n","Requirement already satisfied: scipy in c:\\users\\nicol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from missingno) (1.13.0)\n","Requirement already satisfied: seaborn in c:\\users\\nicol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from missingno) (0.12.2)\n","Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\nicol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->missingno) (1.0.5)\n","Requirement already satisfied: cycler>=0.10 in c:\\users\\nicol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->missingno) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\nicol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->missingno) (4.37.4)\n","Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\nicol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->missingno) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\nicol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->missingno) (21.3)\n","Requirement already satisfied: pillow>=6.2.0 in c:\\users\\nicol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->missingno) (10.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\nicol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->missingno) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\nicol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->missingno) (2.8.2)\n","Requirement already satisfied: pandas>=0.25 in c:\\users\\nicol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from seaborn->missingno) (1.5.3)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\nicol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=0.25->seaborn->missingno) (2022.4)\n","Requirement already satisfied: six>=1.5 in c:\\users\\nicol\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->missingno) (1.16.0)\n"]}],"source":["!pip install --upgrade plotly\n","!pip install missingno"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["#Libreria para plotear\n","import matplotlib.pyplot as plt\n","import plotly.express as px"]},{"cell_type":"markdown","metadata":{"id":"9vJWSlEXYBqq"},"source":["Cargue los datos usando **pyspark**\n","\n","> Nota: Puede ser util el siguiente [enlace](https://www.oracle.com/cl/java/technologies/downloads/#jdk22-windows)"]},{"cell_type":"code","execution_count":60,"metadata":{"id":"i9Uf-BTZXqXe"},"outputs":[{"ename":"ConnectionRefusedError","evalue":"[WinError 10061] No se puede establecer una conexión ya que el equipo de destino denegó expresamente dicha conexión","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)","Cell \u001b[1;32mIn [60], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkConf, SparkContext\n\u001b[1;32m----> 4\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mconfig(conf\u001b[38;5;241m=\u001b[39m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLab4\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatos_lab_spark.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\conf.py:132\u001b[0m, in \u001b[0;36mSparkConf.__init__\u001b[1;34m(self, loadDefaults, _jvm, _jconf)\u001b[0m\n\u001b[0;32m    128\u001b[0m _jvm \u001b[38;5;241m=\u001b[39m _jvm \u001b[38;5;129;01mor\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_jvm\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;66;03m# JVM is created, so create self._jconf directly through JVM\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf \u001b[38;5;241m=\u001b[39m \u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparkConf\u001b[49m(loadDefaults)\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;66;03m# JVM is not created, so store data in self._conf first\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[0;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[1;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[0;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[0;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n","File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n","File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n","File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n","File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[1;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No se puede establecer una conexión ya que el equipo de destino denegó expresamente dicha conexión"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark import SparkConf, SparkContext\n","\n","spark = SparkSession.builder.config(conf=SparkConf().setMaster(\"local\").setAppName(\"Lab4\")).getOrCreate()\n","df = spark.read.parquet(\"datos_lab_spark.parquet\")\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"z6l6GNynYnh4"},"source":["### 2. Limpieza con pyspark [8 puntos]\n","(1 punto por pregunta)"]},{"cell_type":"markdown","metadata":{"id":"8DVdjYyOGRom"},"source":["<center>\n","<img src=\"https://miro.medium.com/v2/resize:fit:600/1*A6PpTrehGLxCJWNcUsDTNg.jpeg\" width=350 />\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sPGV40BjZekP"},"source":["Para comenzar con el análisis exploratorio usted decide empezar limpiando la base de datos con **pyspark** dado el alto volumen de datos que genera diariamente Bodoque E-Shop.\n","\n","**Nota: NO SE PERMITE EL USO DE PANDAS EN ESTA SECCIÓN**\n","\n","\n","\n","1.   Utilice `.printSchema()` para revisar la estructura de los datos\n","2.   Muestre las primeras 10 filas del dataset. Hint: utilice `.show()`\n","3.   Imprima un muestreo aleatorio con el 5% de los datos diponibles. . Hint: utilice `.sample()`\n","4. Revise los tipos de datos de cada columna con `.dtypes()` y responda la siguiente pregunta: ¿Cuál/es columna/s tiene/n un tipo de dato que no es el adecuado y por qué?\n","5. Cree una función **cast_columns** que permita cambiar el tipo de datos de las columnas problemáticas. Luego utilice esta función respecto a lo respondido en la pregunta anterior.\n","6. Cuente la cantidad de datos nulos por variable. Recuerde que Spark no posee un método que le permita calcular directamente los nulos.\n","7. Elimine datos nulos.\n","8. Elimine datos duplicados.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.show(10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.sample(fraction=0.05).show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pyspark.sql.types import IntegerType, BooleanType\n","def cast_columns(df, cols_types):\n","    \"\"\"\n","    Cambia el tipo de múltiples columnas en un DataFrame de Spark.\n","\n","    Parámetros:\n","    - df: DataFrame de Spark.\n","    - cols_types: Diccionario con nombres de columnas como claves y tipos de datos de Spark como valores.\n","\n","    Retorna:\n","    - DataFrame de Spark con tipos de columnas modificados.\n","    \"\"\"\n","    for col_name, new_type in cols_types.items():\n","        df = df.withColumn(col_name, df[col_name].cast(new_type))\n","    return df\n","\n","\n","df = cast_columns(df, {\n","    'Quantity': IntegerType(),\n","    'Customer Age': IntegerType(),\n","    'Is An Alien': BooleanType(),\n","    'Account Age Days': IntegerType(),\n","    'Transaction Hour': IntegerType()\n","})\n","df.dtypes"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"ename":"AssertionError","evalue":"Undefined error message parameter for error class: CANNOT_PARSE_DATATYPE. Parameters: {'error': '[WinError 10061] No se puede establecer una conexión ya que el equipo de destino denegó expresamente dicha conexión'}","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)","File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\clientserver.py:503\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 503\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msendall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n","\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] Se ha forzado la interrupción de una conexión existente por el host remoto","\nDuring handling of the above exception, another exception occurred:\n","\u001b[1;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)","File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n","File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\clientserver.py:506\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    505\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending or receiving.\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 506\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending\u001b[39m\u001b[38;5;124m\"\u001b[39m, e, proto\u001b[38;5;241m.\u001b[39mERROR_ON_SEND)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n","\u001b[1;31mPy4JNetworkError\u001b[0m: Error while sending","\nDuring handling of the above exception, another exception occurred:\n","\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)","File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:574\u001b[0m, in \u001b[0;36mDataFrame.schema\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    573\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m--> 574\u001b[0m         StructType, _parse_datatype_json_string(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson())\n\u001b[0;32m    575\u001b[0m     )\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n","File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1053\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1052\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException while sending command.\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 1053\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m   called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;124;03m   :class:`JavaMember` instances.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;124;03m if `binary` is `True`.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n","File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n","File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[1;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No se puede establecer una conexión ya que el equipo de destino denegó expresamente dicha conexión","\nDuring handling of the above exception, another exception occurred:\n","\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn [54], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col, when, \u001b[38;5;28msum\u001b[39m\n\u001b[1;32m----> 2\u001b[0m exprs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(when(col(c)\u001b[38;5;241m.\u001b[39misNull(), \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39motherwise(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39malias(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m]\n\u001b[0;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39magg(\u001b[38;5;241m*\u001b[39mexprs)\u001b[38;5;241m.\u001b[39mshow()\n","File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:2195\u001b[0m, in \u001b[0;36mDataFrame.columns\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2120\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m   2121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcolumns\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   2122\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2123\u001b[0m \u001b[38;5;124;03m    Retrieves the names of all columns in the :class:`DataFrame` as a list.\u001b[39;00m\n\u001b[0;32m   2124\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2193\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[0;32m   2194\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [f\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[38;5;241m.\u001b[39mfields]\n","File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:577\u001b[0m, in \u001b[0;36mDataFrame.schema\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    573\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m    574\u001b[0m             StructType, _parse_datatype_json_string(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mschema()\u001b[38;5;241m.\u001b[39mjson())\n\u001b[0;32m    575\u001b[0m         )\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 577\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mPySparkValueError\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    578\u001b[0m \u001b[43m            \u001b[49m\u001b[43merror_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCANNOT_PARSE_DATATYPE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessage_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43merror\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema\n","File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\base.py:42\u001b[0m, in \u001b[0;36mPySparkException.__init__\u001b[1;34m(self, message, error_class, message_parameters)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_reader \u001b[38;5;241m=\u001b[39m ErrorClassesReader()\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_error_message\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage_parameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m=\u001b[39m message\n","File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\utils.py:39\u001b[0m, in \u001b[0;36mErrorClassesReader.get_error_message\u001b[1;34m(self, error_class, message_parameters)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Verify message parameters.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m message_parameters_from_template \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<([a-zA-Z0-9_-]+)>\u001b[39m\u001b[38;5;124m\"\u001b[39m, message_template)\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mset\u001b[39m(message_parameters_from_template) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mset\u001b[39m(message_parameters), (\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUndefined error message parameter for error class: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage_parameters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     42\u001b[0m )\n\u001b[0;32m     43\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mmaketrans(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m message_template\u001b[38;5;241m.\u001b[39mtranslate(table)\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmessage_parameters)\n","\u001b[1;31mAssertionError\u001b[0m: Undefined error message parameter for error class: CANNOT_PARSE_DATATYPE. Parameters: {'error': '[WinError 10061] No se puede establecer una conexión ya que el equipo de destino denegó expresamente dicha conexión'}"]}],"source":["from pyspark.sql.functions import col, when, sum\n","exprs = [sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in df.columns]\n","\n","df.agg(*exprs).show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = df.na.drop()\n","df.count()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = df.dropDuplicates()\n","df.count()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.show(5)"]},{"cell_type":"markdown","metadata":{"id":"MjxI2Xd6cRu1"},"source":["### 3. Transformaciones con pyspark [6 puntos]\n","(1 punto por pregunta)"]},{"cell_type":"markdown","metadata":{"id":"bPfhWPZeHXUH"},"source":["<center>\n","<img src=\"https://live.staticflickr.com/13/91801406_0e71d7f019_b.jpg\" width=350 />\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lbIDKn44cWhI"},"source":["**Nota: NO SE PERMITE EL USO DE PANDAS EN ESTA SECCIÓN**\n","\n","Para continuar con el análisis, los especistas de Bodoque les gustaría tener nuevas variables disponibles. Tras las notas de la reunión usted llega a la conclusión de que tiene que realizar las siguientes tareas (con el dataset preprocesado de la seccion anterior):\n","\n","\n","1.   Agregar una columna llamada \"Transaction bp\" con el **monto total** de la transacción en bodoque pesos. Se considera que $x$ dólares equivalen a $log(48+|x^{36}|)$ bodoque pesos.\n","2.   Crear una columna llamada \"Transaction Month\" con el mes en que se realiza una transacción.\n","3.   Crear la variable *Type of purchase* según la catidad de unidades vendidas de acuerdo a las siguientes categorías.\n","  * Compra minorista: 4 productos o menos.\n","  * Compra mayorista: 5 produtos o más.\n","4. Imprima los registros de compras hechas por alienígenas en el comecio mayorista.  Utilice `.filter()`.\n","5. Cuente la cantidad de compras realizadas por humanos y la cantidad de compras realizadas por alienígenas. Utilice `.groupby()`.\n","6. Muestre una tabla con la recaudación promedio por transacción para cada método de pago, tanto para humanos como alienígenas. Utilice `pivot()`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#funciones necesarias para esta parte\n","from pyspark.sql.functions import  log, abs, pow, month, avg"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"cbtFJi3mHnkK"},"outputs":[],"source":["df = df.withColumn(\"Transaction bp\", log(48 + abs(pow(col(\"Transaction Amount\"), 36))))"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["df = df.withColumn(\"Transaction Month\", month(col(\"Transaction Date\")))\n","df.show(5)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["df = df.withColumn(\"Type of purchase\", \n","    when(col(\"Quantity\") <= 4, \"Compra minorista\")\n","    .otherwise(\"Compra mayorista\")\n",")\n","df.show(5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.filter((col(\"Is An Alien\") == True) & (col(\"Type of purchase\") == \"Compra mayorista\")).show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.groupBy(\"Is An Alien\").count().show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.groupBy(\"Is An Alien\").pivot(\"Payment Method\").agg(avg(\"Transaction Amount\")).show()\n"]},{"cell_type":"markdown","metadata":{"id":"17Muj6u2jOLq"},"source":["### 4. EDA [20 puntos]\n","(1 punto por gráfico y 1 punto por su interpretación)"]},{"cell_type":"markdown","metadata":{"id":"7F3yo66wFQ0z"},"source":["<center>\n","<img src=\"https://i.pinimg.com/originals/41/7e/7b/417e7b9089bcc20c4909df8954c6e742.gif\" width=400 />\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ayN5LYRamE7-"},"source":["Esta sección tiene como objetivo evaluar su habilidad para generar reportes y conclusiones a partir de los patrones identificados en los datos proporcionados por Bodoque. Específicamente, se enfoca en **caracterizar las transacciones** y **explorar las diferencias y similitudes en el comportamiento de humanos y aliens**. Utilice el dataset que ya incluye las transformaciones necesarias.\n","\n","Por favor, asegúrese de que **todas** las visualizaciones que realice cumplan con los siguientes criterios:\n","- Deben ser relevantes y fáciles de interpretar.\n","- Cada gráfico debe incluir un título claro, nombres en los ejes y leyendas adecuadas.\n","- Adjunte una breve descripción interpretativa junto a cada gráfico para explicar los resultados visualizados.\n","\n","Para llevar a cabo esta tarea, siga los siguientes pasos utilizando la librería de visualización de su elección (matplotlib, seaborn, plotly, etc):\n","\n","1. **Conversión del DataFrame a formato pandas** (2 puntos): Pase el DataFrame procesado a formato pandas. Evite realizar transformaciones adicionales con pandas.\n","2. **Visualización de Variables Categóricas** (2 puntos por visualización):\n","   - Genere **tres gráficos de barras** que diferencien entre humanos y aliens. Analice y comente cualquier diferencia o similitud observada entre estos dos grupos.\n","3. **Visualización de Variables Numéricas** (2 puntos por visualización):\n","   - Elabore **tres distplots** para examinar las distribuciones de variables numéricas, diferenciando entre humanos y aliens. Comente las diferencias o similitudes notables.\n","4. **Análisis de Patrones en Transacciones** (2 puntos por visualización):\n","   - Cree **tres gráficos avanzados** que ayuden a identificar patrones en las transacciones. Estos gráficos deben incorporar al menos dos dimensiones y diferir de los anteriores. Algunos ejemplos podrían ser un lineplot que muestre la cantidad de transacciones mensuales por canal de venta, o un barplot que exhiba los tres productos más vendidos por canal.\n","\n","Estos pasos le permitirán no solo visualizar datos complejos de manera efectiva, sino también interpretar estos datos para extraer insights valiosos acerca del comportamiento de los consumidores en el contexto de Bodoque."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tGw5y36IxRk3"},"outputs":[],"source":["pandas_df = df.toPandas()\n"]},{"cell_type":"markdown","metadata":{"id":"97zN2_g4vgY6"},"source":["### 5. Particiones y consultas en SQL [2 puntos]"]},{"cell_type":"markdown","metadata":{"id":"viNvNuE_odgc"},"source":["<center>\n","<img src=\"https://gitlab.com/imezadelajara/datos_clase_7_mds7202/-/raw/main/misc_images/1696330143457.gif\" width=400 />"]},{"cell_type":"markdown","metadata":{"id":"SCdHwyGBwVx8"},"source":["El equipo de Bodoque e-shop ha solicitado que los datos estén disponibles en una tabla SQL consultable. Además, están interesados en aprovechar las funciones de ventana en SQL para análisis avanzados. Las funciones de ventana permiten realizar cálculos sobre un conjunto de filas que están relacionadas con la fila actual. Por ejemplo, UNBOUNDED PRECEDING se usa para indicar que el rango de la función de ventana comienza desde la primera fila de la partición o del conjunto de resultados, lo cual es útil para calcular sumas acumulativas hasta la fila actual. Las variaciones comunes de este uso incluyen:\n","\n","- `UNBOUNDED PRECEDING` to `CURRENT ROW`: Calcula desde el inicio de la partición hasta la fila actual.\n","- `UNBOUNDED PRECEDING` to `UNBOUNDED FOLLOWING`: Cubre todas las filas dentro de la partición.\n","- `VALUE PRECEDING` to `VALUE FOLLOWING`: Establece un rango específico basado en valores antes y después de la fila actual."]},{"cell_type":"markdown","metadata":{"id":"VntjejKLleIa"},"source":["<center>\n","<img src=\"https://learnsql.com/blog/sql-window-functions-rows-clause/1.png\" width=500 />"]},{"cell_type":"markdown","metadata":{"id":"D8XJ7NrPllKG"},"source":["Ejemplo de uso en SQL:\n","\n","```sql\n","STAT(COL1_NAME) OVER (PARTITION BY COL2_NAME ORDER BY COL3_NAME ROWS BETWEEN X PRECEDING AND CURRENT ROW)\n","```\n","\n","\n","Responda y realice los siguientes puntos:\n","\n","1. **Creación de Tabla con PySpark** (2 puntos):\n","   - Desarrolle un script utilizando PySpark para crear una tabla a partir de un DataFrame previamente transformado. Seleccione y utilice una variable específica para la partición de la tabla. Justifique su elección de esta variable considerando factores como el tamaño del DataFrame, la distribución de los datos y el impacto potencial en el rendimiento de futuras consultas.\n","\n","2. **Consulta SQL para Principales Clientes** (Bonus: 2 punto):\n","   - Ejecute una consulta SQL para identificar los 10 clientes que más productos han comprado. La consulta debe retornar el ID del cliente junto con el total de productos comprados, ordenados en forma descendente.\n","\n","3. **Implementación de Función de Ventana en SQL y Equivalente en Spark** (Bonus: 2 punto):\n","   - Implemente una función de ventana en SQL para calcular la compra más alta realizada por cada usuario en los últimos tres meses. Además, describa cómo se podría realizar una función equivalente en Spark, considerando las capacidades específicas de PySpark para manejar este tipo de consultas.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AKQs-augfZBv"},"source":["### 6. UDF [12 puntos]"]},{"cell_type":"markdown","metadata":{"id":"ovDBGi-uhhdD"},"source":["<center>\n","<img src=\"https://64.media.tumblr.com/ba8c705edd2bed0a28d9458811155d69/tumblr_pap19zg4ae1w3zg6go1_400.gifv\" width=400 />"]},{"cell_type":"markdown","metadata":{"id":"TJUUnpi8qKHD"},"source":["\n","\n","Un experto en predicciones y programación le ha proporcionado un objeto serializado (`pickle`) diseñado para calcular las probabilidades de que un cliente cometa o no un fraude. Este experto sugiere que, para maximizar las capacidades de procesamiento distribuido de Spark, debería implementar `Scalar User Defined Functions` (udf). Esto le permitirá aplicar el objeto serializado en un entorno distribuido a lo largo de toda la población de datos. Un aspecto clave de la función desarrollada por el experto es que se enfoca exclusivamente en las siguientes columnas para realizar las predicciones: `['Transaction Amount', 'Quantity', 'Customer Age', 'Transaction Hour']`.\n","\n","Aparte, el experto le proporciona las siguientes instrucciones para usar las UDF en Spark:\n","\n","```python\n","from pyspark.sql.functions import udf\n","from pyspark.sql.types import FloatType\n","\n","def custom_function(col1, col2, col3, col4):\n","    pass\n","\n","udf_function = udf(custom_function, FloatType())\n","```\n","\n","Basándose en la estructura proporcionada, debe desarrollar una función que ejecute un código específico. Tenga en cuenta que esta función solo puede recibir columnas de Spark y debe retornar el valor deseado. Posteriormente, deberá utilizar esta función UDF indicando la función personalizada y el formato de salida.\n","\n","Siga los siguientes pasos para implementar la solución y responda las preguntas:\n","\n","1. **Cargar el objeto serializado**: Revise el tipo de objeto y deduzca su función. (1 punto)\n","2. **Explorar el objeto**: Utilice las funciones `dir` y `help` para identificar qué método del objeto predice la probabilidad. (1 punto)\n","3. **Crear una función personalizada**: Elabore una función que prediga la probabilidad de fraude utilizando el último valor de la lista generada por el objeto serializado. Puede modificar el nombre de la función para reflejar su propósito. (6 puntos)\n","4. **Definir la función UDF**: Establezca la función UDF con la función personalizada que ha creado. (2 punto)\n","5. **Generar una nueva columna**: Añada una nueva columna `prediction` a su DataFrame en Spark utilizando la función UDF y muestre un ejemplo de cómo se aplica. ¿Qué beneficios podría generar utilizar udf? (2 puntos)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bPJVs2OBezN_"},"outputs":[],"source":["import pickle\n","with open('object.pkl', 'rb') as file:\n","    model = pickle.load(file)\n","\n","print(dir(model))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","help(model)"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["def predict_fraud(transaction_amount, quantity, customer_age, transaction_hour):\n","    # Aquí asumimos que 'model' tiene un método 'predict_proba' y acepta una lista como entrada\n","    input_data = [[transaction_amount, quantity, customer_age, transaction_hour]]\n","    # Obtenemos las probabilidades de fraude (última columna de la salida)\n","    probabilities = model.predict_proba(input_data)\n","    # Retornamos solo la probabilidad de fraude\n","    return float(probabilities[0][-1])\n"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["from pyspark.sql.functions import udf\n","from pyspark.sql.types import FloatType\n","\n","# Definir la función UDF\n","udf_predict_fraud = udf(predict_fraud, FloatType())\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Supongamos que tu DataFrame de Spark se llama 'df'\n","# Y las columnas relevantes son 'Transaction Amount', 'Quantity', 'Customer Age', 'Transaction Hour'\n","df = df.withColumn(\"prediction\", udf_predict_fraud(df['Transaction Amount'], df['Quantity'], df['Customer Age'], df['Transaction Hour']))\n","\n","# Mostrar las primeras filas del DataFrame con la nueva columna\n","df.show(5)\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"}},"nbformat":4,"nbformat_minor":0}
