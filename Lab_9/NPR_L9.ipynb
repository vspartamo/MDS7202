{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b5c0d2440b3e4995a794ded565213150",
        "deepnote_cell_type": "markdown",
        "id": "_Mql1uRoI5v5"
      },
      "source": [
        "<h1><center>Laboratorio 9: Optimización de modelos 💯</center></h1>\n",
        "\n",
        "<center><strong>MDS7202: Laboratorio de Programación Científica para Ciencia de Datos - Primavera 2024</strong></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "bfb94b9656f145ad83e81b75d218cb70",
        "deepnote_cell_type": "markdown",
        "id": "FAPGIlEAI5v8"
      },
      "source": [
        "### **Cuerpo Docente:**\n",
        "\n",
        "- Profesores: Ignacio Meza, Sebastián Tinoco\n",
        "- Auxiliar: Eduardo Moya\n",
        "- Ayudantes: Nicolás Ojeda, Melanie Peña, Valentina Rojas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b1b537fdd27c43909a49d3476ce64d91",
        "deepnote_cell_type": "markdown",
        "id": "8NozgbkZI5v9"
      },
      "source": [
        "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no serán revisados\n",
        "\n",
        "- Nombre de alumno 1: Lucas Carrasco\n",
        "- Nombre de alumno 2: Nicolás Herrera\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Link de repositorio de GitHub:** [Insertar Repositorio](https://github.com/vspartamo/MDS7202)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b7dbdd30ab544cb8a8afe00648a586ae",
        "deepnote_cell_type": "markdown",
        "id": "vHU9DI6wI5v9"
      },
      "source": [
        "### Temas a tratar\n",
        "\n",
        "- Predicción de demanda usando `xgboost`\n",
        "- Búsqueda del modelo óptimo de clasificación usando `optuna`\n",
        "- Uso de pipelines.\n",
        "\n",
        "### Reglas:\n",
        "\n",
        "- **Grupos de 2 personas**\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente serán respondidos por este medio.\n",
        "- Prohibidas las copias.\n",
        "- Pueden usar cualquer matrial del curso que estimen conveniente.\n",
        "- Código que no se pueda ejecutar, no será revisado.\n",
        "\n",
        "### Objetivos principales del laboratorio\n",
        "\n",
        "- Optimizar modelos usando `optuna`\n",
        "- Recurrir a técnicas de *prunning*\n",
        "- Forzar el aprendizaje de relaciones entre variables mediante *constraints*\n",
        "- Fijar un pipeline con un modelo base que luego se irá optimizando.\n",
        "\n",
        "El laboratorio deberá ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al máximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante más eficientes que los iteradores nativos sobre DataFrames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "f1c73babb7f74af588a4fa6ae14829e0",
        "deepnote_cell_type": "markdown",
        "id": "U_-sNOuOI5v9"
      },
      "source": [
        "# Importamos librerias útiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "51afe4d2df42442b9e5402ffece60ead",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 4957,
        "execution_start": 1699544354044,
        "id": "ekHbM85NI5v9",
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "!pip install -qq xgboost optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6hJXpLCSspz"
      },
      "source": [
        "# El emprendimiento de Fiu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "44d227389a734ac59189c5e0005bc68a",
        "deepnote_cell_type": "markdown",
        "id": "b0bDalAOI5v-"
      },
      "source": [
        "Tras liderar de manera exitosa la implementación de un proyecto de ciencia de datos para caracterizar los datos generados en Santiago 2023, el misterioso corpóreo **Fiu** se anima y decide levantar su propio negocio de consultoría en machine learning. Tras varias e intensas negociaciones, Fiu logra encontrar su *primera chamba*: predecir la demanda (cantidad de venta) de una famosa productora de bebidas de calibre mundial. Al ver el gran potencial y talento que usted ha demostrado en el campo de la ciencia de datos, Fiu lo contrata como data scientist para que forme parte de su nuevo emprendimiento.\n",
        "\n",
        "Para este laboratorio deben trabajar con los datos `sales.csv` subidos a u-cursos, el cual contiene una muestra de ventas de la empresa para diferentes productos en un determinado tiempo.\n",
        "\n",
        "Para comenzar, cargue el dataset señalado y visualice a través de un `.head` los atributos que posee el dataset.\n",
        "\n",
        "<i><p align=\"center\">Fiu siendo felicitado por su excelente desempeño en el proyecto de caracterización de datos</p></i>\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media-front.elmostrador.cl/2023/09/A_UNO_1506411_2440e.jpg\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "cell_id": "2f9c82d204b14515ad27ae07e0b77702",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 92,
        "execution_start": 1699544359006,
        "id": "QvMPOqHuI5v-",
        "outputId": "659e7a12-d74d-45d6-d3c2-33a6cd338585",
        "source_hash": null
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>city</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>pop</th>\n",
              "      <th>shop</th>\n",
              "      <th>brand</th>\n",
              "      <th>container</th>\n",
              "      <th>capacity</th>\n",
              "      <th>price</th>\n",
              "      <th>quantity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>31/01/12</td>\n",
              "      <td>Athens</td>\n",
              "      <td>37.97945</td>\n",
              "      <td>23.71622</td>\n",
              "      <td>672130</td>\n",
              "      <td>shop_1</td>\n",
              "      <td>kinder-cola</td>\n",
              "      <td>glass</td>\n",
              "      <td>500ml</td>\n",
              "      <td>0.96</td>\n",
              "      <td>13280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>31/01/12</td>\n",
              "      <td>Athens</td>\n",
              "      <td>37.97945</td>\n",
              "      <td>23.71622</td>\n",
              "      <td>672130</td>\n",
              "      <td>shop_1</td>\n",
              "      <td>kinder-cola</td>\n",
              "      <td>plastic</td>\n",
              "      <td>1.5lt</td>\n",
              "      <td>2.86</td>\n",
              "      <td>6727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>31/01/12</td>\n",
              "      <td>Athens</td>\n",
              "      <td>37.97945</td>\n",
              "      <td>23.71622</td>\n",
              "      <td>672130</td>\n",
              "      <td>shop_1</td>\n",
              "      <td>kinder-cola</td>\n",
              "      <td>can</td>\n",
              "      <td>330ml</td>\n",
              "      <td>0.87</td>\n",
              "      <td>9848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>31/01/12</td>\n",
              "      <td>Athens</td>\n",
              "      <td>37.97945</td>\n",
              "      <td>23.71622</td>\n",
              "      <td>672130</td>\n",
              "      <td>shop_1</td>\n",
              "      <td>adult-cola</td>\n",
              "      <td>glass</td>\n",
              "      <td>500ml</td>\n",
              "      <td>1.00</td>\n",
              "      <td>20050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>31/01/12</td>\n",
              "      <td>Athens</td>\n",
              "      <td>37.97945</td>\n",
              "      <td>23.71622</td>\n",
              "      <td>672130</td>\n",
              "      <td>shop_1</td>\n",
              "      <td>adult-cola</td>\n",
              "      <td>can</td>\n",
              "      <td>330ml</td>\n",
              "      <td>0.39</td>\n",
              "      <td>25696</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id      date    city       lat      long     pop    shop        brand  \\\n",
              "0   0  31/01/12  Athens  37.97945  23.71622  672130  shop_1  kinder-cola   \n",
              "1   1  31/01/12  Athens  37.97945  23.71622  672130  shop_1  kinder-cola   \n",
              "2   2  31/01/12  Athens  37.97945  23.71622  672130  shop_1  kinder-cola   \n",
              "3   3  31/01/12  Athens  37.97945  23.71622  672130  shop_1   adult-cola   \n",
              "4   4  31/01/12  Athens  37.97945  23.71622  672130  shop_1   adult-cola   \n",
              "\n",
              "  container capacity  price  quantity  \n",
              "0     glass    500ml   0.96     13280  \n",
              "1   plastic    1.5lt   2.86      6727  \n",
              "2       can    330ml   0.87      9848  \n",
              "3     glass    500ml   1.00     20050  \n",
              "4       can    330ml   0.39     25696  "
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "df = pd.read_csv(\"sales.csv\")\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>city</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>pop</th>\n",
              "      <th>shop</th>\n",
              "      <th>brand</th>\n",
              "      <th>container</th>\n",
              "      <th>capacity</th>\n",
              "      <th>price</th>\n",
              "      <th>quantity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2012-01-31</td>\n",
              "      <td>Athens</td>\n",
              "      <td>37.97945</td>\n",
              "      <td>23.71622</td>\n",
              "      <td>672130</td>\n",
              "      <td>shop_1</td>\n",
              "      <td>kinder-cola</td>\n",
              "      <td>glass</td>\n",
              "      <td>500.0</td>\n",
              "      <td>0.96</td>\n",
              "      <td>13280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2012-01-31</td>\n",
              "      <td>Athens</td>\n",
              "      <td>37.97945</td>\n",
              "      <td>23.71622</td>\n",
              "      <td>672130</td>\n",
              "      <td>shop_1</td>\n",
              "      <td>kinder-cola</td>\n",
              "      <td>plastic</td>\n",
              "      <td>1500.0</td>\n",
              "      <td>2.86</td>\n",
              "      <td>6727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2012-01-31</td>\n",
              "      <td>Athens</td>\n",
              "      <td>37.97945</td>\n",
              "      <td>23.71622</td>\n",
              "      <td>672130</td>\n",
              "      <td>shop_1</td>\n",
              "      <td>kinder-cola</td>\n",
              "      <td>can</td>\n",
              "      <td>330.0</td>\n",
              "      <td>0.87</td>\n",
              "      <td>9848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2012-01-31</td>\n",
              "      <td>Athens</td>\n",
              "      <td>37.97945</td>\n",
              "      <td>23.71622</td>\n",
              "      <td>672130</td>\n",
              "      <td>shop_1</td>\n",
              "      <td>adult-cola</td>\n",
              "      <td>glass</td>\n",
              "      <td>500.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>20050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2012-01-31</td>\n",
              "      <td>Athens</td>\n",
              "      <td>37.97945</td>\n",
              "      <td>23.71622</td>\n",
              "      <td>672130</td>\n",
              "      <td>shop_1</td>\n",
              "      <td>adult-cola</td>\n",
              "      <td>can</td>\n",
              "      <td>330.0</td>\n",
              "      <td>0.39</td>\n",
              "      <td>25696</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id       date    city       lat      long     pop    shop        brand  \\\n",
              "0   0 2012-01-31  Athens  37.97945  23.71622  672130  shop_1  kinder-cola   \n",
              "1   1 2012-01-31  Athens  37.97945  23.71622  672130  shop_1  kinder-cola   \n",
              "2   2 2012-01-31  Athens  37.97945  23.71622  672130  shop_1  kinder-cola   \n",
              "3   3 2012-01-31  Athens  37.97945  23.71622  672130  shop_1   adult-cola   \n",
              "4   4 2012-01-31  Athens  37.97945  23.71622  672130  shop_1   adult-cola   \n",
              "\n",
              "  container  capacity  price  quantity  \n",
              "0     glass     500.0   0.96     13280  \n",
              "1   plastic    1500.0   2.86      6727  \n",
              "2       can     330.0   0.87      9848  \n",
              "3     glass     500.0   1.00     20050  \n",
              "4       can     330.0   0.39     25696  "
            ]
          },
          "execution_count": 134,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def convert_capacity_to_ml(capacity):\n",
        "    if 'ml' in capacity:\n",
        "        return float(capacity.replace('ml', ''))\n",
        "    elif 'lt' in capacity:\n",
        "        return float(capacity.replace('lt', '')) * 1000\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown capacity format: {capacity}\")\n",
        "df['capacity'] = df['capacity'].apply(convert_capacity_to_ml)\n",
        "\n",
        "#convertir las columnas lat, long, pop, capacity, price y quantity como variables numericas\n",
        "df['lat'] = pd.to_numeric(df['lat'], errors='coerce')\n",
        "df['long'] = pd.to_numeric(df['long'], errors='coerce')\n",
        "df['pop'] = pd.to_numeric(df['pop'], errors='coerce')\n",
        "df['capacity'] = pd.to_numeric(df['capacity'], errors='coerce')\n",
        "df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
        "df['quantity'] = pd.to_numeric(df['quantity'], errors='coerce')\n",
        "#convertir las columnas city, shop, brand y container a categoricas\n",
        "df['city'] = df['city'].astype('category')\n",
        "df['shop'] = df['shop'].astype('category')\n",
        "df['brand'] = df['brand'].astype('category')\n",
        "df['container'] = df['container'].astype('category')\n",
        "#convertir la columna date a datetime\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7456 entries, 0 to 7455\n",
            "Data columns (total 12 columns):\n",
            " #   Column     Non-Null Count  Dtype         \n",
            "---  ------     --------------  -----         \n",
            " 0   id         7456 non-null   int64         \n",
            " 1   date       7456 non-null   datetime64[ns]\n",
            " 2   city       7456 non-null   category      \n",
            " 3   lat        7456 non-null   float64       \n",
            " 4   long       7456 non-null   float64       \n",
            " 5   pop        7456 non-null   int64         \n",
            " 6   shop       7456 non-null   category      \n",
            " 7   brand      7456 non-null   category      \n",
            " 8   container  7456 non-null   category      \n",
            " 9   capacity   7456 non-null   float64       \n",
            " 10  price      7456 non-null   float64       \n",
            " 11  quantity   7456 non-null   int64         \n",
            "dtypes: category(4), datetime64[ns](1), float64(4), int64(3)\n",
            "memory usage: 496.0 KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b50db6f2cb804932ae3f9e5748a6ea61",
        "deepnote_cell_type": "markdown",
        "id": "pk4ru76pI5v_"
      },
      "source": [
        "## 1 Generando un Baseline (5 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/O-lan6TkadUAAAAC/what-i-wnna-do-after-a-baseline.gif\">\n",
        "</p>\n",
        "\n",
        "Antes de entrenar un algoritmo, usted recuerda los apuntes de su magíster en ciencia de datos y recuerda que debe seguir una serie de *buenas prácticas* para entrenar correcta y debidamente su modelo. Después de un par de vueltas, llega a las siguientes tareas:\n",
        "\n",
        "1. Separe los datos en conjuntos de train (70%), validation (20%) y test (10%). Fije una semilla para controlar la aleatoriedad. [0.5 puntos]\n",
        "2. Implemente un `FunctionTransformer` para extraer el día, mes y año de la variable `date`. Guarde estas variables en el formato categorical de pandas. [1 punto]\n",
        "3. Implemente un `ColumnTransformer` para procesar de manera adecuada los datos numéricos y categóricos. Use `OneHotEncoder` para las variables categóricas. `Nota:` Utilice el método `.set_output(transform='pandas')` para obtener un DataFrame como salida del `ColumnTransformer` [1 punto]\n",
        "4. Guarde los pasos anteriores en un `Pipeline`, dejando como último paso el regresor `DummyRegressor` para generar predicciones en base a promedios. [0.5 punto]\n",
        "5. Entrene el pipeline anterior y reporte la métrica `mean_absolute_error` sobre los datos de validación. ¿Cómo se interpreta esta métrica para el contexto del negocio? [0.5 puntos]\n",
        "6. Finalmente, vuelva a entrenar el `Pipeline` pero esta vez usando `XGBRegressor` como modelo **utilizando los parámetros por default**. ¿Cómo cambia el MAE al implementar este algoritmo? ¿Es mejor o peor que el `DummyRegressor`? [1 punto]\n",
        "7. Guarde ambos modelos en un archivo .pkl (uno cada uno) [0.5 puntos]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "cell_id": "1482c992d9494e5582b23dbd3431dbfd",
        "deepnote_cell_type": "code",
        "id": "sfnN7HubI5v_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((5219, 11), (1498, 11), (739, 11))"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn import set_config\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.drop(columns=[\"quantity\"])\n",
        "y = df[\"quantity\"]\n",
        "\n",
        "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.33, random_state=42)\n",
        "#mostrar las dimensiones de los conjuntos de datos\n",
        "X_train.shape, X_val.shape, X_test.shape\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import FunctionTransformer\n",
        "def extract_date_features(df):\n",
        "    df['date'] = pd.to_datetime(df['date'], format='%d/%m/%y')\n",
        "    df['day'] = df['date'].dt.day.astype('category')\n",
        "    df['month'] = df['date'].dt.month.astype('category')\n",
        "    df['year'] = df['date'].dt.year.astype('category')\n",
        "    #df.drop(columns=['date'], inplace=True)\n",
        "    return df\n",
        "\n",
        "date_transformer = FunctionTransformer(extract_date_features, validate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "numerical_cols = ['lat', 'long', 'pop', 'capacity', 'price']\n",
        "categorical_cols = ['city', 'shop', 'brand', 'container', 'day', 'month', 'year']\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore',sparse_output=False), categorical_cols)\n",
        "    ]\n",
        ").set_output(transform= 'pandas')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipeline_dummy = Pipeline([\n",
        "    ('date_transformer', date_transformer),\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', DummyRegressor(strategy='mean'))\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE (DummyRegressor): 13308.134750658153\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Entrenar pipeline\n",
        "pipeline_dummy.fit(X_train, y_train)\n",
        "\n",
        "# Predicciones en el conjunto de validación\n",
        "val_preds = pipeline_dummy.predict(X_val)\n",
        "\n",
        "# Calcular MAE\n",
        "mae_dummy = mean_absolute_error(y_val, val_preds)\n",
        "print(f\"MAE (DummyRegressor): {mae_dummy}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE (DummyRegressor) en test: 13394.728221101155\n"
          ]
        }
      ],
      "source": [
        "# predicciones en el conjunto de test\n",
        "test_preds = pipeline_dummy.predict(X_test)\n",
        "# calcular MAE\n",
        "mae_dummy_test = mean_absolute_error(y_test, test_preds)\n",
        "print(f\"MAE (DummyRegressor) en test: {mae_dummy_test}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE (XGBRegressor): 2420.2877598190817\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Define the pipeline with XGBRegressor\n",
        "pipeline_xgb = Pipeline([\n",
        "    ('date_transformer', date_transformer),\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', XGBRegressor(random_state=3))\n",
        "])\n",
        "\n",
        "# Train the pipeline\n",
        "pipeline_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "val_preds_xgb = pipeline_xgb.predict(X_val)\n",
        "\n",
        "# Calculate MAE\n",
        "mae_xgb = mean_absolute_error(y_val, val_preds_xgb)\n",
        "print(f\"MAE (XGBRegressor): {mae_xgb}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE (XGBRegressor) in test: 1545.2211458157137\n"
          ]
        }
      ],
      "source": [
        "# predictions in the test set\n",
        "test_preds_xgb = pipeline_xgb.predict(test_df)\n",
        "# calculate MAE\n",
        "mae_xgb_test = mean_absolute_error(test_df['quantity'], test_preds_xgb)\n",
        "print(f\"MAE (XGBRegressor) in test: {mae_xgb_test}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['pipeline_xgb.pkl']"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "# Guardar los modelos\n",
        "joblib.dump(pipeline_dummy, 'pipeline_dummy.pkl')\n",
        "joblib.dump(pipeline_xgb, 'pipeline_xgb.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "7e17e46063774ec28226fe300d42ffe0",
        "deepnote_cell_type": "markdown",
        "id": "wnyMINdKI5v_"
      },
      "source": [
        "## 2. Forzando relaciones entre parámetros con XGBoost (10 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://64.media.tumblr.com/14cc45f9610a6ee341a45fd0d68f4dde/20d11b36022bca7b-bf/s640x960/67ab1db12ff73a530f649ac455c000945d99c0d6.gif\">\n",
        "</p>\n",
        "\n",
        "Un colega aficionado a la economía le *sopla* que la demanda guarda una relación inversa con el precio del producto. Motivado para impresionar al querido corpóreo, se propone hacer uso de esta información para mejorar su modelo realizando las siguientes tareas:\n",
        "\n",
        "1. Vuelva a entrenar el `Pipeline` con `XGBRegressor`, pero esta vez forzando una relación monótona negativa entre el precio y la cantidad. Para aplicar esta restricción apóyese en la siguiente <a href = https://xgboost.readthedocs.io/en/stable/tutorials/monotonic.html>documentación</a>. [6 puntos]\n",
        "\n",
        ">Hint 1: Para implementar el constraint se le sugiere hacerlo especificando el nombre de la variable. De ser así, probablemente le sea útil **mantener el formato de pandas** antes del step de entrenamiento.\n",
        "\n",
        ">Hint 2: Puede obtener el nombre de las columnas en el paso anterior al modelo regresor mediante el método `.get_feature_names_out()`\n",
        "\n",
        "2. Luego, vuelva a reportar el `MAE` sobre el conjunto de validación. [1 puntos]\n",
        "\n",
        "3. ¿Cómo cambia el error al incluir esta relación? ¿Tenía razón su amigo? [2 puntos]\n",
        "\n",
        "4. Guarde su modelo en un archivo .pkl [1 punto]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['num__lat' 'num__long' 'num__pop' 'num__capacity' 'num__price'\n",
            " 'cat__city_Athens' 'cat__city_Irakleion' 'cat__city_Larisa'\n",
            " 'cat__city_Patra' 'cat__city_Thessaloniki' 'cat__shop_shop_1'\n",
            " 'cat__shop_shop_2' 'cat__shop_shop_3' 'cat__shop_shop_4'\n",
            " 'cat__shop_shop_5' 'cat__shop_shop_6' 'cat__brand_adult-cola'\n",
            " 'cat__brand_gazoza' 'cat__brand_kinder-cola' 'cat__brand_lemon-boost'\n",
            " 'cat__brand_orange-power' 'cat__container_can' 'cat__container_glass'\n",
            " 'cat__container_plastic' 'cat__day_28' 'cat__day_29' 'cat__day_30'\n",
            " 'cat__day_31' 'cat__month_1' 'cat__month_2' 'cat__month_3' 'cat__month_4'\n",
            " 'cat__month_5' 'cat__month_6' 'cat__month_7' 'cat__month_8'\n",
            " 'cat__month_9' 'cat__month_10' 'cat__month_11' 'cat__month_12'\n",
            " 'cat__year_2012' 'cat__year_2013' 'cat__year_2014' 'cat__year_2015'\n",
            " 'cat__year_2016' 'cat__year_2017' 'cat__year_2018']\n"
          ]
        }
      ],
      "source": [
        "feature_names = preprocessor.get_feature_names_out()\n",
        "print(feature_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the index of the 'price' column\n",
        "quantity_index = list(feature_names).index('num__price')\n",
        "\n",
        "# Create the monotone constraints list\n",
        "monotone_constraints = [0] * len(feature_names)\n",
        "monotone_constraints[quantity_index] = -1  # Enforce negative relationship for 'price'\n",
        "\n",
        "# Define the pipeline with XGBRegressor and monotone constraints\n",
        "pipeline_xgb_monotonic = Pipeline([\n",
        "    ('date_transformer', date_transformer),\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', XGBRegressor(monotone_constraints=monotone_constraints, random_state=3))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAE con restricción de monotonia: 2524.0239778182536\n"
          ]
        }
      ],
      "source": [
        "# Crear un diccionario de restricciones de monotonía donde la columna 'num__price' tiene un valor de -1 (relación inversa)\n",
        "monotone_constraints_dict = {name: 0 for name in feature_names}  # Todos son 0\n",
        "monotone_constraints_dict['num__price'] = -1  # Restringimos la columna 'num__price'\n",
        "\n",
        "# Crear un nuevo modelo XGBRegressor con las restricciones de monotonía\n",
        "xgb_monotonic = XGBRegressor(\n",
        "    monotone_constraints=monotone_constraints_dict,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Reemplazar el regressor en el pipeline por el nuevo modelo con restricciones\n",
        "pipeline_xgb_monotonic = Pipeline([\n",
        "    ('date_transformer', date_transformer),\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', xgb_monotonic)\n",
        "])\n",
        "\n",
        "# Entrenar el pipeline nuevamente\n",
        "pipeline_xgb_monotonic.fit(X_train, y_train)\n",
        "\n",
        "# Hacer predicciones y calcular el MAE\n",
        "val_preds = pipeline_xgb_monotonic.predict(X_val)\n",
        "mae_monotonic = mean_absolute_error(y_val, val_preds)\n",
        "print(f'MAE con restricción de monotonia: {mae_monotonic}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['pipeline_xgb_monotonic.pkl']"
            ]
          },
          "execution_count": 149,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Guardar el modelo en un archivo .pkl\n",
        "joblib.dump(pipeline_xgb_monotonic, 'pipeline_xgb_monotonic.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e59ef80ed20b4de8921f24da74e87374",
        "deepnote_cell_type": "markdown",
        "id": "5D5-tX4dI5v_"
      },
      "source": [
        "## 1.3 Optimización de Hiperparámetros con Optuna (20 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/fmNdyGN4z5kAAAAi/hacking-lucy.gif\">\n",
        "</p>\n",
        "\n",
        "Luego de presentarle sus resultados, Fiu le pregunta si es posible mejorar *aun más* su modelo. En particular, le comenta de la optimización de hiperparámetros con metodologías bayesianas a través del paquete `optuna`. Como usted es un aficionado al entrenamiento de modelos de ML, se propone implementar la descabellada idea de su jefe.\n",
        "\n",
        "A partir de la mejor configuración obtenida en la sección anterior, utilice `optuna` para optimizar sus hiperparámetros. En particular, se pide que su optimización considere lo siguiente:\n",
        "\n",
        "- Fijar una semilla en las instancias necesarias para garantizar la reproducibilidad de resultados\n",
        "- Utilice `TPESampler` como método de muestreo\n",
        "- De `XGBRegressor`, optimice los siguientes hiperparámetros:\n",
        "    - `learning_rate` buscando valores flotantes en el rango (0.001, 0.1)\n",
        "    - `n_estimators` buscando valores enteros en el rango (50, 1000)\n",
        "    - `max_depth` buscando valores enteros en el rango (3, 10)\n",
        "    - `max_leaves` buscando valores enteros en el rango (0, 100)\n",
        "    - `min_child_weight` buscando valores enteros en el rango (1, 5)\n",
        "    - `reg_alpha` buscando valores flotantes en el rango (0, 1)\n",
        "    - `reg_lambda` buscando valores flotantes en el rango (0, 1)\n",
        "- De `OneHotEncoder`, optimice el hiperparámetro `min_frequency` buscando el mejor valor flotante en el rango (0.0, 1.0)\n",
        "\n",
        "Para ello se pide los siguientes pasos:\n",
        "1. Implemente una función `objective()` que permita minimizar el `MAE` en el conjunto de validación. Use el método `.set_user_attr()` para almacenar el mejor pipeline entrenado. [10 puntos]\n",
        "2. Fije el tiempo de entrenamiento a 5 minutos. [1 punto]\n",
        "3. Optimizar el modelo y reportar el número de *trials*, el `MAE` y los mejores hiperparámetros encontrados. ¿Cómo cambian sus resultados con respecto a la sección anterior? ¿A qué se puede deber esto? [3 puntos]\n",
        "4. Explique cada hiperparámetro y su rol en el modelo. ¿Hacen sentido los rangos de optimización indicados? [5 puntos]\n",
        "5. Guardar su modelo en un archivo .pkl [1 punto]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "cell_id": "de5914621cc64cb0b1bacb9ff565a97e",
        "deepnote_cell_type": "code",
        "id": "kMXXi1ckI5v_"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "def objective(trial):\n",
        "    # Hiperparámetros a optimizar\n",
        "    learning_rate = trial.suggest_float('learning_rate', 0.001, 0.1)\n",
        "    n_estimators = trial.suggest_int('n_estimators', 50, 1000)\n",
        "    max_depth = trial.suggest_int('max_depth', 3, 10)\n",
        "    max_leaves = trial.suggest_int('max_leaves', 0, 100)\n",
        "    min_child_weight = trial.suggest_int('min_child_weight', 1, 5)\n",
        "    reg_alpha = trial.suggest_float('reg_alpha', 0.0, 1.0)\n",
        "    reg_lambda = trial.suggest_float('reg_lambda', 0.0, 1.0)\n",
        "    min_frequency = trial.suggest_float('min_frequency', 0.0, 1.0)\n",
        "    \n",
        "    # Crear el modelo\n",
        "    xgb_monotonic_opt = XGBRegressor(\n",
        "        learning_rate=learning_rate,\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        max_leaves=max_leaves,\n",
        "        min_child_weight=min_child_weight,\n",
        "        reg_alpha=reg_alpha,\n",
        "        reg_lambda=reg_lambda,\n",
        "        random_state=42,\n",
        "        #monotone_constraints=monotone_constraints\n",
        "    )\n",
        "    encoder = OneHotEncoder(min_frequency=min_frequency, handle_unknown='ignore', sparse_output=False)\n",
        "    \n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', StandardScaler(), numerical_cols),\n",
        "            ('cat', encoder, categorical_cols)\n",
        "        ]\n",
        "    ).set_output(transform= 'pandas')\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('date_transformer', date_transformer),\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', xgb_monotonic_opt)\n",
        "    ])\n",
        "    \n",
        "    # Entrenar el pipeline\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    \n",
        "    # Predecir y calcular MAE en el conjunto de validación\n",
        "    val_preds = pipeline.predict(X_val)\n",
        "    mae = mean_absolute_error(y_val, val_preds)\n",
        "    \n",
        "    # Almacenar el mejor pipeline entrenado\n",
        "    trial.set_user_attr('best_pipeline', pipeline)\n",
        "    \n",
        "    return mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear un estudio con TPESampler\n",
        "study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))\n",
        "\n",
        "# Ejecutar la optimización con límite de tiempo de 5 minutos\n",
        "study.optimize(objective, timeout=30)  # 300 segundos = 5 minutos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of trials: 37\n",
            "Best MAE: 2005.0433978703375\n",
            "Best hyperparameters: {'learning_rate': 0.08344666466543191, 'n_estimators': 577, 'max_depth': 9, 'max_leaves': 93, 'min_child_weight': 5, 'reg_alpha': 0.725569997204854, 'reg_lambda': 0.2432359796388749, 'min_frequency': 0.06336581659673984}\n"
          ]
        }
      ],
      "source": [
        "best_trial = study.best_trial\n",
        "print(f\"Number of trials: {len(study.trials)}\")\n",
        "print(f\"Best MAE: {best_trial.value}\")\n",
        "print(f\"Best hyperparameters: {best_trial.params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['pipeline_xgb_opt.pkl']"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#save the best model in a .pkl file\n",
        "joblib.dump(best_trial.user_attrs['best_pipeline'], 'pipeline_xgb_opt.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5195ccfc37e044ad9453f6eb2754f631",
        "deepnote_cell_type": "markdown",
        "id": "ZglyD_QWI5wA"
      },
      "source": [
        "## 4. Optimización de Hiperparámetros con Optuna y Prunners (17 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.pinimg.com/originals/90/16/f9/9016f919c2259f3d0e8fe465049638a7.gif\">\n",
        "</p>\n",
        "\n",
        "Después de optimizar el rendimiento de su modelo varias veces, Fiu le pregunta si no es posible optimizar el entrenamiento del modelo en sí mismo. Después de leer un par de post de personas de dudosa reputación en la *deepweb*, usted llega a la conclusión que puede cumplir este objetivo mediante la implementación de **Prunning**.\n",
        "\n",
        "Vuelva a optimizar los mismos hiperparámetros que la sección pasada, pero esta vez utilizando **Prunning** en la optimización. En particular, usted debe:\n",
        "\n",
        "- Responder: ¿Qué es prunning? ¿De qué forma debería impactar en el entrenamiento? [2 puntos]\n",
        "- Redefinir la función `objective()` utilizando `optuna.integration.XGBoostPruningCallback` como método de **Prunning** [10 puntos]\n",
        "- Fijar nuevamente el tiempo de entrenamiento a 5 minutos [1 punto]\n",
        "- Reportar el número de *trials*, el `MAE` y los mejores hiperparámetros encontrados. ¿Cómo cambian sus resultados con respecto a la sección anterior? ¿A qué se puede deber esto? [3 puntos]\n",
        "- Guardar su modelo en un archivo .pkl [1 punto]\n",
        "\n",
        "Nota: Si quieren silenciar los prints obtenidos en el prunning, pueden hacerlo mediante el siguiente comando:\n",
        "\n",
        "```\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "```\n",
        "\n",
        "De implementar la opción anterior, pueden especificar `show_progress_bar = True` en el método `optimize` para *más sabor*.\n",
        "\n",
        "Hint: Si quieren especificar parámetros del método .fit() del modelo a través del pipeline, pueden hacerlo por medio de la siguiente sintaxis: `pipeline.fit(stepmodelo__parametro = valor)`\n",
        "\n",
        "Hint2: Este <a href = https://stackoverflow.com/questions/40329576/sklearn-pass-fit-parameters-to-xgboost-in-pipeline>enlace</a> les puede ser de ayuda en su implementación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Qué es Prunning?** Pruning es una técnica que consiste en detener los trials que no están logrando un buen rendimiento durante las primeras etapas del entrenamiento. En lugar de continuar entrenando un modelo que probablemente no alcance el rendimiento deseado, el algoritmo de pruning finaliza su ejecución de forma anticipada, lo que permite reasignar los recursos a trials más prometedores.\n",
        "El impacto de esta técnica es reducir el tiempo total de entrenamiento y optimización, ya que se evita gastar tiempo en configuraciones de hiperparámetros que claramente no proporcionarán buenos resultados. El pruning debería mejorar la eficiencia del proceso de optimización, permitiendo explorar más configuraciones de hiperparámetros en menos tiempo y enfocarse en las configuraciones con mayor potencial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install optuna-integration[xgboost]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "import optuna\n",
        "from optuna.integration import XGBoostPruningCallback\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "def objective(trial):\n",
        "    # Hiperparámetros a optimizar\n",
        "    learning_rate = trial.suggest_float('learning_rate', 0.001, 0.1)\n",
        "    n_estimators = trial.suggest_int('n_estimators', 50, 1000)\n",
        "    max_depth = trial.suggest_int('max_depth', 3, 10)\n",
        "    max_leaves = trial.suggest_int('max_leaves', 0, 100)\n",
        "    min_child_weight = trial.suggest_int('min_child_weight', 1, 5)\n",
        "    reg_alpha = trial.suggest_float('reg_alpha', 0.0, 1.0)\n",
        "    reg_lambda = trial.suggest_float('reg_lambda', 0.0, 1.0)\n",
        "    min_frequency = trial.suggest_float('min_frequency', 0.0, 1.0)\n",
        "    \n",
        "    # Crear el modelo\n",
        "    xgb_monotonic_opt = XGBRegressor(\n",
        "        learning_rate=learning_rate,\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        max_leaves=max_leaves,\n",
        "        min_child_weight=min_child_weight,\n",
        "        reg_alpha=reg_alpha,\n",
        "        reg_lambda=reg_lambda,\n",
        "        random_state=42,\n",
        "        #monotone_constraints=monotone_constraints,\n",
        "        enable_categorical=True\n",
        "\n",
        "    )\n",
        "    encoder = OneHotEncoder(min_frequency=min_frequency, handle_unknown='ignore', sparse_output=False)\n",
        "    \n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', StandardScaler(), numerical_cols),\n",
        "            ('cat', encoder, categorical_cols)\n",
        "        ]\n",
        "    ).set_output(transform= 'pandas')\n",
        "    pipeline = Pipeline([\n",
        "        ('date_transformer', date_transformer),\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', xgb_monotonic_opt)\n",
        "    ])\n",
        "    \n",
        "    # Entrenar el pipeline con el callback de pruning\n",
        "    pipeline.fit(\n",
        "        X_train, y_train,\n",
        "        regressor__eval_set=[(X_val, y_val)],\n",
        "        regressor__early_stopping_rounds=10,\n",
        "        regressor__callbacks=[XGBoostPruningCallback(trial, \"validation_0-mae\")]\n",
        "    )\n",
        "    \n",
        "    # Predecir y calcular MAE en el conjunto de validación\n",
        "    val_preds = pipeline.predict(X_val)\n",
        "    mae = mean_absolute_error(y_val, val_preds)\n",
        "    \n",
        "    # Almacenar el mejor pipeline entrenado\n",
        "    trial.set_user_attr('best_pipeline', pipeline)\n",
        "    \n",
        "    return mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [],
      "source": [
        "import optuna\n",
        "from optuna.integration import XGBoostPruningCallback\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "def objective(trial):\n",
        "    # Hiperparámetros a optimizar\n",
        "    learning_rate = trial.suggest_float('learning_rate', 0.001, 0.1)\n",
        "    n_estimators = trial.suggest_int('n_estimators', 50, 1000)\n",
        "    max_depth = trial.suggest_int('max_depth', 3, 10)\n",
        "    max_leaves = trial.suggest_int('max_leaves', 0, 100)\n",
        "    min_child_weight = trial.suggest_int('min_child_weight', 1, 5)\n",
        "    reg_alpha = trial.suggest_float('reg_alpha', 0.0, 1.0)\n",
        "    reg_lambda = trial.suggest_float('reg_lambda', 0.0, 1.0)\n",
        "    min_frequency = trial.suggest_float('min_frequency', 0.0, 1.0)\n",
        "\n",
        "    # Asegúrate de que X y y estén definidos antes de dividir\n",
        "    X = df.drop(columns=['quantity'])  # Asegúrate de que 'quantity' es la columna objetivo\n",
        "    y = df['quantity']\n",
        "\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    # Transformación de las columnas de fecha\n",
        "    X_train['year'] = X_train['date'].dt.year\n",
        "    X_train['month'] = X_train['date'].dt.month\n",
        "    X_train['day'] = X_train['date'].dt.day\n",
        "    X_val['year'] = X_val['date'].dt.year\n",
        "    X_val['month'] = X_val['date'].dt.month\n",
        "    X_val['day'] = X_val['date'].dt.day\n",
        "\n",
        "    # Eliminamos la columna 'date' que causa conflicto\n",
        "    X_train = X_train.drop(columns=['date'])\n",
        "    X_val = X_val.drop(columns=['date'])\n",
        "\n",
        "    # Crear el modelo\n",
        "    xgb_monotonic_opt = XGBRegressor(\n",
        "        learning_rate=learning_rate,\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        max_leaves=max_leaves,\n",
        "        min_child_weight=min_child_weight,\n",
        "        reg_alpha=reg_alpha,\n",
        "        reg_lambda=reg_lambda,\n",
        "        random_state=42,\n",
        "        enable_categorical=True  # Asegúrate de que esto esté habilitado\n",
        "    )\n",
        "\n",
        "    # Codificar categorías y escalar variables numéricas\n",
        "    encoder = OneHotEncoder(min_frequency=min_frequency, handle_unknown='ignore', sparse_output=False)\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', StandardScaler(), numerical_cols),\n",
        "            ('cat', encoder, categorical_cols)\n",
        "        ]\n",
        "    ).set_output(transform='pandas')\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        ('date_transformer', date_transformer),\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('regressor', xgb_monotonic_opt)\n",
        "    ])\n",
        "\n",
        "    # Entrenar el pipeline con el callback de pruning\n",
        "    pipeline.fit(\n",
        "        X_train, y_train,\n",
        "        regressor__eval_set=[(X_val, y_val)],\n",
        "        regressor__early_stopping_rounds=10,\n",
        "        regressor__callbacks=[XGBoostPruningCallback(trial, \"validation_0-mae\")]\n",
        "    )\n",
        "\n",
        "    # Predecir y calcular MAE en el conjunto de validación\n",
        "    val_preds = pipeline.predict(X_val)\n",
        "    mae = mean_absolute_error(y_val, val_preds)\n",
        "\n",
        "    # Almacenar el mejor pipeline entrenado\n",
        "    trial.set_user_attr('best_pipeline', pipeline)\n",
        "\n",
        "    return mae\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[W 2024-10-23 00:01:29,521] Trial 0 failed with parameters: {'learning_rate': 0.03807947176588889, 'n_estimators': 954, 'max_depth': 8, 'max_leaves': 60, 'min_child_weight': 1, 'reg_alpha': 0.15599452033620265, 'reg_lambda': 0.05808361216819946, 'min_frequency': 0.8661761457749352} because of the following error: KeyError('date').\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3802, in get_loc\n",
            "    return self._engine.get_loc(casted_key)\n",
            "  File \"pandas\\_libs\\index.pyx\", line 138, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas\\_libs\\index.pyx\", line 165, in pandas._libs.index.IndexEngine.get_loc\n",
            "  File \"pandas\\_libs\\hashtable_class_helper.pxi\", line 5745, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "  File \"pandas\\_libs\\hashtable_class_helper.pxi\", line 5753, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
            "KeyError: 'date'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"C:\\Users\\nicol\\AppData\\Local\\Temp\\ipykernel_16920\\421633869.py\", line 64, in objective\n",
            "    pipeline.fit(\n",
            "  File \"c:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"c:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 469, in fit\n",
            "    Xt = self._fit(X, y, routed_params)\n",
            "  File \"c:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 406, in _fit\n",
            "    X, fitted_transformer = fit_transform_one_cached(\n",
            "  File \"c:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "  File \"c:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 1310, in _fit_transform_one\n",
            "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
            "  File \"c:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
            "    data_to_wrap = f(self, X, *args, **kwargs)\n",
            "  File \"c:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 1101, in fit_transform\n",
            "    return self.fit(X, y, **fit_params).transform(X)\n",
            "  File \"c:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
            "    data_to_wrap = f(self, X, *args, **kwargs)\n",
            "  File \"c:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py\", line 252, in transform\n",
            "    out = self._transform(X, func=self.func, kw_args=self.kw_args)\n",
            "  File \"c:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py\", line 379, in _transform\n",
            "    return func(X, **(kw_args if kw_args else {}))\n",
            "  File \"C:\\Users\\nicol\\AppData\\Local\\Temp\\ipykernel_16920\\2158759670.py\", line 3, in extract_date_features\n",
            "    df['date'] = pd.to_datetime(df['date'], format='%d/%m/%y')\n",
            "  File \"c:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py\", line 3807, in __getitem__\n",
            "    indexer = self.columns.get_loc(key)\n",
            "  File \"c:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3804, in get_loc\n",
            "    raise KeyError(key) from err\n",
            "KeyError: 'date'\n",
            "[W 2024-10-23 00:01:29,619] Trial 0 failed with value None.\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'date'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: 'date'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [113], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m, sampler\u001b[38;5;241m=\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Ejecutar la optimización con límite de tiempo de 5 minutos\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
            "File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
            "File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
            "Cell \u001b[1;32mIn [112], line 64\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     57\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m     58\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_transformer\u001b[39m\u001b[38;5;124m'\u001b[39m, date_transformer),\n\u001b[0;32m     59\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocessor),\n\u001b[0;32m     60\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregressor\u001b[39m\u001b[38;5;124m'\u001b[39m, xgb_monotonic_opt)\n\u001b[0;32m     61\u001b[0m ])\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Entrenar el pipeline con el callback de pruning\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregressor__eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregressor__early_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregressor__callbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mXGBoostPruningCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation_0-mae\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Predecir y calcular MAE en el conjunto de validación\u001b[39;00m\n\u001b[0;32m     72\u001b[0m val_preds \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(X_val)\n",
            "File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:469\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \n\u001b[0;32m    428\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and sequentially transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    468\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m--> 469\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:406\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, routed_params)\u001b[0m\n\u001b[0;32m    404\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[0;32m    405\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 406\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
            "File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:1310\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m   1309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1310\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\n\u001b[0;32m   1311\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1312\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[0;32m   1313\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m   1314\u001b[0m         )\n",
            "File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
            "File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1101\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
            "File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:252\u001b[0m, in \u001b[0;36mFunctionTransformer.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m\"\"\"Transform X using the forward function.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \n\u001b[0;32m    240\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m    Transformed input.\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    251\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_input(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 252\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkw_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    253\u001b[0m output_config \u001b[38;5;241m=\u001b[39m _get_output_config(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdense\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(out, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;66;03m# check the consistency between the column provided by `transform` and\u001b[39;00m\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;66;03m# the the column names provided by `get_feature_names_out`.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:379\u001b[0m, in \u001b[0;36mFunctionTransformer._transform\u001b[1;34m(self, X, func, kw_args)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    377\u001b[0m     func \u001b[38;5;241m=\u001b[39m _identity\n\u001b[1;32m--> 379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kw_args \u001b[38;5;28;01mif\u001b[39;00m kw_args \u001b[38;5;28;01melse\u001b[39;00m {}))\n",
            "Cell \u001b[1;32mIn [62], line 3\u001b[0m, in \u001b[0;36mextract_date_features\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_date_features\u001b[39m(df):\n\u001b[1;32m----> 3\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mday\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mmonth\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[1;32mc:\\Users\\nicol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[1;31mKeyError\u001b[0m: 'date'"
          ]
        }
      ],
      "source": [
        "# Crear un estudio con TPESampler\n",
        "study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))\n",
        "\n",
        "# Ejecutar la optimización con límite de tiempo de 5 minutos\n",
        "study.optimize(objective, timeout=30)  # 300 segundos = 5 minutos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8a081778cc704fc6bed05393a5419327",
        "deepnote_cell_type": "markdown",
        "id": "ZMiiVaCUI5wA"
      },
      "source": [
        "## 5. Visualizaciones (5 puntos)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/F-LgB1xTebEAAAAd/look-at-this-graph-nickelback.gif\">\n",
        "</p>\n",
        "\n",
        "\n",
        "Satisfecho con su trabajo, Fiu le pregunta si es posible generar visualizaciones que permitan entender el entrenamiento de su modelo.\n",
        "\n",
        "A partir del siguiente <a href = https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/005_visualization.html#visualization>enlace</a>, genere las siguientes visualizaciones:\n",
        "\n",
        "1. Gráfico de historial de optimización [1 punto]\n",
        "2. Gráfico de coordenadas paralelas [1 punto]\n",
        "3. Gráfico de importancia de hiperparámetros [1 punto]\n",
        "\n",
        "Comente sus resultados:\n",
        "\n",
        "4. ¿Desde qué *trial* se empiezan a observar mejoras notables en sus resultados? [0.5 puntos]\n",
        "5. ¿Qué tendencias puede observar a partir del gráfico de coordenadas paralelas? [1 punto]\n",
        "6. ¿Cuáles son los hiperparámetros con mayor importancia para la optimización de su modelo? [0.5 puntos]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "0e706dc9a8d946eda7a9eb1f0463c6d7",
        "deepnote_cell_type": "code",
        "id": "xjxAEENAI5wA"
      },
      "outputs": [],
      "source": [
        "# Inserte su código acá"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ac8a20f445d045a3becf1a518d410a7d",
        "deepnote_cell_type": "markdown",
        "id": "EoW32TA9I5wA"
      },
      "source": [
        "## 6. Síntesis de resultados (3 puntos)\n",
        "\n",
        "Finalmente:\n",
        "\n",
        "1. Genere una tabla resumen del MAE en el conjunto de validación obtenido en los 5 modelos entrenados desde Baseline hasta XGBoost con Constraints, Optuna y Prunning. [1 punto]\n",
        "2. Compare los resultados de la tabla y responda, ¿qué modelo obtiene el mejor rendimiento? [0.5 puntos]\n",
        "3. Cargue el mejor modelo, prediga sobre el conjunto de **test** y reporte su MAE. [0.5 puntos]\n",
        "4. ¿Existen diferencias con respecto a las métricas obtenidas en el conjunto de validación? ¿Porqué puede ocurrir esto? [1 punto]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq5C6cDnJg9h"
      },
      "outputs": [],
      "source": [
        "# Inserte su código acá"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5c4654d12037494fbd385b4dc6bd1059",
        "deepnote_cell_type": "markdown",
        "id": "E_19tgBEI5wA"
      },
      "source": [
        "# Conclusión\n",
        "Eso ha sido todo para el lab de hoy, recuerden que el laboratorio tiene un plazo de entrega de una semana. Cualquier duda del laboratorio, no duden en contactarnos por mail o U-cursos.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/8CT1AXElF_cAAAAC/gojo-satoru.gif\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown",
        "id": "rAp9UxwiI5wA"
      },
      "source": [
        "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=87110296-876e-426f-b91d-aaf681223468' target=\"_blank\">\n",
        "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
        "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "deepnote": {},
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "f63d38450a6b464c9bb6385cf11db4d9",
    "deepnote_persisted_session": {
      "createdAt": "2023-11-09T16:18:30.203Z"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
